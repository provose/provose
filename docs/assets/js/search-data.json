{"0": {
    "doc": "About",
    "title": "About Provose",
    "content": "Provose is the easiest way to manage your Amazon Web Services (AWS) infrastructure. Provose creates high-level abstractions over multiple AWS services, making it easy to deploy and manage complex infrastructure without managing all of the details. Provose is an MIT-licensed open source project. It was incubated at Neocrym, a record label that uses artificial intelligence to find and promote musicians. Provose was founded by James Mishra, the CEO of Neocrym. The near-entirety of Neocrym’s AWS infrastructure currently runs on Provose’s main branch. ",
    "url": "/about/#about-provose",
    "relUrl": "/about/#about-provose"
  },"1": {
    "doc": "About",
    "title": "About",
    "content": " ",
    "url": "/about/",
    "relUrl": "/about/"
  },"2": {
    "doc": "Amazon Machine Images (AMIs)",
    "title": "Provose Amazon Machine Images (AMIs)",
    "content": "These are Amazon Machine Images that are built for the Provose project. Provose is a new and easy way to configure cloud infrastructure. Provose uses these images internally for launching hosts for ec2_instances, containers, and other configurations. ",
    "url": "/amazon-machine-images-amis/#provose-amazon-machine-images-amis",
    "relUrl": "/amazon-machine-images-amis/#provose-amazon-machine-images-amis"
  },"3": {
    "doc": "Amazon Machine Images (AMIs)",
    "title": "Building these AMIs",
    "content": "These AMIs are built using Hashicorp Packer, and the source code for the AMIs can be found at https://github.com/provose/provose-amis . Versions . provose-docker-amazon-linux-2–v0.1 . This is a EBS-backed, GPU-enabled, ECS-enabled Amazon Linux 2-based AMI. The parent AMI is amzn2-ami-ecs-gpu-hvm-2.0.20200218-x86_64-ebs. It comes installed with Python 3 pip, Docker, Docker Compose, and the AWS CLI. | AWS AMI ID: | ami-08b64fc665987971b | . | AWS AMI source: | 826470379119/provose-docker-amazon-linux-2–v0.1 | . ",
    "url": "/amazon-machine-images-amis/#building-these-amis",
    "relUrl": "/amazon-machine-images-amis/#building-these-amis"
  },"4": {
    "doc": "Amazon Machine Images (AMIs)",
    "title": "Amazon Machine Images (AMIs)",
    "content": " ",
    "url": "/amazon-machine-images-amis/",
    "relUrl": "/amazon-machine-images-amis/"
  },"5": {
    "doc": "batch",
    "title": "batch",
    "content": " ",
    "url": "/v3.0/reference/batch/",
    "relUrl": "/v3.0/reference/batch/"
  },"6": {
    "doc": "batch",
    "title": "Description",
    "content": "The Provose batch configures resources for AWS Batch, which is fully-managed batch processing system for Dockerized workloads. How AWS Batch works . A full explanation of AWS Batch is outside the scope of the Provose documentation. You can learn more by reading the AWS Batch documentation. However, there are some important AWS Batch concepts to understand in order to use them with Provose: . | Jobs – A Job describes a computation to be done as described with a Docker image and some parameters (see below). This Provose module does not specify or submit Jobs. You must submit them with the AWS Console, the AWS API or the AWS CLI. The AWS Batch documentation page titled Submitting a Job has more information. | Job Definitions – A Job Definition is a blueprint for how Jobs are made. Job Definitions are configured below under the job_definitions key. Some of the parameters specified in a Job Definition can be overriden when specifying a job. | Docker images – AWS Batch Job Definitions are based on a Docker image that contains the code to be run in a job. You can read more about building and running Docker images on your local machine in the Docker documentation. You can create images on your your local computer and use Provose to upload your images to Amazon Web Services, after which they will be available for AWS Batch. | Job Queues – A Job Queue accepts Jobs until the Compute Environment is ready to accept the Job. You can define multiple Job Queues with different priorities below with the job_queues key. | Compute Environments – A Compute Environment pulls Jobs from the Job Queues to run them. A Compute Environment contains a pool of EC2 instances–with instance types of your choosing. The number of instances in the Compute Environment can scale up or down with the length of the queue. | . You can read more precise versions of the above definitions in the AWS Batch documentation. AWS Batch allows many-to-many configurations between Job Definitions and Compute Environments, but for the sake of simplicity, Provose assumes that you’ll only want one Compute Environment for every Job Definition you write. If you need more complex configurations, you should write your AWS Batch configuration directly with the Terraform AWS Batch module. Security limitations of AWS Batch . AWS Elastic Container Service (ECS) can fetch secrets from AWS Secrets Manager and present them as environment variables to a container. However, AWS Batch currently does not have a similar integration with Secrets Manager. If you want to pass secrets to AWS Batch using environment variables, they will not be encrypted. If you want to pass environment variables to a container using AWS Batch, you can use the environment key documented below. You can use this method to pass secret values, but it would be more secure for your container process to contact AWS Secrets Manager directly. Naming conventions . The names you provide Provose for queue and job names are global within the AWS Region. This means that if you use AWS Batch in two different Provose modules, you should take care not to reuse the same names. ",
    "url": "/v3.0/reference/batch/#description",
    "relUrl": "/v3.0/reference/batch/#description"
  },"7": {
    "doc": "batch",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } batch = { \"compute-environment-1\" = { instances = { instance_types = [\"m5.large\"] compute_environment_type = \"SPOT\" min_vcpus = 0 max_vcpus = 2 } job_queues = { \"mainqueue\" = { state = \"ENABLED\" priority = 1 } } job_definitions = { \"job-1\" = { image = { name = \"busybox\" tag = \"latest\" private_registry = false } vcpus = 1 memory = 512 command = [\"echo\", \"hello\", \"world\"] environment = { SOME_KEY = \"some_var\" } } } } } } . ",
    "url": "/v3.0/reference/batch/#examples",
    "relUrl": "/v3.0/reference/batch/#examples"
  },"8": {
    "doc": "batch",
    "title": "Inputs",
    "content": ". | instances – Required. This is an object that defines the compute resources for the given AWS Batch Compute Environment. | instance_types – Required. This is a list of AWS EC2 instance types that will run the Jobs. Not all instance types are available for AWS Batch. The list must also only contain instances of the same CPU architecture–you cannot have both x86 instances and ARM instances in the same list. | compute_environment_type – Required. The type of compute environment. Set this to \"EC2\" to deploy EC2 On-Demand instances for this Compute Environment. If you set this vlaue to \"SPOT\", you can save money by using AWS Spot Instances instead. | min_vcpus – Required. This is the minimum number of virtual CPUs (vCPUs) to maintain in the Compute Environment. These many vCPUs will be continually available for new jobs, even if the queue is empty. However, you can set this value to 0 to instruct AWS Batch to shut off the compute resources when the queue is empty. | max_vcpus – Required. This is the maximum number of virtual CPUs (vCPUs) that are available in the Compute Environment.o . | ami_id – Optional. If you wish, you can set this to a custom Amazon Machine Image for the compute resources to run. Presumably most of your job’s software is written in the Docker container, but you may need a custom AMI to mount filesystems or install device drivers. | . | job_queues – Required. This is mapping from queue names to objects with the given keys. | state – Required. This is \"ENABLED\" to enable the queue or \"DISABLED\" to disable the queue. | priority – Required. This is an integer value that describes the priority of this queue. A larger number gives this queue a higher priority in the attached Compute Environment. | . | job_definitions – Optional. This is a mapping from job definition names to job definition values. | image – Required. This defines the Docker image deployed by this AWS Batch job. | name – Required. The name of the Docker image to deploy. | tag – Required. This is the tag of the image to deploy. This is often a specific version or the string \"latest\". | private_registry – Required. Set this to true if the given image is ion your AWS account’s private Elastic Container Registry. If set to false, AWS Batch will look for an image with the given name in the public Docker Hub registry. | . | vcpus – Required. The number of vCPUs to dedicate to each Job that belongs to this Job Definition. | memory – Required. The amount of memory–in megabytes–designated to each job. | command – Required. This is the command to pass to the Docker container. Note that AWS Batch does not currently let you override the container entrypoint. | environment – Optional. This is mapping from environment variable names to their values. Remember, these values are uenncrypted. AWS Batch currently does not have a way to directly pull encrypted secrets from AWS Secrets Manager. | . | . ",
    "url": "/v3.0/reference/batch/#inputs",
    "relUrl": "/v3.0/reference/batch/#inputs"
  },"9": {
    "doc": "batch",
    "title": "Outputs",
    "content": ". | batch.aws_security_group.batch – A mapping from AWS Batch Compute Environment names to aws_security_group resources. By default, the security groups allow AWS Batch Jobs outbound access to the Internet. This may be undesirable for you if you are running jobs containing untrusted code or have other needs for network isolation. | batch.aws_batch_compute_environment.batch – A mapping from AWS Batch Compute Environment names to aws_batch_compute_environment resources. | batch.aws_batch_job_queue.batch – A mapping from AWS Batch Job Queue names to aws_batch_job_queue resources. | batch.aws_batch_job_definition.batch – A mapping from AWS Batch Job Definition names to aws_batch_job_definition resources. | . ",
    "url": "/v3.0/reference/batch/#outputs",
    "relUrl": "/v3.0/reference/batch/#outputs"
  },"10": {
    "doc": "batch",
    "title": "batch",
    "content": " ",
    "url": "/v4.0/reference/batch/",
    "relUrl": "/v4.0/reference/batch/"
  },"11": {
    "doc": "batch",
    "title": "Description",
    "content": "The Provose batch configures resources for AWS Batch, which is fully-managed batch processing system for Dockerized workloads. How AWS Batch works . A full explanation of AWS Batch is outside the scope of the Provose documentation. You can learn more by reading the AWS Batch documentation. However, there are some important AWS Batch concepts to understand in order to use them with Provose: . | Jobs – A Job describes a computation to be done as described with a Docker image and some parameters (see below). This Provose module does not specify or submit Jobs. You must submit them with the AWS Console, the AWS API or the AWS CLI. The AWS Batch documentation page titled Submitting a Job has more information. | Job Definitions – A Job Definition is a blueprint for how Jobs are made. Job Definitions are configured below under the job_definitions key. Some of the parameters specified in a Job Definition can be overriden when specifying a job. | Docker images – AWS Batch Job Definitions are based on a Docker image that contains the code to be run in a job. You can read more about building and running Docker images on your local machine in the Docker documentation. You can create images on your your local computer and use Provose to upload your images to Amazon Web Services, after which they will be available for AWS Batch. | Job Queues – A Job Queue accepts Jobs until the Compute Environment is ready to accept the Job. You can define multiple Job Queues with different priorities below with the job_queues key. | Compute Environments – A Compute Environment pulls Jobs from the Job Queues to run them. A Compute Environment contains a pool of EC2 instances–with instance types of your choosing. The number of instances in the Compute Environment can scale up or down with the length of the queue. | . You can read more precise versions of the above definitions in the AWS Batch documentation. AWS Batch allows many-to-many configurations between Job Definitions and Compute Environments, but for the sake of simplicity, Provose assumes that you’ll only want one Compute Environment for every Job Definition you write. If you need more complex configurations, you should write your AWS Batch configuration directly with the Terraform AWS Batch module. Security limitations of AWS Batch . AWS Elastic Container Service (ECS) can fetch secrets from AWS Secrets Manager and present them as environment variables to a container. However, AWS Batch currently does not have a similar integration with Secrets Manager. If you want to pass secrets to AWS Batch using environment variables, they will not be encrypted. If you want to pass environment variables to a container using AWS Batch, you can use the environment key documented below. You can use this method to pass secret values, but it would be more secure for your container process to contact AWS Secrets Manager directly. Naming conventions . The names you provide Provose for queue and job names are global within the AWS Region. This means that if you use AWS Batch in two different Provose modules, you should take care not to reuse the same names. ",
    "url": "/v4.0/reference/batch/#description",
    "relUrl": "/v4.0/reference/batch/#description"
  },"12": {
    "doc": "batch",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } batch = { \"compute-environment-1\" = { instances = { instance_types = [\"m5.large\"] compute_environment_type = \"SPOT\" min_vcpus = 0 max_vcpus = 2 } job_queues = { \"mainqueue\" = { state = \"ENABLED\" priority = 1 } } job_definitions = { \"job-1\" = { image = { name = \"busybox\" tag = \"latest\" private_registry = false } vcpus = 1 memory = 512 command = [\"echo\", \"hello\", \"world\"] environment = { SOME_KEY = \"some_var\" } } } } } } . ",
    "url": "/v4.0/reference/batch/#examples",
    "relUrl": "/v4.0/reference/batch/#examples"
  },"13": {
    "doc": "batch",
    "title": "Inputs",
    "content": ". | instances – Required. This is an object that defines the compute resources for the given AWS Batch Compute Environment. | instance_types – Required. This is a list of AWS EC2 instance types that will run the Jobs. Not all instance types are available for AWS Batch. The list must also only contain instances of the same CPU architecture–you cannot have both x86 instances and ARM instances in the same list. | compute_environment_type – Required. The type of compute environment. Set this to \"EC2\" to deploy EC2 On-Demand instances for this Compute Environment. If you set this vlaue to \"SPOT\", you can save money by using AWS Spot Instances instead. | min_vcpus – Required. This is the minimum number of virtual CPUs (vCPUs) to maintain in the Compute Environment. These many vCPUs will be continually available for new jobs, even if the queue is empty. However, you can set this value to 0 to instruct AWS Batch to shut off the compute resources when the queue is empty. | max_vcpus – Required. This is the maximum number of virtual CPUs (vCPUs) that are available in the Compute Environment.o . | ami_id – Optional. If you wish, you can set this to a custom Amazon Machine Image for the compute resources to run. Presumably most of your job’s software is written in the Docker container, but you may need a custom AMI to mount filesystems or install device drivers. | . | job_queues – Required. This is mapping from queue names to objects with the given keys. | state – Required. This is \"ENABLED\" to enable the queue or \"DISABLED\" to disable the queue. | priority – Required. This is an integer value that describes the priority of this queue. A larger number gives this queue a higher priority in the attached Compute Environment. | . | job_definitions – Optional. This is a mapping from job definition names to job definition values. | image – Required. This defines the Docker image deployed by this AWS Batch job. | name – Required. The name of the Docker image to deploy. | tag – Required. This is the tag of the image to deploy. This is often a specific version or the string \"latest\". | private_registry – Required. Set this to true if the given image is ion your AWS account’s private Elastic Container Registry. If set to false, AWS Batch will look for an image with the given name in the public Docker Hub registry. | . | vcpus – Required. The number of vCPUs to dedicate to each Job that belongs to this Job Definition. | memory – Required. The amount of memory–in megabytes–designated to each job. | command – Required. This is the command to pass to the Docker container. Note that AWS Batch does not currently let you override the container entrypoint. | environment – Optional. This is mapping from environment variable names to their values. Remember, these values are uenncrypted. AWS Batch currently does not have a way to directly pull encrypted secrets from AWS Secrets Manager. | . | . ",
    "url": "/v4.0/reference/batch/#inputs",
    "relUrl": "/v4.0/reference/batch/#inputs"
  },"14": {
    "doc": "batch",
    "title": "Outputs",
    "content": ". | batch.aws_security_group.batch – A mapping from AWS Batch Compute Environment names to aws_security_group resources. By default, the security groups allow AWS Batch Jobs outbound access to the Internet. This may be undesirable for you if you are running jobs containing untrusted code or have other needs for network isolation. | batch.aws_batch_compute_environment.batch – A mapping from AWS Batch Compute Environment names to aws_batch_compute_environment resources. | batch.aws_batch_job_queue.batch – A mapping from AWS Batch Job Queue names to aws_batch_job_queue resources. | batch.aws_batch_job_definition.batch – A mapping from AWS Batch Job Definition names to aws_batch_job_definition resources. | . ",
    "url": "/v4.0/reference/batch/#outputs",
    "relUrl": "/v4.0/reference/batch/#outputs"
  },"15": {
    "doc": "Provose and Certificate Transparency",
    "title": "Provose and Certificate Transparency",
    "content": "If you are going to use Provose, there are a few things you should know about Certificate Transparency. ",
    "url": "/certificate-transparency/",
    "relUrl": "/certificate-transparency/"
  },"16": {
    "doc": "Provose and Certificate Transparency",
    "title": "What is Certificate Transparency?",
    "content": "Certificate Transparency is an Internet security standard at protecting the chain of trust for digital certificates used on the Internet. Digital certificates are used to prevent communications on the Internet from eavesdropping or tampering in-transit. These certificates are issued by organizations called Certificate Authorities (CAs), who must be trusted to issue certificates that contain accurate information. Malicious actors, such as individual hackers or national governments, often target Certificate Authorities with the goal of issuing fraudulent certificates that can be used to impersonate a website. When they succeed, it is very difficult to detect and revoke these fraudulent certificates. Certificate Transparency aims to make detecting fraudulent certificates by creating a public log to record certificates issued by publicly trusted Certificate Authorities. ",
    "url": "/certificate-transparency/#what-is-certificate-transparency",
    "relUrl": "/certificate-transparency/#what-is-certificate-transparency"
  },"17": {
    "doc": "Provose and Certificate Transparency",
    "title": "How does Provose interact with Certificate Transparency?",
    "content": "Provose creates multiple certificates–for securing communciations both within private networks and across the Internet. All of these certificates are published to Certificate Transparency. Every time you create a Provose module with the module keyword, Provose creates a new Virtual Private Cloud (VPC) and an Amazon Certificate Manager (ACM) certificate to protect communications within the VPC. In order for this certificate to automatically be trusted by web browsers like Google Chrome, Amazon publishes the certificate’s metadata to the Certificate Transparency log. It is possible to disable this publishing, but then web browsers would throw an error when visiting a webpage secured by this certificate. This means that if your internal_root_domain is set to \"example-internal.com\" and your internal_subdomain is set to \"myproject\", then anybody in the world can see that Amazon issued a wildcard certificate for \"*.myproject.example-internal.com\". Provose provisions an internal HTTPS Application Load Balancer (ALB) that uses this wildcard certificate. If you have an Elasticsearch instance located at \"elasticsearch.myproject.example-internal.com\", Provose will serve traffic from the already-created wildcard certificate, so Certificate Transparency will not log the full DNS name of these services. However, if you use Provose to deploy containers serving HTTP requests–whether they come from within the VPC or from the public Internet–then Provose will provision another ACM certificate with the exact DNS name for your service–not a wildcard certificate. This means that if you deployed a container that serves traffic at \"container.example-internal.com\" or \"container.example.com\", Amazon will log the full DNS name to the public Certificate Transparency logs. In the future, Provose might reissue this certificate as a wildcard of \"*.example.com\" in an effort to leak less information to Certificate Transparency. However, this will only benefit DNS names that were created after this change was made. Additionally, it is only possible to protect only one subdomain level with a wildcard certificate. If you are serving traffic at \"long.dns.name.example.com\", the wildcard certificate logged to Certificate Transparency would be \"*.dns.name.example.com\". And remember–the Certificate Transparency log is forever. If Provose issues a certificate, and the certificate later expires or is deleted, the Certificate Transparency log will still have recorded the certificate’s existence. Do not issue certificates with DNS names that are embarrassing, must be kept secret, or reveal information about your infrastructure that you do not want to give away. ",
    "url": "/certificate-transparency/#how-does-provose-interact-with-certificate-transparency",
    "relUrl": "/certificate-transparency/#how-does-provose-interact-with-certificate-transparency"
  },"18": {
    "doc": "Provose and Certificate Transparency",
    "title": "How can I check the Certificate Transparency logs for my certificates?",
    "content": "Certificate Transparency logs are public, and there are various online tools like crt.sh. You can enter the various root domain names that you own, and it will return any certificates containing that root domain name–including certificates you may have generated with software other than Provose. ",
    "url": "/certificate-transparency/#how-can-i-check-the-certificate-transparency-logs-for-my-certificates",
    "relUrl": "/certificate-transparency/#how-can-i-check-the-certificate-transparency-logs-for-my-certificates"
  },"19": {
    "doc": "Provose and Certificate Transparency",
    "title": "Will Provose add a feature that would disable logging to Certificate Transparency?",
    "content": "No. It is still possible to use certificates to keep your infrastructure secure without having the certificates’ metadata published to the rest of the world, but it is far less confusing for Provose to log every certificate it creates. ",
    "url": "/certificate-transparency/#will-provose-add-a-feature-that-would-disable-logging-to-certificate-transparency",
    "relUrl": "/certificate-transparency/#will-provose-add-a-feature-that-would-disable-logging-to-certificate-transparency"
  },"20": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/changelog/",
    "relUrl": "/changelog/"
  },"21": {
    "doc": "Changelog",
    "title": "v3.0.0",
    "content": "January 19, 2021. | Updated the Terraform AWS provider to version 3.9.0. The new provider is not backwards compatible. We had to add a field in order to provision Luster clusters. | Enabled AWS Container Insights by default for all clusters created with the containers module. Users might notice a small increase in their bill due to the increased writes to AWS CloudWatch, but it should be worth it given how much easier it makes debugging containers. | Changed the Comment field on AWS Route 53 Hosted Zones from the value \"Managed by Terraform\" to a more descriptive comment like “Provose private zone for module [...], DNS path [...com], and VPC vpc-[...].\" | We now correctly propagate AWS resource tags from ECS services to the tasks they create. This should help users more accurately track their cloud costs per Provose module. | . ",
    "url": "/changelog/#v300",
    "relUrl": "/changelog/#v300"
  },"22": {
    "doc": "Changelog",
    "title": "v2.0.0",
    "content": "October 5, 2020. | Replaced the v1.x ec2_instances module with the brand-new (and slightly incompatible) ec2_on_demand_instances module. | Upgraded our pin of the Terraform AWS provider version 2.54.0 to 3.0.0. The new provider version is not backwards-compatible. We had to make some changes in how we verify Amazon Certificate Manager (ACM) certificates. | Updated the following Terraform providers: TLS to 2.2.0 and random to 2.3.0. | You can now specify a custom user for containers, just like with docker run --user. | Added the ability to specify arbitrary HTTPS redirects using the https_redirects module. This uses an AWS Application Load Balancer to route traffic from any Route 53 zone in the AWS account to any target on the web. | Added support for AWS FSx Lustre–Amazon Web Services’ managed offering of the high-performance Lustre filesystem–in the lustre_file_systems module. | Added support for AWS Elastic File Systems (EFS) by creating the elastic_file_systems module module. | Added support for AWS Batch–via the batch module. | Added support for granting access to S3 buckets for EC2 On-Demand instances (ec2_on_demand_instances) and EC2 Spot instances (ec2_spot_instances). | . ",
    "url": "/changelog/#v200",
    "relUrl": "/changelog/#v200"
  },"23": {
    "doc": "Changelog",
    "title": "v1.1.0",
    "content": "July 21, 2020. | Add the internal_http_health_check_success_status_codes parameter to the containers module. | Added support for AWS Fargate Spot instances to the containers module. | Specify that AWS ECS EC2 host instances should not communicate to ECS via an HTTP proxy, if the user happens to have otherwise set up a proxy on the instance. | . ",
    "url": "/changelog/#v110",
    "relUrl": "/changelog/#v110"
  },"24": {
    "doc": "Changelog",
    "title": "v1.0.2",
    "content": "June 28, 2020. | Various minor fixes to the Provose documentation. | We checked in an example terraform.tf to the root of the Provose GitHub repository, but this file was causing errors with Terraform so we subsequently removed it. | . ",
    "url": "/changelog/#v102",
    "relUrl": "/changelog/#v102"
  },"25": {
    "doc": "Changelog",
    "title": "v1.0.1",
    "content": "June 22, 2020. | Fixed an error when computing the for_each values for Elastic File System mount targets. | . ",
    "url": "/changelog/#v101",
    "relUrl": "/changelog/#v101"
  },"26": {
    "doc": "Changelog",
    "title": "v1.0.0",
    "content": "June 19, 2020. This is the initial release, so from a philosophical standpoint, there have not been any changes yet. Despite this being marked as a 1.0 release, Provose is still alpha-quality software. ",
    "url": "/changelog/#v100",
    "relUrl": "/changelog/#v100"
  },"27": {
    "doc": "containers",
    "title": "containers",
    "content": " ",
    "url": "/v4.0/reference/containers/",
    "relUrl": "/v4.0/reference/containers/"
  },"28": {
    "doc": "containers",
    "title": "Description",
    "content": "The Provose containers module automatically configures Docker containers to run on AWS Elastic Container Service (ECS). Provose abstracts over ECS concepts like clusters, services, task definitions, and image repositories. The containers module also helps set up Route 53 DNS settings, Amazon Certificate Manager (ACM) certificates to route TLS traffic, and AWS Application Load Balancer (ALBs) to balance the traffic across multiple containers. Provose supports running containers on EC2 instances or via AWS Fargate. It is generally easier to use AWS Fargate, but you should choose EC2 if you want to be able to SSH into the host instances or bind-mount host paths into your containers. Provose can pull publicly-available Docker images from Docker Hub, or you can use the Provose images module to build and upload your own containers to Provose-managed Elastic Container Registry image repositories. As of Provose 3.0, this module automatically enables AWS Container Insights, which writes some extremely useful container metrics to the AWS CloudWatch monitoring service. ",
    "url": "/v4.0/reference/containers/#description",
    "relUrl": "/v4.0/reference/containers/#description"
  },"29": {
    "doc": "containers",
    "title": "Examples",
    "content": "Running a public Docker container on AWS ECS EC2. This example shows a total of four nginx “Hello World” containers running on two EC2 instances of the t3.small instance type. module \"myproject-ec2\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject-ec2\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject-ec2\" } containers = { hellooec2 = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"ec2.example.com\"] } } instances = { instance_type = \"t2.small\" container_count = 4 instance_count = 2 cpu = 256 memory = 512 bash_user_data = &lt;&lt;EOF #!/bin/bash # Install Vim on the container hosts. yum update -y yum install vim EOF } } } } . Running a public Docker image on AWS Fargate. This example shows ten nginx “Hello World” containers running on AWS Fargate. module \"myproject-fargate\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject-fargate\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject-fargate\" } containers = { hellofargate = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"fargate.example.com\"] } } instances = { # set `instance_type` to be \"FARGATE_SPOT\" to save money by # using AWS EC2 Spot instances behind the scenes. instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } } } } . ",
    "url": "/v4.0/reference/containers/#examples",
    "relUrl": "/v4.0/reference/containers/#examples"
  },"30": {
    "doc": "containers",
    "title": "Inputs",
    "content": ". | image – Required. This object defines information about the Docker image that we are deploying. | name – Required. The name of the image to use, including the namespace. For example, to use the nginxdemos/hello container on Docker Hub, set name to \"nginxdemos/hello\" and private_registry to false. | tag – Required. The Docker Registry tag of the image to use. This may be a particular tag or version number in Docker Hub or Elastic Container Registry (ECR). However, \"latest\" is a common value to pick the latest version of an image in the registry. | private_registry – Required. Set this to true to look for a container with the name set in name in your AWS account’s private Elastic Container Registry (ECR). You should configure the ECR repository for this image with the Provose image key. Set this to false to use a publicly-available container on Docker Hub. | . | instances – Required. This is an object that defines how this container is run. | ami_id – Required. The ID of the Amazon Machine Image (AMI) to deploy for this instance. | instance_type – Required. Set this to \"FARGATE\" to deploy the containers on AWS Fargate. Set this to \"FARGATE_SPOT\" to use Fargate with Spot instances–which can give cost savings of up to 70%. Note that with Fargate, it will not be possible to use bind_mounts to mount to the host. However, if you want to deploy these containers on AWS EC2 instances, set this to the instance type of your choice, like \"t3.small\". Keep in mind that AWS does not make all instance types available in all Availability Zones. | container_count – Required. This is the number of containers to deploy. | instance_count – Optional. This field is required if instance_type is an EC2 instance type, but is unused when instance_type is \"FARGATE\" or \"FARGATE_SPOT\". | key_name – Optional. Set this to the name of an AWS EC2 key pair in your account to enable SSH access to the instances. This only works if the instance_type is an EC2 instance type and not \"FARGATE\" or \"FARGATE_SPOT\". | bash_user_data – Optional. This is a bash script that will be run on the creation of the underlying AWS instances. This field does nothing if the container is deployed with AWS Fargate. | cpu – Required. The CPU units given to each container. 1024 CPU units maps to one vCPU on AWS. This is a minimum of 128 if instance_type is an EC2 instance type, or 256 if the instance type is Fargate. | memory – Required. The amount of memory–in megabytes–given to each container. This must be in proportion. For example, a Fargate task with 256 CPU units can have 512, 1024, or 2048 megabytes of memory. | . | user – Optional. Defines an alternate user to run the container’s command. This is equivalent to passing --user to docker run. Docker containers typically have a default user they run as, and this key is only needed to override that default. You can specify a user or group either by name or by UID and GID, in the following formats: . | \"user\" | \"user:group\" | \"uid\" | \"gid\" | \"user:gid\" | \"uid:group\" | . | entrypoint – Optional. Defines a custom container entrypoint, if you do not want to use the entrypoint defined within the container. | command – Optional. Defines a custom container command. Use this if you do not want to use the command defined within the container. | environment – Optional. This is a mapping of environment variables and their values. | secrets – Optional. This is a mapping of environment variable names to Provose secret names. | efs_volumes – Optional. This is a mapping of the AWS Elastic File System (EFS) volumes to mount within the container. | container_mount – Required. The path in the container to mount the EFS volume. | . | bind_mounts – Optional. This is only for containers running with an EC2 instance_type. These are mounted filesystem paths from the EC2 host into the containers. This will not work for containers running on AWS Fargate because there is no concept of a host with a filesystem. | host_mount – Required. The directory on the container host to mount in the container. | container_mount – Required. The path in the container to mount the host filesystem. | . | public – Optional. This mapping configures network access from the public IPv4 Internet to the container. | https – Optional. This mapping configures the Application Load Balancer (ALB) that allows HTTP/HTTPS requests from the public Internet to the container. Use this if your container is an HTTP server. | public_dns_names – Required. A list of DNS names–like example.com or subdomain.example.com to point at this container. Your AWS account must own the underlying domain name and be able to set the DNS records. Provose will also generate Amazon Certificate Manager (ACM) certificate for the DNS names provided–enabling access to the container via HTTPS. | internal_http_port – Required. This is the port exposed by the container to serve HTTP requests. The load balancer listens on port 80 and 443 to forward ports to the given port on the container. | internal_http_health_check_path – Required. This is a URL path, like /robots.txt, that the Application Load Balancer (ALB) checks to determine whether the container is healthy. This path must return a 200 OK If the ALB decides a container is unhealthy, it will be removed from routing. | internal_http_health_check_success_status_codes – Optional. A list or range of HTTP status codes that the Application Load Balancer (ALB) will consider to be healthy. This can be a list of values like \"200,301\" or a range of values like \"200-299\". This corresponds with the matcher parameter of health_check objects on Terraform aws_lb_target_group resources. This defaults to only considering the HTTP code 200 as healthy. | internal_http_health_check_timeout – Optional. This sets the timeout for the HTTP requests that the Application Load Balancer (ALB)’s health checks. If this field is omitted, it defaults to 5 seconds. | stickiness_cookie_duration_seconds – Optional. If this value is present, it enables stickiness on the Application Load Balancer. Stickiness is the mechanism for client requests to consistently be routed to the same container instance behind the Application Load Balancer (ALB). The ALB sets an HTTP cookie for the first client request it receives, and then checks for the cookie on subsequent requests. The cookie eventually expires, and this value sets the expiration for the cookie–in seconds. | . | . | vpc – Optional. This mapping configures network access to the container from within the VPC that Provose creates. | https – Optional. This mapping configures the Application Load Balancer (ALB) that allows HTTP/HTTPS traffic from within the VPC. Use this if your container is an HTTP server. | vpc_dns_names – Required. A list of DNS names–like example.com or subdomain.example.com to point at this container. These domains must be used to serve internal traffic to your VPC. The vpc Provose configuration does not serve traffic to the public Internet. Your AWS account must own the underlying domain name and be able to set the DNS records. Provose will also generate Amazon Certificate Manager (ACM) certificate for the DNS names provided–enabling access to the container via HTTPS. | internal_http_port – Required. This is the port exposed by the container to serve HTTP requests. The load balancer listens on port 80 and 443 to forward ports to the given port on the container. | internal_http_health_check_path – Required. This is a URL path, like /robots.txt, that the Application Load Balancer (ALB) checks to determine whether the container is healthy. This path must return a 200 OK If the ALB decides a container is unhealthy, it will be removed from routing. | internal_http_health_check_timeout – Optional. This sets the timeout for the HTTP requests that the Application Load Balancer (ALB)’s health checks. If this field is omitted, it defaults to 5 seconds. | stickiness_cookie_duration_seconds – Optional. If this value is present, it enables stickiness on the Application Load Balancer. Stickiness is the mechanism for client requests to consistently be routed to the same container instance behind the Application Load Balancer (ALB). The ALB sets an HTTP cookie for the first client request it receives, and then checks for the cookie on subsequent requests. The cookie eventually expires, and this value sets the expiration for the cookie–in seconds. | . | . | s3_buckets – Optional. This is a mapping of S3 buckets to the classes of permissions available to the instances. The four classes of permissions available are list, get, put, and delete, and the values for each one is true or false. To use this configuration, place the s3_buckets key inside a block that defines a container. Below is an example of how to give the container access to two buckets–one with list and get permissions, and another with get and delete permissions. | . s3_buckets = { \"some-bucket-name.example-internal.com\" = { permissions = { list = true get = true put = false delete = false } } \"another-bucket.com\" = { permissions = { list = false get = true put = false delete = true } } } . ",
    "url": "/v4.0/reference/containers/#inputs",
    "relUrl": "/v4.0/reference/containers/#inputs"
  },"31": {
    "doc": "containers",
    "title": "Outputs",
    "content": "Provose provides these Terraform outputs to enable users to patch certain advanced configurations that cannot be configured via the inputs. | containers.aws_security_group.container__internal_http_port – A mapping between container names and the aws_security_group resources that govern network access to each container. | containers.aws_ecs_cluster.container – This is a mapping between container names and their corresponding aws_ecs_cluster resources. Provose gives every container definition its own cluster. | containers.aws_ecs_task_definition.container – A mapping between container names and their corresponding aws_ecs_task_definition resourcehttps://www.terraform.io/docs/providers/aws/r/ecs_task_definition.html. | containers.aws_lb_target_group.container__public_https – A mapping between container names and the containers’ corresponding aws_lb_target_group resources. | containers.aws_lb_target_group.container__vpc_https – These are aws_lb_target_group resources for each container that give HTTPS access from within a VPC. | containers.aws_lb_listener_rule.container__public_https – These are aws_lb_listener_rule resources for each container that give public HTTPS access. | containers.aws_ecs_service.container – A mapping of aws_ecs_service resources for each container name. In Provose, each container only belongs to one service. | containers.aws_instance.container__instance – A mapping of aws_instance resources being used to host containers. This will be empty if you are running all of your containers with AWS Fargate as opposed to using EC2 hosts. | containers.aws_route53_record.container__instance – A mapping of aws_route53_record resources for EC2 instances hosting containers. This will be empty if you are running all of your containers with AWS Fargate as opposed to EC2. | containers.aws_route53_record.container__public_https – A mapping of aws_route53_record resources that point to public HTTPS websites served by containers. | containers.aws_route53_record.container__public_https_validation – A mapping of aws_route53_record resources that were created specifically to validate the Amazon Certificate Manager certificates for domain names that we point a container to. | containers.aws_route53_record.container__vpc_https – A mapping of aws_route53_record resources created for DNS resolution to HTTPS websites within the VPC. | containers.aws_route53_record.container__instance_spot – A mapping of aws_route53_record resources for container spot instances. | containers.aws_route53_zone.external_dns__for_containers – A mapping from container names to aws_route53_zone resources describing the Route 53 zones that contain the domain names that serve traffic to each container. | containers.aws_acm_certificate.container__public_https – A mapping of aws_acm_certificate resources representing the TLS certificates that communicate connections to the container from the public Internet. | containers.aws_acm_certificate_validation.container__public_https_validation – A mapping of aws_acm_certificate_validation resources. Use this to debug issues with Amazon Certificate Manager (ACM) certificate validation. | . ",
    "url": "/v4.0/reference/containers/#outputs",
    "relUrl": "/v4.0/reference/containers/#outputs"
  },"32": {
    "doc": "containers",
    "title": "Implementation details",
    "content": "Container networking modes . Containers launched with the \"FARGATE\" instance type are launched with the \"awsvpc\" container networking mode. Containers launched with one of the EC2 instance types are launched with the \"bridge\" container networking mode. This is because ECS EC2 containers do not have public Internet access unless the containers are both launched in a private subnet and connected to the Internet through a NAT gateway. The NAT gateway costs additional money, so we can save money by using the \"bridge\" networking mode instead. This, according to Amazon, is slightly less performant, but the faster \"awsvpc\" networking mode is still available for Fargate containers through Provose. ",
    "url": "/v4.0/reference/containers/#implementation-details",
    "relUrl": "/v4.0/reference/containers/#implementation-details"
  },"33": {
    "doc": "containers",
    "title": "containers",
    "content": " ",
    "url": "/v3.0/reference/containers/",
    "relUrl": "/v3.0/reference/containers/"
  },"34": {
    "doc": "containers",
    "title": "Description",
    "content": "The Provose containers module automatically configures Docker containers to run on AWS Elastic Container Service (ECS). Provose abstracts over ECS concepts like clusters, services, task definitions, and image repositories. The containers module also helps set up Route 53 DNS settings, Amazon Certificate Manager (ACM) certificates to route TLS traffic, and AWS Application Load Balancer (ALBs) to balance the traffic across multiple containers. Provose supports running containers on EC2 instances or via AWS Fargate. It is generally easier to use AWS Fargate, but you should choose EC2 if you want to be able to SSH into the host instances or bind-mount host paths into your containers. Provose can pull publicly-available Docker images from Docker Hub, or you can use the Provose images module to build and upload your own containers to Provose-managed Elastic Container Registry image repositories. As of Provose 3.0, this module automatically enables AWS Container Insights, which writes some extremely useful container metrics to the AWS CloudWatch monitoring service. ",
    "url": "/v3.0/reference/containers/#description",
    "relUrl": "/v3.0/reference/containers/#description"
  },"35": {
    "doc": "containers",
    "title": "Examples",
    "content": "Running a public Docker container on AWS ECS EC2. This example shows a total of four nginx “Hello World” containers running on two EC2 instances of the t3.small instance type. module \"myproject-ec2\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject-ec2\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject-ec2\" } containers = { hellooec2 = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"ec2.example.com\"] } } instances = { instance_type = \"t2.small\" container_count = 4 instance_count = 2 cpu = 256 memory = 512 bash_user_data = &lt;&lt;EOF #!/bin/bash # Install Vim on the container hosts. yum update -y yum install vim EOF } } } } . Running a public Docker image on AWS Fargate. This example shows ten nginx “Hello World” containers running on AWS Fargate. module \"myproject-fargate\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject-fargate\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject-fargate\" } containers = { hellofargate = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"fargate.example.com\"] } } instances = { # set `instance_type` to be \"FARGATE_SPOT\" to save money by # using AWS EC2 Spot instances behind the scenes. instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } } } } . ",
    "url": "/v3.0/reference/containers/#examples",
    "relUrl": "/v3.0/reference/containers/#examples"
  },"36": {
    "doc": "containers",
    "title": "Inputs",
    "content": ". | image – Required. This object defines information about the Docker image that we are deploying. | name – Required. The name of the image to use, including the namespace. For example, to use the nginxdemos/hello container on Docker Hub, set name to \"nginxdemos/hello\" and private_registry to false. | tag – Required. The Docker Registry tag of the image to use. This may be a particular tag or version number in Docker Hub or Elastic Container Registry (ECR). However, \"latest\" is a common value to pick the latest version of an image in the registry. | private_registry – Required. Set this to true to look for a container with the name set in name in your AWS account’s private Elastic Container Registry (ECR). You should configure the ECR repository for this image with the Provose image key. Set this to false to use a publicly-available container on Docker Hub. | . | instances – Required. This is an object that defines how this container is run. | instance_type – Required. Set this to \"FARGATE\" to deploy the containers on AWS Fargate. Set this to \"FARGATE_SPOT\" to use Fargate with Spot instances–which can give cost savings of up to 70%. Note that with Fargate, it will not be possible to use bind_mounts to mount to the host. However, if you want to deploy these containers on AWS EC2 instances, set this to the instance type of your choice, like \"t3.small\". Keep in mind that AWS does not make all instance types available in all Availability Zones. | container_count – Required. This is the number of containers to deploy. | instance_count – Optional. This field is required if instance_type is an EC2 instance type, but is unused when instance_type is \"FARGATE\" or \"FARGATE_SPOT\". | key_name – Optional. Set this to the name of an AWS EC2 key pair in your account to enable SSH access to the instances. This only works if the instance_type is an EC2 instance type and not \"FARGATE\" or \"FARGATE_SPOT\". | bash_user_data – Optional. This is a bash script that will be run on the creation of the underlying AWS instances. This field does nothing if the container is deployed with AWS Fargate. | cpu – Required. The CPU units given to each container. 1024 CPU units maps to one vCPU on AWS. This is a minimum of 128 if instance_type is an EC2 instance type, or 256 if the instance type is Fargate. | memory – Required. The amount of memory–in megabytes–given to each container. This must be in proportion. For example, a Fargate task with 256 CPU units can have 512, 1024, or 2048 megabytes of memory. | . | user – Optional. Defines an alternate user to run the container’s command. This is equivalent to passing --user to docker run. Docker containers typically have a default user they run as, and this key is only needed to override that default. You can specify a user or group either by name or by UID and GID, in the following formats: . | \"user\" | \"user:group\" | \"uid\" | \"gid\" | \"user:gid\" | \"uid:group\" | . | entrypoint – Optional. Defines a custom container entrypoint, if you do not want to use the entrypoint defined within the container. | command – Optional. Defines a custom container command. Use this if you do not want to use the command defined within the container. | environment – Optional. This is a mapping of environment variables and their values. | secrets – Optional. This is a mapping of environment variable names to Provose secret names. | efs_volumes – Optional. This is a mapping of the AWS Elastic File System (EFS) volumes to mount within the container. | container_mount – Required. The path in the container to mount the EFS volume. | . | bind_mounts – Optional. This is only for containers running with an EC2 instance_type. These are mounted filesystem paths from the EC2 host into the containers. This will not work for containers running on AWS Fargate because there is no concept of a host with a filesystem. | host_mount – Required. The directory on the container host to mount in the container. | container_mount – Required. The path in the container to mount the host filesystem. | . | public – Optional. This mapping configures network access from the public IPv4 Internet to the container. | https – Optional. This mapping configures the Application Load Balancer (ALB) that allows HTTP/HTTPS requests from the public Internet to the container. Use this if your container is an HTTP server. | public_dns_names – Required. A list of DNS names–like example.com or subdomain.example.com to point at this container. Your AWS account must own the underlying domain name and be able to set the DNS records. Provose will also generate Amazon Certificate Manager (ACM) certificate for the DNS names provided–enabling access to the container via HTTPS. | internal_http_port – Required. This is the port exposed by the container to serve HTTP requests. The load balancer listens on port 80 and 443 to forward ports to the given port on the container. | internal_http_health_check_path – Required. This is a URL path, like /robots.txt, that the Application Load Balancer (ALB) checks to determine whether the container is healthy. This path must return a 200 OK If the ALB decides a container is unhealthy, it will be removed from routing. | internal_http_health_check_success_status_codes – Optional. A list or range of HTTP status codes that the Application Load Balancer (ALB) will consider to be healthy. This can be a list of values like \"200,301\" or a range of values like \"200-299\". This corresponds with the matcher parameter of health_check objects on Terraform aws_lb_target_group resources. This defaults to only considering the HTTP code 200 as healthy. | internal_http_health_check_timeout – Optional. This sets the timeout for the HTTP requests that the Application Load Balancer (ALB)’s health checks. If this field is omitted, it defaults to 5 seconds. | stickiness_cookie_duration_seconds – Optional. If this value is present, it enables stickiness on the Application Load Balancer. Stickiness is the mechanism for client requests to consistently be routed to the same container instance behind the Application Load Balancer (ALB). The ALB sets an HTTP cookie for the first client request it receives, and then checks for the cookie on subsequent requests. The cookie eventually expires, and this value sets the expiration for the cookie–in seconds. | . | . | vpc – Optional. This mapping configures network access to the container from within the VPC that Provose creates. | https – Optional. This mapping configures the Application Load Balancer (ALB) that allows HTTP/HTTPS traffic from within the VPC. Use this if your container is an HTTP server. | vpc_dns_names – Required. A list of DNS names–like example.com or subdomain.example.com to point at this container. These domains must be used to serve internal traffic to your VPC. The vpc Provose configuration does not serve traffic to the public Internet. Your AWS account must own the underlying domain name and be able to set the DNS records. Provose will also generate Amazon Certificate Manager (ACM) certificate for the DNS names provided–enabling access to the container via HTTPS. | internal_http_port – Required. This is the port exposed by the container to serve HTTP requests. The load balancer listens on port 80 and 443 to forward ports to the given port on the container. | internal_http_health_check_path – Required. This is a URL path, like /robots.txt, that the Application Load Balancer (ALB) checks to determine whether the container is healthy. This path must return a 200 OK If the ALB decides a container is unhealthy, it will be removed from routing. | internal_http_health_check_timeout – Optional. This sets the timeout for the HTTP requests that the Application Load Balancer (ALB)’s health checks. If this field is omitted, it defaults to 5 seconds. | stickiness_cookie_duration_seconds – Optional. If this value is present, it enables stickiness on the Application Load Balancer. Stickiness is the mechanism for client requests to consistently be routed to the same container instance behind the Application Load Balancer (ALB). The ALB sets an HTTP cookie for the first client request it receives, and then checks for the cookie on subsequent requests. The cookie eventually expires, and this value sets the expiration for the cookie–in seconds. | . | . | s3_buckets – Optional. This is a mapping of S3 buckets to the classes of permissions available to the instances. The four classes of permissions available are list, get, put, and delete, and the values for each one is true or false. To use this configuration, place the s3_buckets key inside a block that defines a container. Below is an example of how to give the container access to two buckets–one with list and get permissions, and another with get and delete permissions. | . s3_buckets = { \"some-bucket-name.example-internal.com\" = { permissions = { list = true get = true put = false delete = false } } \"another-bucket.com\" = { permissions = { list = false get = true put = false delete = true } } } . ",
    "url": "/v3.0/reference/containers/#inputs",
    "relUrl": "/v3.0/reference/containers/#inputs"
  },"37": {
    "doc": "containers",
    "title": "Outputs",
    "content": "Provose provides these Terraform outputs to enable users to patch certain advanced configurations that cannot be configured via the inputs. | containers.aws_security_group.container__internal_http_port – A mapping between container names and the aws_security_group resources that govern network access to each container. | containers.aws_ecs_cluster.container – This is a mapping between container names and their corresponding aws_ecs_cluster resources. Provose gives every container definition its own cluster. | containers.aws_ecs_task_definition.container – A mapping between container names and their corresponding aws_ecs_task_definition resourcehttps://www.terraform.io/docs/providers/aws/r/ecs_task_definition.html. | containers.aws_lb_target_group.container__public_https – A mapping between container names and the containers’ corresponding aws_lb_target_group resources. | containers.aws_lb_target_group.container__vpc_https – These are aws_lb_target_group resources for each container that give HTTPS access from within a VPC. | containers.aws_lb_listener_rule.container__public_https – These are aws_lb_listener_rule resources for each container that give public HTTPS access. | containers.aws_ecs_service.container – A mapping of aws_ecs_service resources for each container name. In Provose, each container only belongs to one service. | containers.aws_instance.container__instance – A mapping of aws_instance resources being used to host containers. This will be empty if you are running all of your containers with AWS Fargate as opposed to using EC2 hosts. | containers.aws_route53_record.container__instance – A mapping of aws_route53_record resources for EC2 instances hosting containers. This will be empty if you are running all of your containers with AWS Fargate as opposed to EC2. | containers.aws_route53_record.container__public_https – A mapping of aws_route53_record resources that point to public HTTPS websites served by containers. | containers.aws_route53_record.container__public_https_validation – A mapping of aws_route53_record resources that were created specifically to validate the Amazon Certificate Manager certificates for domain names that we point a container to. | containers.aws_route53_record.container__vpc_https – A mapping of aws_route53_record resources created for DNS resolution to HTTPS websites within the VPC. | containers.aws_route53_record.container__instance_spot – A mapping of aws_route53_record resources for container spot instances. | containers.aws_route53_zone.external_dns__for_containers – A mapping from container names to aws_route53_zone resources describing the Route 53 zones that contain the domain names that serve traffic to each container. | containers.aws_acm_certificate.container__public_https – A mapping of aws_acm_certificate resources representing the TLS certificates that communicate connections to the container from the public Internet. | containers.aws_acm_certificate_validation.container__public_https_validation – A mapping of aws_acm_certificate_validation resources. Use this to debug issues with Amazon Certificate Manager (ACM) certificate validation. | . ",
    "url": "/v3.0/reference/containers/#outputs",
    "relUrl": "/v3.0/reference/containers/#outputs"
  },"38": {
    "doc": "containers",
    "title": "Implementation details",
    "content": "Container networking modes . Containers launched with the \"FARGATE\" instance type are launched with the \"awsvpc\" container networking mode. Containers launched with one of the EC2 instance types are launched with the \"bridge\" container networking mode. This is because ECS EC2 containers do not have public Internet access unless the containers are both launched in a private subnet and connected to the Internet through a NAT gateway. The NAT gateway costs additional money, so we can save money by using the \"bridge\" networking mode instead. This, according to Amazon, is slightly less performant, but the faster \"awsvpc\" networking mode is still available for Fargate containers through Provose. ",
    "url": "/v3.0/reference/containers/#implementation-details",
    "relUrl": "/v3.0/reference/containers/#implementation-details"
  },"39": {
    "doc": "Contributing to Provose",
    "title": "Contributing to Provose",
    "content": "Provose welcomes contributions to code and documentation from the community. However, there are some ground rules. ",
    "url": "/contributing/",
    "relUrl": "/contributing/"
  },"40": {
    "doc": "Contributing to Provose",
    "title": "Your contributions owned by Neocrym Records Inc.",
    "content": "All of Provose’s code and documentation is copyrighted by Neocrym Records Inc and licensed under the MIT license. The same must be true for your contribution. Currently, Provose does not require the signing of Contributor Licence Agreements (CLAs), but this may change in the future. ",
    "url": "/contributing/#your-contributions-owned-by-neocrym-records-inc",
    "relUrl": "/contributing/#your-contributions-owned-by-neocrym-records-inc"
  },"41": {
    "doc": "Contributing to Provose",
    "title": "Terraform resources are named after the file they are contained in.",
    "content": "For example, if we have a file named redis_clusters.tf, we think Terraform resources should look like . resource \"aws_security_group\" \"redis_clusters\" { # content goes here } . When there are multiple resources that would have conflicting names, give them different name after a double underscore. resource \"aws_security_group\" \"redis_clusters__sg1\" { # content goes here } resource \"aws_security_group\" \"redis_clusters__sg2 { # more content goes here } . ",
    "url": "/contributing/#terraform-resources-are-named-after-the-file-they-are-contained-in",
    "relUrl": "/contributing/#terraform-resources-are-named-after-the-file-they-are-contained-in"
  },"42": {
    "doc": "ebs_volumes",
    "title": "ebs_volumes",
    "content": " ",
    "url": "/v4.0/reference/ebs_volumes/",
    "relUrl": "/v4.0/reference/ebs_volumes/"
  },"43": {
    "doc": "ebs_volumes",
    "title": "Description",
    "content": "The Provose ebs_volumes module configures Elastic Block Storage (EBS) volumes that exist independently of any EC2 instance or ECS container. Containers and EC2 instances come with their own root volumes, but those volumes will not persist of the container or instance are destroyed and recreated. The ebs_volumes module is a great way to set up filesystems that will continue to exist even when the EC2 instance mounting the filesystem is destroyed. ",
    "url": "/v4.0/reference/ebs_volumes/#description",
    "relUrl": "/v4.0/reference/ebs_volumes/#description"
  },"44": {
    "doc": "ebs_volumes",
    "title": "Examples",
    "content": "Creating two EBS volumes in different availability zones. module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ebs_volumes = { some_volume = { availability_zone = \"us-east-1c\" size_gb = 100 } provisioned_volume = { availability_zone = \"us-east-1a\" size_gb = 50 iops = 2500 type = \"io1\" encrypted = true } } } . ",
    "url": "/v4.0/reference/ebs_volumes/#examples",
    "relUrl": "/v4.0/reference/ebs_volumes/#examples"
  },"45": {
    "doc": "ebs_volumes",
    "title": "Inputs",
    "content": ". | availability_zone – Required. The AWS availability zone to place the EBS volume. Keep in mind that the EC2 instance that mounts this EBS volume must be in the same availability zone. | size_gb – Required. The size of the EBS volume in gigabytes. | type – Optional. The type of the EBS volume. This defaults to \"gp2\", but can also be \"standard\", \"io1\", \"sc1\", or \"st1\". The AWS documentation describes in detail what the different EBS volume types mean. | iops – Optional. The number of I/O Operations Per Second (IOS) to provision for the disk. | encrypted – Optional. Defaults to false. If set to true, this enables encryption at rest for the EBS volume. | kms_key_id – Optional. The Amazon Resource Name (ARN) for an AWS Key Management Service (KMS) key to use when encrypted is set to true. | . ",
    "url": "/v4.0/reference/ebs_volumes/#inputs",
    "relUrl": "/v4.0/reference/ebs_volumes/#inputs"
  },"46": {
    "doc": "ebs_volumes",
    "title": "Outputs",
    "content": ". | ebs_volumes.aws_ebs_volume.ebs_volume – This is the underlying aws_ebs_volume resource. | . ",
    "url": "/v4.0/reference/ebs_volumes/#outputs",
    "relUrl": "/v4.0/reference/ebs_volumes/#outputs"
  },"47": {
    "doc": "ebs_volumes",
    "title": "ebs_volumes",
    "content": " ",
    "url": "/v3.0/reference/ebs_volumes/",
    "relUrl": "/v3.0/reference/ebs_volumes/"
  },"48": {
    "doc": "ebs_volumes",
    "title": "Description",
    "content": "The Provose ebs_volumes module configures Elastic Block Storage (EBS) volumes that exist independently of any EC2 instance or ECS container. Containers and EC2 instances come with their own root volumes, but those volumes will not persist of the container or instance are destroyed and recreated. The ebs_volumes module is a great way to set up filesystems that will continue to exist even when the EC2 instance mounting the filesystem is destroyed. ",
    "url": "/v3.0/reference/ebs_volumes/#description",
    "relUrl": "/v3.0/reference/ebs_volumes/#description"
  },"49": {
    "doc": "ebs_volumes",
    "title": "Examples",
    "content": "Creating two EBS volumes in different availability zones. module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ebs_volumes = { some_volume = { availability_zone = \"us-east-1c\" size_gb = 100 } provisioned_volume = { availability_zone = \"us-east-1a\" size_gb = 50 iops = 2500 type = \"io1\" encrypted = true } } } . ",
    "url": "/v3.0/reference/ebs_volumes/#examples",
    "relUrl": "/v3.0/reference/ebs_volumes/#examples"
  },"50": {
    "doc": "ebs_volumes",
    "title": "Inputs",
    "content": ". | availability_zone – Required. The AWS availability zone to place the EBS volume. Keep in mind that the EC2 instance that mounts this EBS volume must be in the same availability zone. | size_gb – Required. The size of the EBS volume in gigabytes. | type – Optional. The type of the EBS volume. This defaults to \"gp2\", but can also be \"standard\", \"io1\", \"sc1\", or \"st1\". The AWS documentation describes in detail what the different EBS volume types mean. | iops – Optional. The number of I/O Operations Per Second (IOS) to provision for the disk. | encrypted – Optional. Defaults to false. If set to true, this enables encryption at rest for the EBS volume. | kms_key_id – Optional. The Amazon Resource Name (ARN) for an AWS Key Management Service (KMS) key to use when encrypted is set to true. | . ",
    "url": "/v3.0/reference/ebs_volumes/#inputs",
    "relUrl": "/v3.0/reference/ebs_volumes/#inputs"
  },"51": {
    "doc": "ebs_volumes",
    "title": "Outputs",
    "content": ". | ebs_volumes.aws_ebs_volume.ebs_volume – This is the underlying aws_ebs_volume resource. | . ",
    "url": "/v3.0/reference/ebs_volumes/#outputs",
    "relUrl": "/v3.0/reference/ebs_volumes/#outputs"
  },"52": {
    "doc": "ebs_volumes",
    "title": "ebs_volumes",
    "content": " ",
    "url": "/v2.0/reference/ebs_volumes/",
    "relUrl": "/v2.0/reference/ebs_volumes/"
  },"53": {
    "doc": "ebs_volumes",
    "title": "Description",
    "content": "The Provose ebs_volumes module configures Elastic Block Storage (EBS) volumes that exist independently of any EC2 instance or ECS container. Containers and EC2 instances come with their own root volumes, but those volumes will not persist of the container or instance are destroyed and recreated. The ebs_volumes module is a great way to set up filesystems that will continue to exist even when the EC2 instance mounting the filesystem is destroyed. ",
    "url": "/v2.0/reference/ebs_volumes/#description",
    "relUrl": "/v2.0/reference/ebs_volumes/#description"
  },"54": {
    "doc": "ebs_volumes",
    "title": "Examples",
    "content": "Creating two EBS volumes in different availability zones. module \"myproject\" { source = \"github.com/provose/provose?ref=v2.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ebs_volumes = { some_volume = { availability_zone = \"us-east-1c\" size_gb = 100 } provisioned_volume = { availability_zone = \"us-east-1a\" size_gb = 50 iops = 2500 type = \"io1\" encrypted = true } } } . ",
    "url": "/v2.0/reference/ebs_volumes/#examples",
    "relUrl": "/v2.0/reference/ebs_volumes/#examples"
  },"55": {
    "doc": "ebs_volumes",
    "title": "Inputs",
    "content": ". | availability_zone – Required. The AWS availability zone to place the EBS volume. Keep in mind that the EC2 instance that mounts this EBS volume must be in the same availability zone. | size_gb – Required. The size of the EBS volume in gigabytes. | type – Optional. The type of the EBS volume. This defaults to \"gp2\", but can also be \"standard\", \"io1\", \"sc1\", or \"st1\". The AWS documentation describes in detail what the different EBS volume types mean. | iops – Optional. The number of I/O Operations Per Second (IOS) to provision for the disk. | encrypted – Optional. Defaults to false. If set to true, this enables encryption at rest for the EBS volume. | kms_key_id – Optional. The Amazon Resource Name (ARN) for an AWS Key Management Service (KMS) key to use when encrypted is set to true. | . ",
    "url": "/v2.0/reference/ebs_volumes/#inputs",
    "relUrl": "/v2.0/reference/ebs_volumes/#inputs"
  },"56": {
    "doc": "ebs_volumes",
    "title": "Outputs",
    "content": ". | ebs_volumes.aws_ebs_volume.ebs_volume – This is the underlying aws_ebs_volume resource. | . ",
    "url": "/v2.0/reference/ebs_volumes/#outputs",
    "relUrl": "/v2.0/reference/ebs_volumes/#outputs"
  },"57": {
    "doc": "ebs_volumes",
    "title": "ebs_volumes",
    "content": " ",
    "url": "/v1.1/reference/ebs_volumes/",
    "relUrl": "/v1.1/reference/ebs_volumes/"
  },"58": {
    "doc": "ebs_volumes",
    "title": "Description",
    "content": "The Provose ebs_volumes module configures Elastic Block Storage (EBS) volumes that exist independently of any EC2 instance or ECS container. Containers and EC2 instances come with their own root volumes, but those volumes will not persist of the container or instance are destroyed and recreated. The ebs_volumes module is a great way to set up filesystems that will continue to exist even when the EC2 instance mounting the filesystem is destroyed. ",
    "url": "/v1.1/reference/ebs_volumes/#description",
    "relUrl": "/v1.1/reference/ebs_volumes/#description"
  },"59": {
    "doc": "ebs_volumes",
    "title": "Examples",
    "content": "Creating two EBS volumes in different availability zones. module \"myproject\" { source = \"github.com/provose/provose?ref=v1.1.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ebs_volumes = { some_volume = { availability_zone = \"us-east-1c\" size_gb = 100 } provisioned_volume = { availability_zone = \"us-east-1a\" size_gb = 50 iops = 2500 type = \"io1\" encrypted = true } } } . ",
    "url": "/v1.1/reference/ebs_volumes/#examples",
    "relUrl": "/v1.1/reference/ebs_volumes/#examples"
  },"60": {
    "doc": "ebs_volumes",
    "title": "Inputs",
    "content": ". | availability_zone – Required. The AWS availability zone to place the EBS volume. Keep in mind that the EC2 instance that mounts this EBS volume must be in the same availability zone. | size_gb – Required. The size of the EBS volume in gigabytes. | type – Optional. The type of the EBS volume. This defaults to \"gp2\", but can also be \"standard\", \"io1\", \"sc1\", or \"st1\". The AWS documentation describes in detail what the different EBS volume types mean. | iops – Optional. The number of I/O Operations Per Second (IOS) to provision for the disk. | encrypted – Optional. Defaults to false. If set to true, this enables encryption at rest for the EBS volume. | kms_key_id – Optional. The Amazon Resource Name (ARN) for an AWS Key Management Service (KMS) key to use when encrypted is set to true. | . ",
    "url": "/v1.1/reference/ebs_volumes/#inputs",
    "relUrl": "/v1.1/reference/ebs_volumes/#inputs"
  },"61": {
    "doc": "ebs_volumes",
    "title": "Outputs",
    "content": ". | ebs_volumes.aws_ebs_volume.ebs_volume – This is the underlying aws_ebs_volume resource. | . ",
    "url": "/v1.1/reference/ebs_volumes/#outputs",
    "relUrl": "/v1.1/reference/ebs_volumes/#outputs"
  },"62": {
    "doc": "ec2_on_demand_instances",
    "title": "ec2_on_demand_instances",
    "content": " ",
    "url": "/v3.0/reference/ec2_on_demand_instances/",
    "relUrl": "/v3.0/reference/ec2_on_demand_instances/"
  },"63": {
    "doc": "ec2_on_demand_instances",
    "title": "Description",
    "content": "The Provose ec2_on_demand_instances module enables the creation and deployment of Amazon EC2 instances. The phrase “On Demand” means these are regular ol’ AWS instances that are billed at the same price per-second. This is in contrast to Spot instances–where the price can fluctuate over time–or Reserved Instances–which are partially or completely purchased upfront. If you are looking to deploy the same application across multiple services–perhaps behind an HTTP load balancer–you might find it handier to use the containers module instead. ",
    "url": "/v3.0/reference/ec2_on_demand_instances/#description",
    "relUrl": "/v3.0/reference/ec2_on_demand_instances/#description"
  },"64": {
    "doc": "ec2_on_demand_instances",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ec2_on_demand_instances = { my-instance-name = { instances = { instance_type = \"t3.micro\" # Create and name keys pairs in the AWS console. You can attach # one key pair to an instance on creation, but then you can add # more SSH keys via the user's ~/.ssh/authorized_keys file. key_name = \"james_laptop\" } public = { # Open port 22 for public SSH access. tcp = [22] } vpc = { # Open ports 80 and 443 to run HTTP and HTTPS servers only # available in the VPC. tcp = [80, 443] } } } } . ",
    "url": "/v3.0/reference/ec2_on_demand_instances/#examples",
    "relUrl": "/v3.0/reference/ec2_on_demand_instances/#examples"
  },"65": {
    "doc": "ec2_on_demand_instances",
    "title": "Inputs",
    "content": ". | instances – Required. This object contains various meta-settings about the AWS instance. | instance_type – Required. The instance type. | instance_count – Optional. The number of instances to deploy. This defaults to 1. If you deploy one instance named bob, then it will be named bob in the AWS console and Provose creates a DNS record for your internal subdomain named bob. If you set instance_count to be greater than one, then the instances will be bob-1, bob-2, and so forth. | key_name – Optional. The name of the AWS key pair. | ami_id – Optional. The ID of the Amazon Machine Image (AMI) to deploy for this instance. By default, Provose will deploy an ECS-optimized GPU-ready Amazon Linux 2 AMI–specifically amzn2-ami-ecs-gpu-hvm-2.0.20200218-x86_64-ebs. New users of Provose might want to choose a newer AMI, but Provose cannot update the default AMI for existing users without causing their existing instances to be destroyed and recreated. | availability_zone – Optional. Set this to a specific Availability Zone in your AWS Region if you have a preference for what availability zone to deploy your instance in. | bash_user_data – Optional. This is a user data script–a Bash script that will be run on the EC2 instance’s creation. This script will not be rerun when the instance reboots. Provose currently does not support the cloud-init standard for user data. | . | public – Optional. This is a grouping for network settings for the public Internet. | tcp Optional. This is a list of TCP ports to open to the public Internet. | udp Optional. This is a list of UDP ports to open to the public Internet. | . | vpc – Optional. This is a grouping for network settings only within the Virtual Private Cloud (VPC) that Provose creates. | secrets – Optional. This is a list of AWS Secrets Manager secret names that this EC2 instance should have access to. This setting only configures access. You have to fetch the secrets yourself in your application with the AWS API. The secrets key in the containers module goes a step further and loads your secrets as environment variables. | associate_public_ip_address – Optional. Defaults to true, which provisions a public IPv4 address for this instance. However, it will not be possible to make inbound requests to the instance using this IP address unless you also choose to open TCP ports with the public.tcp key or the UDP ports with the public.udp key. The public IP address that AWS gives this instance should be considered temporary. If you want a more permanent IP address, you should provision an Elastic IP and assign it to this instance. | vpc_security_group_ids – Optional. This key is for adding additional, custom security groups in addition to what Provose sets up from the public and vpc keys. You may want to add a custom security group with a more specific CIDR. | root_block_device – Optional These are optional settings about the Elastic Block Storage (EBS) volume that stores the root filesystem for this EC2 instance. | volume_type – Optional. This is the type of EBS volume. Values can be either \"standard\", \"gp2\", \"io1\", \"sc1\", or \"st1\", with \"standard\" being the default. | volume_size_gb – Optional. This is the size of the EBS volume in gigabytes. This defaults to the root volume size defined in the underlying Amazon Machine Image (AMI) and will never be less than the minimum. | delete_on_termination – Optional. This defaults to true, which deletes the EBS volume if the instance is terminated. Set this to false to keep the root EBS volume in your account after instance termination. | encrypted – Optional. Set this to true to encrypt the EBS volume. This value is false by default. | kms_key_id – Optional. This is the Amazon Resource Name (ARN) for the custom AWS Key Management Service (KMS) key that you would like to use to encrypt the drive. | . | s3_buckets – Optional. This is a mapping of S3 buckets to the classes of permissions available to the instances. The four classes of permissions available are list, get, put, and delete, and the values for each one is true or false. To use this configuration, place the s3_buckets key inside a block that defines an instance. Below is an example of how to give an EC2 instance access to two buckets–one with list and get permissions, and another with get and delete permissions. | . s3_buckets = { \"some-bucket-name.example-internal.com\" = { permissions = { list = true get = true put = false delete = false } } \"another-bucket.com\" = { permissions = { list = false get = true put = false delete = true } } } . ",
    "url": "/v3.0/reference/ec2_on_demand_instances/#inputs",
    "relUrl": "/v3.0/reference/ec2_on_demand_instances/#inputs"
  },"66": {
    "doc": "ec2_on_demand_instances",
    "title": "Outputs",
    "content": ". | ec2_on_demand_instances.aws_security_group.ec2_on_demand_instances – A map with a key for every instance and every value is a Terraform aws_security_group type. | ec2_on_demand_instances.aws_instance.ec2_on_demand_instances – A map with the keys as the names of the on-demand instances–dashed with a number if we set the instances.instance_count parameter to be greater than 1. Each value is a Terraform aws_instance type. | ec2_on_demand_instances.aws_route53_record.ec2_on_demand_instances – This is a mapping from the names EC2 On-Demand instances to the aws_route53_record resource that describes the DNS records internal to the VPC. | . ",
    "url": "/v3.0/reference/ec2_on_demand_instances/#outputs",
    "relUrl": "/v3.0/reference/ec2_on_demand_instances/#outputs"
  },"67": {
    "doc": "ec2_on_demand_instances",
    "title": "ec2_on_demand_instances",
    "content": " ",
    "url": "/v4.0/reference/ec2_on_demand_instances/",
    "relUrl": "/v4.0/reference/ec2_on_demand_instances/"
  },"68": {
    "doc": "ec2_on_demand_instances",
    "title": "Description",
    "content": "The Provose ec2_on_demand_instances module enables the creation and deployment of Amazon EC2 instances. The phrase “On Demand” means these are regular ol’ AWS instances that are billed at the same price per-second. This is in contrast to Spot instances–where the price can fluctuate over time–or Reserved Instances–which are partially or completely purchased upfront. If you are looking to deploy the same application across multiple services–perhaps behind an HTTP load balancer–you might find it handier to use the containers module instead. ",
    "url": "/v4.0/reference/ec2_on_demand_instances/#description",
    "relUrl": "/v4.0/reference/ec2_on_demand_instances/#description"
  },"69": {
    "doc": "ec2_on_demand_instances",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ec2_on_demand_instances = { my-instance-name = { instances = { instance_type = \"t3.micro\" # Create and name keys pairs in the AWS console. You can attach # one key pair to an instance on creation, but then you can add # more SSH keys via the user's ~/.ssh/authorized_keys file. key_name = \"james_laptop\" } public = { # Open port 22 for public SSH access. tcp = [22] } vpc = { # Open ports 80 and 443 to run HTTP and HTTPS servers only # available in the VPC. tcp = [80, 443] } } } } . ",
    "url": "/v4.0/reference/ec2_on_demand_instances/#examples",
    "relUrl": "/v4.0/reference/ec2_on_demand_instances/#examples"
  },"70": {
    "doc": "ec2_on_demand_instances",
    "title": "Inputs",
    "content": ". | instances – Required. This object contains various meta-settings about the AWS instance. | instance_type – Required. The instance type. | ami_id – Required. The ID of the Amazon Machine Image (AMI) to deploy for this instance. | instance_count – Optional. The number of instances to deploy. This defaults to 1. If you deploy one instance named bob, then it will be named bob in the AWS console and Provose creates a DNS record for your internal subdomain named bob. If you set instance_count to be greater than one, then the instances will be bob-1, bob-2, and so forth. | key_name – Optional. The name of the AWS key pair. | availability_zone – Optional. Set this to a specific Availability Zone in your AWS Region if you have a preference for what availability zone to deploy your instance in. | bash_user_data – Optional. This is a user data script–a Bash script that will be run on the EC2 instance’s creation. This script will not be rerun when the instance reboots. Provose currently does not support the cloud-init standard for user data. | . | public – Optional. This is a grouping for network settings for the public Internet. | tcp Optional. This is a list of TCP ports to open to the public Internet. | udp Optional. This is a list of UDP ports to open to the public Internet. | . | vpc – Optional. This is a grouping for network settings only within the Virtual Private Cloud (VPC) that Provose creates. | secrets – Optional. This is a list of AWS Secrets Manager secret names that this EC2 instance should have access to. This setting only configures access. You have to fetch the secrets yourself in your application with the AWS API. The secrets key in the containers module goes a step further and loads your secrets as environment variables. | associate_public_ip_address – Optional. Defaults to true, which provisions a public IPv4 address for this instance. However, it will not be possible to make inbound requests to the instance using this IP address unless you also choose to open TCP ports with the public.tcp key or the UDP ports with the public.udp key. The public IP address that AWS gives this instance should be considered temporary. If you want a more permanent IP address, you should provision an Elastic IP and assign it to this instance. | vpc_security_group_ids – Optional. This key is for adding additional, custom security groups in addition to what Provose sets up from the public and vpc keys. You may want to add a custom security group with a more specific CIDR. | root_block_device – Optional These are optional settings about the Elastic Block Storage (EBS) volume that stores the root filesystem for this EC2 instance. | volume_size_gb – Required. This is the size of the root EBS volume in gigabytes. | volume_type – Optional. This is the type of EBS volume. Values can be either \"standard\", \"gp2\", \"io1\", \"sc1\", or \"st1\", with \"standard\" being the default. | delete_on_termination – Optional. This defaults to true, which deletes the EBS volume if the instance is terminated. Set this to false to keep the root EBS volume in your account after instance termination. | encrypted – Optional. Set this to true to encrypt the EBS volume. This value is false by default. | kms_key_id – Optional. This is the Amazon Resource Name (ARN) for the custom AWS Key Management Service (KMS) key that you would like to use to encrypt the drive. | . | s3_buckets – Optional. This is a mapping of S3 buckets to the classes of permissions available to the instances. The four classes of permissions available are list, get, put, and delete, and the values for each one is true or false. To use this configuration, place the s3_buckets key inside a block that defines an instance. Below is an example of how to give an EC2 instance access to two buckets–one with list and get permissions, and another with get and delete permissions. | . s3_buckets = { \"some-bucket-name.example-internal.com\" = { permissions = { list = true get = true put = false delete = false } } \"another-bucket.com\" = { permissions = { list = false get = true put = false delete = true } } } . ",
    "url": "/v4.0/reference/ec2_on_demand_instances/#inputs",
    "relUrl": "/v4.0/reference/ec2_on_demand_instances/#inputs"
  },"71": {
    "doc": "ec2_on_demand_instances",
    "title": "Outputs",
    "content": ". | ec2_on_demand_instances.aws_security_group.ec2_on_demand_instances – A map with a key for every instance and every value is a Terraform aws_security_group type. | ec2_on_demand_instances.aws_instance.ec2_on_demand_instances – A map with the keys as the names of the on-demand instances–dashed with a number if we set the instances.instance_count parameter to be greater than 1. Each value is a Terraform aws_instance type. | ec2_on_demand_instances.aws_route53_record.ec2_on_demand_instances – This is a mapping from the names EC2 On-Demand instances to the aws_route53_record resource that describes the DNS records internal to the VPC. | . ",
    "url": "/v4.0/reference/ec2_on_demand_instances/#outputs",
    "relUrl": "/v4.0/reference/ec2_on_demand_instances/#outputs"
  },"72": {
    "doc": "ec2_spot_instances",
    "title": "ec2_spot_instances",
    "content": " ",
    "url": "/v3.0/reference/ec2_spot_instances/",
    "relUrl": "/v3.0/reference/ec2_spot_instances/"
  },"73": {
    "doc": "ec2_spot_instances",
    "title": "Description",
    "content": "The Provose ec2_spot_instances module enables the creation and deployment of Amazon EC2 Spot instances. How Spot instances work . The AWS documentation has more information about how Spot instances work. Unlike other AWS resources that Provose can provision for you, a Spot instance is not guaranteed to be provisioned and may be shut down by Amazon at any time. Should you use ec2_spot_instances? . If your application cannot tolerate the periodic shutdown of a Spot instance, you can provision an EC2 On-Demand instance instead with the ec2_on_demand_instances module instead. If you are looking to deploy the same application across multiple services–perhaps behind an HTTP load balancer–you might find it handier to use the containers module instead. You can get Spot pricing through the containers module by setting your instance_type to be \"FARGATE_SPOT\". That does not work for this module, however. ",
    "url": "/v3.0/reference/ec2_spot_instances/#description",
    "relUrl": "/v3.0/reference/ec2_spot_instances/#description"
  },"74": {
    "doc": "ec2_spot_instances",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ec2_spot_instances = { my-instance-name = { spot_price = 0.1 instances = { instance_type = \"t3.micro\" # Create and name keys pairs in the AWS console. You can attach # one key pair to an instance on creation, but then you can add # more SSH keys via the user's ~/.ssh/authorized_keys file. key_name = \"james_laptop\" } public = { # Open port 22 for public SSH access. tcp = [22] } vpc = { # Open ports 80 and 443 to run HTTP and HTTPS servers only # available in the VPC. tcp = [80, 443] } } } } . ",
    "url": "/v3.0/reference/ec2_spot_instances/#examples",
    "relUrl": "/v3.0/reference/ec2_spot_instances/#examples"
  },"75": {
    "doc": "ec2_spot_instances",
    "title": "Inputs",
    "content": "The ec2_spot_instances module supports all of the same configuration key as the ec2_on_demand_instances module, plus the following additional keys: . | spot_price – Optional. The maximum price to request in the Spot market. If left blank, your maximum price will default to the On-Demand price. However, this does not mean that you pay the On-Demand price. | spot_type – Optional. This value defaults to \"persistent\", which means that AWS will resubmit the Spot request if the instance is terminated. You can set this value to \"one-time\" to close the Spot request when the instance is closed. | . ",
    "url": "/v3.0/reference/ec2_spot_instances/#inputs",
    "relUrl": "/v3.0/reference/ec2_spot_instances/#inputs"
  },"76": {
    "doc": "ec2_spot_instances",
    "title": "Outputs",
    "content": ". | ec2_spot_instances.aws_security_group.ec2_spot_instances – A map with a key for every instance and every value is a Terraform aws_security_group type. | ec2_spot_instances.aws_spot_instance_request.ec2_spot_instances – A map with the keys as the names of the Spot instances–dashed with a number if we set the instances.instance_count parameter to be greater than 1. Each value is a Terraform aws_spot_instance_request type. | ec2_spot_instances.aws_route53_record.ec2_spot_instances – This is a mapping from the names EC2 Spot instances to the aws_route53_record resource that describes the DNS records internal to the VPC. | . ",
    "url": "/v3.0/reference/ec2_spot_instances/#outputs",
    "relUrl": "/v3.0/reference/ec2_spot_instances/#outputs"
  },"77": {
    "doc": "ec2_spot_instances",
    "title": "ec2_spot_instances",
    "content": " ",
    "url": "/v4.0/reference/ec2_spot_instances/",
    "relUrl": "/v4.0/reference/ec2_spot_instances/"
  },"78": {
    "doc": "ec2_spot_instances",
    "title": "Description",
    "content": "The Provose ec2_spot_instances module enables the creation and deployment of Amazon EC2 Spot instances. How Spot instances work . The AWS documentation has more information about how Spot instances work. Unlike other AWS resources that Provose can provision for you, a Spot instance is not guaranteed to be provisioned and may be shut down by Amazon at any time. Should you use ec2_spot_instances? . If your application cannot tolerate the periodic shutdown of a Spot instance, you can provision an EC2 On-Demand instance instead with the ec2_on_demand_instances module instead. If you are looking to deploy the same application across multiple services–perhaps behind an HTTP load balancer–you might find it handier to use the containers module instead. You can get Spot pricing through the containers module by setting your instance_type to be \"FARGATE_SPOT\". That does not work for this module, however. ",
    "url": "/v4.0/reference/ec2_spot_instances/#description",
    "relUrl": "/v4.0/reference/ec2_spot_instances/#description"
  },"79": {
    "doc": "ec2_spot_instances",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } ec2_spot_instances = { my-instance-name = { spot_price = 0.1 instances = { instance_type = \"t3.micro\" # Create and name keys pairs in the AWS console. You can attach # one key pair to an instance on creation, but then you can add # more SSH keys via the user's ~/.ssh/authorized_keys file. key_name = \"james_laptop\" } public = { # Open port 22 for public SSH access. tcp = [22] } vpc = { # Open ports 80 and 443 to run HTTP and HTTPS servers only # available in the VPC. tcp = [80, 443] } } } } . ",
    "url": "/v4.0/reference/ec2_spot_instances/#examples",
    "relUrl": "/v4.0/reference/ec2_spot_instances/#examples"
  },"80": {
    "doc": "ec2_spot_instances",
    "title": "Inputs",
    "content": "The ec2_spot_instances module supports all of the same configuration key as the ec2_on_demand_instances module, plus the following additional keys: . | spot_price – Optional. The maximum price to request in the Spot market. If left blank, your maximum price will default to the On-Demand price. However, this does not mean that you pay the On-Demand price. | spot_type – Optional. This value defaults to \"persistent\", which means that AWS will resubmit the Spot request if the instance is terminated. You can set this value to \"one-time\" to close the Spot request when the instance is closed. | . ",
    "url": "/v4.0/reference/ec2_spot_instances/#inputs",
    "relUrl": "/v4.0/reference/ec2_spot_instances/#inputs"
  },"81": {
    "doc": "ec2_spot_instances",
    "title": "Outputs",
    "content": ". | ec2_spot_instances.aws_security_group.ec2_spot_instances – A map with a key for every instance and every value is a Terraform aws_security_group type. | ec2_spot_instances.aws_spot_instance_request.ec2_spot_instances – A map with the keys as the names of the Spot instances–dashed with a number if we set the instances.instance_count parameter to be greater than 1. Each value is a Terraform aws_spot_instance_request type. | ec2_spot_instances.aws_route53_record.ec2_spot_instances – This is a mapping from the names EC2 Spot instances to the aws_route53_record resource that describes the DNS records internal to the VPC. | . ",
    "url": "/v4.0/reference/ec2_spot_instances/#outputs",
    "relUrl": "/v4.0/reference/ec2_spot_instances/#outputs"
  },"82": {
    "doc": "elastic_file_systems",
    "title": "elastic_file_systems",
    "content": " ",
    "url": "/v4.0/reference/elastic_file_systems/",
    "relUrl": "/v4.0/reference/elastic_file_systems/"
  },"83": {
    "doc": "elastic_file_systems",
    "title": "Description",
    "content": "The Provose elastic_file_systems module is used to configure AWS Elastic File Systems . Mounting a file system . Provose helps you create Elastic File Systems and makes them accessible within your Virtual Private Cloud (VPC), but you must take manual steps to mount the Elastic File Systems that you create. Read the EFS documentation for more information. ",
    "url": "/v4.0/reference/elastic_file_systems/#description",
    "relUrl": "/v4.0/reference/elastic_file_systems/#description"
  },"84": {
    "doc": "elastic_file_systems",
    "title": "Examples",
    "content": "The simplest example . This creates an unencrypted filesystem, defaulting to \"bursting\" as the throughput mode and with the performance mode being \"generalPurpose\". module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } elastic_file_systems = { # This creates an Elastic File system at the domain name # myfiles.myproject.example-internal.com, which is only accessible # by the VPC created by the \"myproject\" Provose module. \"myfiles\" = {} } } . ",
    "url": "/v4.0/reference/elastic_file_systems/#examples",
    "relUrl": "/v4.0/reference/elastic_file_systems/#examples"
  },"85": {
    "doc": "elastic_file_systems",
    "title": "Inputs",
    "content": "Currently, there are no additional inputs for creating Elastic File Systems. Currently, there are no required parameters for creating Elastic File Systems. Below are optional parameters for tuning performance or security. Beware that changing any of these parameters for an already-existing filesystem may cause Terraform to delete and recreate your filesystem. | performance_mode – Optional. This is the performance mode for the filesystem. This defaults to the value \"generalPurpose\" but can be set to \"maxIO\" for better performance. | throughput_mode – Optional. This sets the throughput mode for the filesytem. The default value is \"bursting\" which provides additional IOPS for this filesystem in bursts. To provision IOPS for this filesystem, set it to \"provisioned\". | provisioned_throughput_in_mib_per_second – Optional. When the throughput_mode is set to \"provisioned\", use this input to set how much throughput to provision in mebibytes per second. | encrypted – Optional. Set to true to encrypt the filesystem at rest. The default is false. | kms_key_id – Optional. The ID for the AWS Key Management System key used to encrypt the filesystem. Only use this parameter when you are setting encrypted to true. | . ",
    "url": "/v4.0/reference/elastic_file_systems/#inputs",
    "relUrl": "/v4.0/reference/elastic_file_systems/#inputs"
  },"86": {
    "doc": "elastic_file_systems",
    "title": "Outputs",
    "content": ". | elastic_file_systems.aws_security_group.elastic_file_systems – The aws_security_group resource that allows access to Elastic File Systems within the VPC. | elastic_file_systems.aws_efs_file_system.elastic_file_systems – The aws_efs_file_system resource. | elastic_file_systems.aws_efs_mount_target.elastic_file_systems – The aws_efs_mount_target resource for the filesystem. We create a mount target for every subnet in the VPC containing the Elastic File System. | elastic_file_systems.aws_route53_record.elastic_file_systems – The aws_route53_record resource for the filesystem. EFS creates hard-to-remember DNS names, and this DNS record applies the name specified by the user. | . ",
    "url": "/v4.0/reference/elastic_file_systems/#outputs",
    "relUrl": "/v4.0/reference/elastic_file_systems/#outputs"
  },"87": {
    "doc": "elastic_file_systems",
    "title": "elastic_file_systems",
    "content": " ",
    "url": "/v3.0/reference/elastic_file_systems/",
    "relUrl": "/v3.0/reference/elastic_file_systems/"
  },"88": {
    "doc": "elastic_file_systems",
    "title": "Description",
    "content": "The Provose elastic_file_systems module is used to configure AWS Elastic File Systems . Mounting a file system . Provose helps you create Elastic File Systems and makes them accessible within your Virtual Private Cloud (VPC), but you must take manual steps to mount the Elastic File Systems that you create. Read the EFS documentation for more information. ",
    "url": "/v3.0/reference/elastic_file_systems/#description",
    "relUrl": "/v3.0/reference/elastic_file_systems/#description"
  },"89": {
    "doc": "elastic_file_systems",
    "title": "Examples",
    "content": "The simplest example . This creates an unencrypted filesystem, defaulting to \"bursting\" as the throughput mode and with the performance mode being \"generalPurpose\". module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } elastic_file_systems = { # This creates an Elastic File system at the domain name # myfiles.myproject.example-internal.com, which is only accessible # by the VPC created by the \"myproject\" Provose module. \"myfiles\" = {} } } . ",
    "url": "/v3.0/reference/elastic_file_systems/#examples",
    "relUrl": "/v3.0/reference/elastic_file_systems/#examples"
  },"90": {
    "doc": "elastic_file_systems",
    "title": "Inputs",
    "content": "Currently, there are no additional inputs for creating Elastic File Systems. Currently, there are no required parameters for creating Elastic File Systems. Below are optional parameters for tuning performance or security. Beware that changing any of these parameters for an already-existing filesystem may cause Terraform to delete and recreate your filesystem. | performance_mode – Optional. This is the performance mode for the filesystem. This defaults to the value \"generalPurpose\" but can be set to \"maxIO\" for better performance. | throughput_mode – Optional. This sets the throughput mode for the filesytem. The default value is \"bursting\" which provides additional IOPS for this filesystem in bursts. To provision IOPS for this filesystem, set it to \"provisioned\". | provisioned_throughput_in_mib_per_second – Optional. When the throughput_mode is set to \"provisioned\", use this input to set how much throughput to provision in mebibytes per second. | encrypted – Optional. Set to true to encrypt the filesystem at rest. The default is false. | kms_key_id – Optional. The ID for the AWS Key Management System key used to encrypt the filesystem. Only use this parameter when you are setting encrypted to true. | . ",
    "url": "/v3.0/reference/elastic_file_systems/#inputs",
    "relUrl": "/v3.0/reference/elastic_file_systems/#inputs"
  },"91": {
    "doc": "elastic_file_systems",
    "title": "Outputs",
    "content": ". | elastic_file_systems.aws_security_group.elastic_file_systems – The aws_security_group resource that allows access to Elastic File Systems within the VPC. | elastic_file_systems.aws_efs_file_system.elastic_file_systems – The aws_efs_file_system resource. | elastic_file_systems.aws_efs_mount_target.elastic_file_systems – The aws_efs_mount_target resource for the filesystem. We create a mount target for every subnet in the VPC containing the Elastic File System. | elastic_file_systems.aws_route53_record.elastic_file_systems – The aws_route53_record resource for the filesystem. EFS creates hard-to-remember DNS names, and this DNS record applies the name specified by the user. | . ",
    "url": "/v3.0/reference/elastic_file_systems/#outputs",
    "relUrl": "/v3.0/reference/elastic_file_systems/#outputs"
  },"92": {
    "doc": "elasticsearch_clusters",
    "title": "elasticsearch_clusters",
    "content": " ",
    "url": "/v3.0/reference/elasticsearch_clusters/",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/"
  },"93": {
    "doc": "elasticsearch_clusters",
    "title": "Description",
    "content": "The Provose elasticsearch_clusters creates Elasticsearch clusters on the Amazon Elasticsearch Service. ",
    "url": "/v3.0/reference/elasticsearch_clusters/#description",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/#description"
  },"94": {
    "doc": "elasticsearch_clusters",
    "title": "Examples",
    "content": "Running a single node of Elasticsearch 7.1 with Logstash . module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } elasticsearch_clusters = { # This creates one Elasticsearch cluster named `ecsluster` escluster = { engine_version = \"7.1\" instances = { instance_type = \"t2.small.elasticsearch\" instance_count = 1 storage_per_instance_gb = 20 } # This spins up an additional `t2.small` EC2 instance running # a Logstash instance that will forward inputs to the `escluster` # Elasticsearch cluster. # You could connect to this Logstash instance on UDP port 5959 # on escluster-logstash.production.example-internal.com logstash = { instance_type = \"t2.small\" key_name = \"james_laptop\" } } } } . ",
    "url": "/v3.0/reference/elasticsearch_clusters/#examples",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/#examples"
  },"95": {
    "doc": "elasticsearch_clusters",
    "title": "Inputs",
    "content": ". | engine_version – Required. The version of Elasticsearch to deploy, like \"7.4\". You can find the currently-available versions of Elasticsearch on the AWS website here. | instances – Required. Settings for the instances running Elasticsearch. | instance_type – Required. The Elasticsearch-specific instance type, such as \"t2.small.elasticsearch\". You can find a list of supported instance types on the AWS Elasticsearch service pricing page. | instance_count – Required. The number of instances to deploy for the Elasticsearch cluster. | storage_per_instance_gb – Required. The amount of storage to provision–in gigabytes–for each instance in the cluster. | . | logstash – Optional. Optional settings to install Logstash on an EC2 instance that would write logs into the Elasticsearch cluster. Logstash is a data processing engine commonly used to format and redirect logs for indexing into Elasticsearch. It is not a required component to Elasticsearch, so Logstash configuration is purely optional. There are more details below about how to configure and use Logstash with Provose. | instance_type – Required. The EC2 instance type to run Logstash on, like \"t2.small\". | key_name – Optional. The name of an AWS EC2 key pair that can be used to log into the Logstash instance. | . | . ",
    "url": "/v3.0/reference/elasticsearch_clusters/#inputs",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/#inputs"
  },"96": {
    "doc": "elasticsearch_clusters",
    "title": "Outputs",
    "content": ". | elasticsearch_clusters.aws_security_group.elasticsearch – The AWS security group used to govern access to the Elasticsearch cluster. | elasticsearch_clusters.aws_elasticsearch_domain.elasticsearch – The aws_elasticsearch_domain object that defines the Elasticsearch cluster. | elasticsearch_clusters.aws_lb_listener_rule.elasticsearch – The listener rule for the AWS Application Load Balancer (ALB) that redirects DNS names to the cluster. This is for the VPC-only ALB that Provose provisions. | elasticsearch_clusters.aws_route53_record.elasticsearch – The Route 53 DNS record that gives a friendly DNS name to the Elasticsearch cluster. | elasticsearch_clusters.aws_route53_record.es_kibana – The Route 53 DNS record that gives a friendly DNS name to the Kibana endpoint. | . ",
    "url": "/v3.0/reference/elasticsearch_clusters/#outputs",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/#outputs"
  },"97": {
    "doc": "elasticsearch_clusters",
    "title": "Implementation details",
    "content": "How DNS works for Provose Elasticsearch clusters . The Amazon Elasticsearch service creates long and hard-to-remember names for clusters and their associated Kibana dashboards. Provose sets up an internal Application Load Balancer to map an easy-to-remember DNS name name to the cluster. This DNS name, load balancer, Elasticsearch cluster, and Kibana dashboard is not available on the public Internet. For security reasons, they are all only available within the VPC that Provose creates for Elasticsearch cluster. The internal Application Load Balance redirects requests to the Elasticsearch cluster or Kibana dashboard via an HTTP 301 redirect. Some Elasticsearch clients–such as the one Logstash uses–will treat the HTTP 301 code as an error as opposed to following the redirect. For these clients, you will need to use the DNS name for the Elasticsearch cluster or the Kibana dashboard set by AWS. Provose also offers its own deployment of Logstash that is configured to work correctly with Provose’s deployment of Elasticsearch. How to use Provose to deploy Logstash . The Provose elasticsearch_clusters module allows the provisioning of Logstash on an AWS EC2 instance. Logstash is a data processing engine used to commonly format and ingest logs for indexing into Elasticsearch. Logstash is not a required component to Elasticsearch, so Logstash configuration is purely optional. This Logstash configuration is for development purposes only. It will not autoscale to large amounts of logs to be ingested. If your provose_config.internal_root_domain is \"example.com\", your provose_config.internal_subdomain is \"subdomain\", and your Elasticsearch cluster name is \"cluster\", then the Logstash DNS name is \"cluster-logstash.subdomain.example.com\" on UDP port 5959. Provose currently does not support advanced Logstash configurations. If you need that, you should consider running your own Logstash instance through either the Provose ec2_instances or containers modules. The differences between Amazon Elasticsearch Service and Elastic.co’s Elasticsearch . Elasticsearch is an open-source search engine and database that is primarily maintained by the company Elastic NV. Elastic has created various proprietary add-ons for Elasticsearch. These are generally not available on the AWS Elasticsearch Service. However, Amazon has developed many equivalent features and released them under the Apache 2.0 license as the Open Distro for Elasticsearch. If the Open Distro does not fit your needs and you want to use Elastic’s proprietary features, Provose’s elasticsearch_clusters module will not be able to fulfill your needs. ",
    "url": "/v3.0/reference/elasticsearch_clusters/#implementation-details",
    "relUrl": "/v3.0/reference/elasticsearch_clusters/#implementation-details"
  },"98": {
    "doc": "elasticsearch_clusters",
    "title": "elasticsearch_clusters",
    "content": " ",
    "url": "/v4.0/reference/elasticsearch_clusters/",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/"
  },"99": {
    "doc": "elasticsearch_clusters",
    "title": "Description",
    "content": "The Provose elasticsearch_clusters creates Elasticsearch clusters on the Amazon Elasticsearch Service. ",
    "url": "/v4.0/reference/elasticsearch_clusters/#description",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/#description"
  },"100": {
    "doc": "elasticsearch_clusters",
    "title": "Examples",
    "content": "Running a single node of Elasticsearch 7.1 . module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } elasticsearch_clusters = { # This creates one Elasticsearch cluster named `ecsluster` escluster = { engine_version = \"7.1\" instances = { instance_type = \"t2.small.elasticsearch\" instance_count = 1 storage_per_instance_gb = 20 } } } } . ",
    "url": "/v4.0/reference/elasticsearch_clusters/#examples",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/#examples"
  },"101": {
    "doc": "elasticsearch_clusters",
    "title": "Inputs",
    "content": ". | engine_version – Required. The version of Elasticsearch to deploy, like \"7.4\". You can find the currently-available versions of Elasticsearch on the AWS website here. | instances – Required. Settings for the instances running Elasticsearch. | instance_type – Required. The Elasticsearch-specific instance type, such as \"t2.small.elasticsearch\". You can find a list of supported instance types on the AWS Elasticsearch service pricing page. | instance_count – Required. The number of instances to deploy for the Elasticsearch cluster. | storage_per_instance_gb – Required. The amount of storage to provision–in gigabytes–for each instance in the cluster. | . | . ",
    "url": "/v4.0/reference/elasticsearch_clusters/#inputs",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/#inputs"
  },"102": {
    "doc": "elasticsearch_clusters",
    "title": "Outputs",
    "content": ". | elasticsearch_clusters.aws_security_group.elasticsearch – The AWS security group used to govern access to the Elasticsearch cluster. | elasticsearch_clusters.aws_elasticsearch_domain.elasticsearch – The aws_elasticsearch_domain object that defines the Elasticsearch cluster. | elasticsearch_clusters.aws_lb_listener_rule.elasticsearch – The listener rule for the AWS Application Load Balancer (ALB) that redirects DNS names to the cluster. This is for the VPC-only ALB that Provose provisions. | elasticsearch_clusters.aws_route53_record.elasticsearch – The Route 53 DNS record that gives a friendly DNS name to the Elasticsearch cluster. | elasticsearch_clusters.aws_route53_record.es_kibana – The Route 53 DNS record that gives a friendly DNS name to the Kibana endpoint. | . ",
    "url": "/v4.0/reference/elasticsearch_clusters/#outputs",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/#outputs"
  },"103": {
    "doc": "elasticsearch_clusters",
    "title": "Implementation details",
    "content": "How DNS works for Provose Elasticsearch clusters . The Amazon Elasticsearch service creates long and hard-to-remember names for clusters and their associated Kibana dashboards. Provose sets up an internal Application Load Balancer to map an easy-to-remember DNS name name to the cluster. This DNS name, load balancer, Elasticsearch cluster, and Kibana dashboard is not available on the public Internet. For security reasons, they are all only available within the VPC that Provose creates for Elasticsearch cluster. The internal Application Load Balance redirects requests to the Elasticsearch cluster or Kibana dashboard via an HTTP 301 redirect. Some Elasticsearch clients–such as the one Logstash uses–will treat the HTTP 301 code as an error as opposed to following the redirect. For these clients, you will need to use the DNS name for the Elasticsearch cluster or the Kibana dashboard set by AWS. The differences between Amazon Elasticsearch Service and Elastic.co’s Elasticsearch . Elasticsearch is an open-source search engine and database that is primarily maintained by the company Elastic NV. Elastic has created various proprietary add-ons for Elasticsearch. These are generally not available on the AWS Elasticsearch Service. However, Amazon has developed many equivalent features and released them under the Apache 2.0 license as the Open Distro for Elasticsearch. If the Open Distro does not fit your needs and you want to use Elastic’s proprietary features, Provose’s elasticsearch_clusters module will not be able to fulfill your needs. ",
    "url": "/v4.0/reference/elasticsearch_clusters/#implementation-details",
    "relUrl": "/v4.0/reference/elasticsearch_clusters/#implementation-details"
  },"104": {
    "doc": "https_redirects",
    "title": "https_redirects",
    "content": " ",
    "url": "/v4.0/reference/https_redirects/",
    "relUrl": "/v4.0/reference/https_redirects/"
  },"105": {
    "doc": "https_redirects",
    "title": "Description",
    "content": "The Provose https_redirects module is mapping of DNS names and settings on how to select redirects for them. What this module does . As the name suggests, this module can create HTTP 301 permanent redirects or HTTP 302 temporary redirects from all paths under a domain name to another. This module does not create DNS CNAME or other types of redirects. You can use this module to redirect HTTP(S) traffic to anywhere on the Internet, but your AWS account must own the domain name for the source of the redirection. What this module does NOT do . This module redirects traffic with the public-facing Application Load Balancer (ALB), and currently cannot be used to route HTTP(S) traffic within the Virtual Private Cloud (VPC) that Provose sets up. How to configure this module . There are two forwarding types that govern how the redirects work. | \"DOMAIN_NAME\" – This takes all HTTP(S) URLs under your source DNS name and forwards it to the corresponding location in the destination DNS name. For example, if your source name is source.example.com and you want to forward requests to https://destination.com, setting your forwarding_type to \"DOMAIN_NAME\" will send the URL \"https://source.example.com/some-path?q=a\" to \"https://destination.com?some-path?q=a\". | \"EXACT_URL\" – This takes HTTP(S) URLs from the source and sends them to the exact same URL at the destination. For example, if your source name is \"source.example.com\" and you want to forward requests to \"https://destination.com\", setting your forwarding_type to \"EXACT_URL\" will send \"https://source.example.com/some-path?q=a\"to \"https://destination.com\". You can also set your destination to something like \"https://destination.com?some-path?q=a\" and all source URLs will go there. | . ",
    "url": "/v4.0/reference/https_redirects/#description",
    "relUrl": "/v4.0/reference/https_redirects/#description"
  },"106": {
    "doc": "https_redirects",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } https_redirects = { \"some.example.com\" = { status_code = 302 destination = \"https://destination.com\" forwarding_type = \"DOMAIN_NAME\" } \"example.com\" : { destination = \"https://google.com/robots.txt\" forwarding_type = \"EXACT_URL\" } } } . ",
    "url": "/v4.0/reference/https_redirects/#examples",
    "relUrl": "/v4.0/reference/https_redirects/#examples"
  },"107": {
    "doc": "https_redirects",
    "title": "Inputs",
    "content": ". | destination – Required. This is the destination URL to send redirects to, like https://example.com. You should always include the protocol (like \"http\" or “https\"). This can be an exact URL if the forwarding_type is \"EXACT_URL\", or a generic domain name if the forwarding_type is \"DOMAIN_NAME\". | forwarding_type – Optional. This specifies whether all possible sources are sent to corresponding destination URLs (\"DOMAIN_NAME\") or whether to send them to the exact same destination URL (\"EXACT_URL\"). | status_code – Optional. This is the HTTP status code to use in the redirect. By default this is 301 making the redirect “permanent” in the eyes of web browsers, search engines, and other HTTP clients. You can set this to 302 to tell HTTP clients that the redirect is “temporary.” . | . ",
    "url": "/v4.0/reference/https_redirects/#inputs",
    "relUrl": "/v4.0/reference/https_redirects/#inputs"
  },"108": {
    "doc": "https_redirects",
    "title": "Outputs",
    "content": " ",
    "url": "/v4.0/reference/https_redirects/#outputs",
    "relUrl": "/v4.0/reference/https_redirects/#outputs"
  },"109": {
    "doc": "https_redirects",
    "title": "https_redirects",
    "content": " ",
    "url": "/v3.0/reference/https_redirects/",
    "relUrl": "/v3.0/reference/https_redirects/"
  },"110": {
    "doc": "https_redirects",
    "title": "Description",
    "content": "The Provose https_redirects module is mapping of DNS names and settings on how to select redirects for them. What this module does . As the name suggests, this module can create HTTP 301 permanent redirects or HTTP 302 temporary redirects from all paths under a domain name to another. This module does not create DNS CNAME or other types of redirects. You can use this module to redirect HTTP(S) traffic to anywhere on the Internet, but your AWS account must own the domain name for the source of the redirection. What this module does NOT do . This module redirects traffic with the public-facing Application Load Balancer (ALB), and currently cannot be used to route HTTP(S) traffic within the Virtual Private Cloud (VPC) that Provose sets up. How to configure this module . There are two forwarding types that govern how the redirects work. | \"DOMAIN_NAME\" – This takes all HTTP(S) URLs under your source DNS name and forwards it to the corresponding location in the destination DNS name. For example, if your source name is source.example.com and you want to forward requests to https://destination.com, setting your forwarding_type to \"DOMAIN_NAME\" will send the URL \"https://source.example.com/some-path?q=a\" to \"https://destination.com?some-path?q=a\". | \"EXACT_URL\" – This takes HTTP(S) URLs from the source and sends them to the exact same URL at the destination. For example, if your source name is \"source.example.com\" and you want to forward requests to \"https://destination.com\", setting your forwarding_type to \"EXACT_URL\" will send \"https://source.example.com/some-path?q=a\"to \"https://destination.com\". You can also set your destination to something like \"https://destination.com?some-path?q=a\" and all source URLs will go there. | . ",
    "url": "/v3.0/reference/https_redirects/#description",
    "relUrl": "/v3.0/reference/https_redirects/#description"
  },"111": {
    "doc": "https_redirects",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } https_redirects = { \"some.example.com\" = { status_code = 302 destination = \"https://destination.com\" forwarding_type = \"DOMAIN_NAME\" } \"example.com\" : { destination = \"https://google.com/robots.txt\" forwarding_type = \"EXACT_URL\" } } } . ",
    "url": "/v3.0/reference/https_redirects/#examples",
    "relUrl": "/v3.0/reference/https_redirects/#examples"
  },"112": {
    "doc": "https_redirects",
    "title": "Inputs",
    "content": ". | destination – Required. This is the destination URL to send redirects to, like https://example.com. You should always include the protocol (like \"http\" or “https\"). This can be an exact URL if the forwarding_type is \"EXACT_URL\", or a generic domain name if the forwarding_type is \"DOMAIN_NAME\". | forwarding_type – Optional. This specifies whether all possible sources are sent to corresponding destination URLs (\"DOMAIN_NAME\") or whether to send them to the exact same destination URL (\"EXACT_URL\"). | status_code – Optional. This is the HTTP status code to use in the redirect. By default this is 301 making the redirect “permanent” in the eyes of web browsers, search engines, and other HTTP clients. You can set this to 302 to tell HTTP clients that the redirect is “temporary.” . | . ",
    "url": "/v3.0/reference/https_redirects/#inputs",
    "relUrl": "/v3.0/reference/https_redirects/#inputs"
  },"113": {
    "doc": "https_redirects",
    "title": "Outputs",
    "content": " ",
    "url": "/v3.0/reference/https_redirects/#outputs",
    "relUrl": "/v3.0/reference/https_redirects/#outputs"
  },"114": {
    "doc": "images",
    "title": "images",
    "content": " ",
    "url": "/v4.0/reference/images/",
    "relUrl": "/v4.0/reference/images/"
  },"115": {
    "doc": "images",
    "title": "Description",
    "content": "The Provose images module creates AWS Elastic Container Registry (ECR) image repositories, optionally building containers from local directories to upload. How to build and upload Docker images . If you have the docker command installed on the machine you use to run Provose, then Provose can build and upload your container when it creates the repository to upload your container. The Docker documentation describes how to install the docker command. To upload new versions of your container manually, visit the Elastic Container Registry page in the AWS console. Select the repository that you created via Provose and click the button View push commands. This will give you commands specific to your AWS account to upload containers from a Windows, Mac, or Linux machine. ",
    "url": "/v4.0/reference/images/#description",
    "relUrl": "/v4.0/reference/images/#description"
  },"116": {
    "doc": "images",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } images = { \"my_organization/my_container_1\" = { # local_path is the location of the Dockerfile or the build context local_path = \"../../src/my_container_1\" } \"my_organization/my_container_2\" = { local_path = \"../../src/my_container_2\" } # It is also possible to not specify a local path, and instead create # an Amazon Web Services Elastic Container Registry (ECR) repository # to where you will push an image to later. # Note that if you have containers that depend on this ECR images, the # Application Load Balancer will throw HTTP 503 errors until you have # uploaded an image that can run. \"my_organization/my_container_2\" = {} } } . ",
    "url": "/v4.0/reference/images/#examples",
    "relUrl": "/v4.0/reference/images/#examples"
  },"117": {
    "doc": "images",
    "title": "Inputs",
    "content": ". | local_path – Optional. This is the filesystem path to pass to docker build. This builds, tags, and uploads a container to the registry. The path you pass should have a Dockerfile. | . ",
    "url": "/v4.0/reference/images/#inputs",
    "relUrl": "/v4.0/reference/images/#inputs"
  },"118": {
    "doc": "images",
    "title": "Outputs",
    "content": ". | image.aws_ecr_repository.image – map with a key for every repository name and every value is a Terraform aws_ecr_repository type. | image.aws_ecr_repository_policy.image – A map with a key for every repository name and every value is a Terraform aws_ecr_repository_policy type. | . ",
    "url": "/v4.0/reference/images/#outputs",
    "relUrl": "/v4.0/reference/images/#outputs"
  },"119": {
    "doc": "images",
    "title": "images",
    "content": " ",
    "url": "/v3.0/reference/images/",
    "relUrl": "/v3.0/reference/images/"
  },"120": {
    "doc": "images",
    "title": "Description",
    "content": "The Provose images module creates AWS Elastic Container Registry (ECR) image repositories, optionally building containers from local directories to upload. How to build and upload Docker images . If you have the docker command installed on the machine you use to run Provose, then Provose can build and upload your container when it creates the repository to upload your container. The Docker documentation describes how to install the docker command. To upload new versions of your container manually, visit the Elastic Container Registry page in the AWS console. Select the repository that you created via Provose and click the button View push commands. This will give you commands specific to your AWS account to upload containers from a Windows, Mac, or Linux machine. ",
    "url": "/v3.0/reference/images/#description",
    "relUrl": "/v3.0/reference/images/#description"
  },"121": {
    "doc": "images",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } images = { \"my_organization/my_container_1\" = { # local_path is the location of the Dockerfile or the build context local_path = \"../../src/my_container_1\" } \"my_organization/my_container_2\" = { local_path = \"../../src/my_container_2\" } # It is also possible to not specify a local path, and instead create # an Amazon Web Services Elastic Container Registry (ECR) repository # to where you will push an image to later. # Note that if you have containers that depend on this ECR images, the # Application Load Balancer will throw HTTP 503 errors until you have # uploaded an image that can run. \"my_organization/my_container_2\" = {} } } . ",
    "url": "/v3.0/reference/images/#examples",
    "relUrl": "/v3.0/reference/images/#examples"
  },"122": {
    "doc": "images",
    "title": "Inputs",
    "content": ". | local_path – Optional. This is the filesystem path to pass to docker build. This builds, tags, and uploads a container to the registry. The path you pass should have a Dockerfile. | . ",
    "url": "/v3.0/reference/images/#inputs",
    "relUrl": "/v3.0/reference/images/#inputs"
  },"123": {
    "doc": "images",
    "title": "Outputs",
    "content": ". | image.aws_ecr_repository.image – map with a key for every repository name and every value is a Terraform aws_ecr_repository type. | image.aws_ecr_repository_policy.image – A map with a key for every repository name and every value is a Terraform aws_ecr_repository_policy type. | . ",
    "url": "/v3.0/reference/images/#outputs",
    "relUrl": "/v3.0/reference/images/#outputs"
  },"124": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": "This page describes various common errors that Terraform may throw when running a Provose-based configuration and how to fix them. If you are receiving an error message that you don’t know how to fix, feel free to file an issue on Provose’s GitHub page. ",
    "url": "/v3.0/debugging/#debugging",
    "relUrl": "/v3.0/debugging/#debugging"
  },"125": {
    "doc": "Debugging",
    "title": "Table of contents",
    "content": ". | Error putting S3 policy: OperationAborted: A conflicting conditional operation is currently in progress against this resource. Please try again. | Error creating IAM instance profile [...]: EntityAlreadyExists | Error: Provider configuration not present | Error deleting ECS cluster: ClusterContainsTasksException: The Cluster cannot be deleted while Tasks are active. | Error: you have reached your limit of 20 certificates in the last year. | . ",
    "url": "/v3.0/debugging/#table-of-contents",
    "relUrl": "/v3.0/debugging/#table-of-contents"
  },"126": {
    "doc": "Debugging",
    "title": "Error putting S3 policy: OperationAborted: A conflicting conditional operation is currently in progress against this resource. Please try again.",
    "content": "This error often appears when creating, deleting, or changing the security settings of an Amazon S3 bucket. It can happen for various reasons, especially if you have deleted and recreated a bucket. Try running terraform apply again. If the issue persists, file an issue on Provose’s GitHub page. Provose creates Amazon S3 buckets with the s3_buckets module. Provose also creates S3 buckets to store logs produced by Elastic Load Balancers provisioned by the containers module. ",
    "url": "/v3.0/debugging/#error-putting-s3-policy-operationaborted-a-conflicting-conditional-operation-is-currently-in-progress-against-this-resource-please-try-again",
    "relUrl": "/v3.0/debugging/#error-putting-s3-policy-operationaborted-a-conflicting-conditional-operation-is-currently-in-progress-against-this-resource-please-try-again"
  },"127": {
    "doc": "Debugging",
    "title": "Error creating IAM instance profile [...]: EntityAlreadyExists",
    "content": "This error often happens when a Terraform operation that was intended to destroy an IAM instance profile was interrupted. You can find the IAM instance profile in the AWS console, but attempting to delete it from the console will not resolve the error. Instead, you need to use the AWS CLI to delete the instance profile. Run the following command . aws iam delete-instance-profile --instance-profile-name &lt;name of your instance profile&gt; . and then run terraform apply. ",
    "url": "/v3.0/debugging/#error-creating-iam-instance-profile--entityalreadyexists",
    "relUrl": "/v3.0/debugging/#error-creating-iam-instance-profile--entityalreadyexists"
  },"128": {
    "doc": "Debugging",
    "title": "Error: Provider configuration not present",
    "content": "This may happen if you tried to delete the entire Provose module and then ran terraform apply or terraform destroy. This confuses Terraform because it does not know what to do with the now orphaned resources created by the module now that the module’s existence has been wiped out. It is easier to try and delete the resources created by your Provose module before removing the module entirely. You can read how to delete a Provose module in Uninstalling Provose. It is easier to try and delete the resources created by your Provose module before removing the module entirely. ",
    "url": "/v3.0/debugging/#error-provider-configuration-not-present",
    "relUrl": "/v3.0/debugging/#error-provider-configuration-not-present"
  },"129": {
    "doc": "Debugging",
    "title": "Error deleting ECS cluster: ClusterContainsTasksException: The Cluster cannot be deleted while Tasks are active.",
    "content": "This happens when you are deleting an Elastic Container Service cluster that still has tasks in it. Because Provose abstracts over ECS clusters, services, and tasks, they tend to be deleted all at the same time. However, deleting the cluster may not succeed until the tasks have finished draining. You can solve this by logging into the AWS console to stop the tasks belonging to the cluster you want to delete. ",
    "url": "/v3.0/debugging/#error-deleting-ecs-cluster-clustercontainstasksexception-the-cluster-cannot-be-deleted-while-tasks-are-active",
    "relUrl": "/v3.0/debugging/#error-deleting-ecs-cluster-clustercontainstasksexception-the-cluster-cannot-be-deleted-while-tasks-are-active"
  },"130": {
    "doc": "Debugging",
    "title": "Error: you have reached your limit of 20 certificates in the last year.",
    "content": "This happens when you repeatedly create and delete Provose configurations. By default, Amazon rate-limits the number of certificates that can be generated via AWS Certificate Manager (ACM). To fix this, contact AWS Support for your account and request that they increase the limit named ACM certificates created in last 365 days. ",
    "url": "/v3.0/debugging/#error-you-have-reached-your-limit-of-20-certificates-in-the-last-year",
    "relUrl": "/v3.0/debugging/#error-you-have-reached-your-limit-of-20-certificates-in-the-last-year"
  },"131": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": " ",
    "url": "/v3.0/debugging/",
    "relUrl": "/v3.0/debugging/"
  },"132": {
    "doc": "Docs v4.0 (Alpha Preview)",
    "title": "Provose v3.0 Documentation",
    "content": " ",
    "url": "/v4.0/#provose-v30-documentation",
    "relUrl": "/v4.0/#provose-v30-documentation"
  },"133": {
    "doc": "Docs v4.0 (Alpha Preview)",
    "title": "Docs v4.0 (Alpha Preview)",
    "content": " ",
    "url": "/v4.0/",
    "relUrl": "/v4.0/"
  },"134": {
    "doc": "Tutorial",
    "title": "Provose v3.0 Tutorial",
    "content": "This is a tutorial aimed at teaching the beginner how to use Provose. It helps to be familiar with Amazon Web Services and HashiCorp Terraform, but this tutorial tries to give you the knowledge you need as you go. If you have ideas on improving this tutorial, please file an issue on Provose’s GitHub page. ",
    "url": "/v4.0/tutorial/#provose-v30-tutorial",
    "relUrl": "/v4.0/tutorial/#provose-v30-tutorial"
  },"135": {
    "doc": "Tutorial",
    "title": "Table of contents",
    "content": ". | Buy or move a domain name in your AWS account | Install Provose’s prerequisites . | HashiCorp Terraform | AWS CLI v2 | Docker (optional) | . | Set up your credentials | Set up an S3 bucket to store Terraform state | Begin using Terraform remote state | Initialize Provose | Start using Provose modules | . ",
    "url": "/v4.0/tutorial/#table-of-contents",
    "relUrl": "/v4.0/tutorial/#table-of-contents"
  },"136": {
    "doc": "Tutorial",
    "title": "Buy or move a domain name in your AWS account",
    "content": "Provose requires that you have a top-level domain name in your AWS account that will be delegated to serving internal services. This domain name is used as the root DNS name for service discovery internal to the VPC–not exposed to the public Internet. Provose needs this to be a real, registered public domain name attached to your AWS account so Provose can register AWS Certificate Manager (ACM) TLS certificates. These certificates are used to encrypt and authenticate traffic within the VPC. If your company’s main website is served on example.com, you should purchase another domain, such as example-internal.com for Provose. This domain name may host a few publicly-accessible instances, such as SSH “bastion” hosts or VPN endpoints, but mostly will only be used to route network requests within Virtual Private Clouds (VPCs) that Provose creates. Amazon has a guide for registering a new domain and another one for transferring a domain registered elsewhere into your AWS account. ",
    "url": "/v4.0/tutorial/#buy-or-move-a-domain-name-in-your-aws-account",
    "relUrl": "/v4.0/tutorial/#buy-or-move-a-domain-name-in-your-aws-account"
  },"137": {
    "doc": "Tutorial",
    "title": "Install Provose’s prerequisites",
    "content": "Whether you are running Provose on your local machine, an EC2 instance, or a Continuous Integration (CI) provider, you will need to make sure that the machine has the following dependencies installed: . HashiCorp Terraform . Provose is built on top of HashiCorp Terraform, and industry-leading infrastructure-as-code tool. Terraform’s documentation describes how to download and install the latest version of Terraform for your operating system. You will need Terraform 0.13 or newer to use Provose. AWS CLI v2 . There are some configurations on AWS that Terraform’s AWS provider does not currently support. Provose works around these limitations by setting some configurations with the AWS CLI. If you have v1 of the AWS CLI, make sure to uninstall it first. Then you can follow Amazon’s instructions for installing AWS CLI v2 on Mac, Windows, or Linux. Docker (optional) . The docker command is currently only required if you want to build and upload Docker images with the Provose images module. The Docker documentation describes how to install the docker command. ",
    "url": "/v4.0/tutorial/#install-provoses-prerequisites",
    "relUrl": "/v4.0/tutorial/#install-provoses-prerequisites"
  },"138": {
    "doc": "Tutorial",
    "title": "Set up your credentials",
    "content": "Provose strongly discourages placing credentials in code. If you want to run Provose (and its underlying Terraform setup) on your local machine, we recommend placing your credentials in the ~/.aws/credentials file in your home directory, with additional configuration in the ~/.aws/config directory. The AWS documentation has more information on setting up configuration and credential files. An example ~/.aws/credentials file might look like: . [my_profile_name] aws_access_key_id = ... aws_secret_access_key = ... and an example ‘~/.aws/config` file might look like: . [my-profile-name] region = us-east-1 . If you have multiple sets of credentials in these files, you can tell Provose which credentials you want to use with the AWS_PROFILE environment variable. You can also set credentials directly with the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. If you are running Provose on an AWS EC2 instance, Provose can use the credentials in the IAM instance profile for the instance. AWS has a documentation page about how to use instance profiles. ",
    "url": "/v4.0/tutorial/#set-up-your-credentials",
    "relUrl": "/v4.0/tutorial/#set-up-your-credentials"
  },"139": {
    "doc": "Tutorial",
    "title": "Set up an S3 bucket to store Terraform state",
    "content": "By default, Terraform stores state about your resources in a local file called terraform.tfstate. Terraform is also capable of storing this state in various remote backends like an Amazon S3 bucket. Terraform’s enterprise version also offers remote state storage, but that is beyond the scope of this tutorial. All features of Provose are available through the free and open-source version of Terraform. If you do not already have a project with pre-configured Terraform state, we recommend you follow these steps to: . | Use Terraform to create the S3 bucket that we will use to store Terraform state. | Terraform stores the local state from #1 in a local terraform.tfstate file. | We tell Terraform to move the contents of the local state file from #2 into the S3 bucket we created in #1. | . If the domain name you have chosen was example-internal.com and you want to deploy to the AWS region us-east-1, then the first version of our Terraform state configuration looks like this: . terraform { # Look out. We're going to replace this `backend \"local\"` block with # a `backend \"s3\"` block later in the tutorial. backend \"local\" { path = \"terraform.tfstate\" } required_providers { # Provose v3.0 currently uses the Terraform AWS provider 3.9.0. # Stick with this version for your own code to avoid compatibility # issues. aws = \"3.9.0\" } } provider \"aws\" { region = \"us-east-1\" } # This is an AWS Key Management Service (KMS) key that we will use to # encrypt the AWS S3 bucket. resource \"aws_kms_key\" \"terraform\" { description = \"Used to encrypt the Terraform state S3 bucket at rest.\" tags = { Provose = \"terraform\" } lifecycle { prevent_destroy = true } } # This is where we store the Terraform remote tfstate. resource \"aws_s3_bucket\" \"terraform\" { # This should be a globally-unique Amazon S3 bucket, although it # should not be accessible outside of the AWS credentials you use to run # Terraform. bucket = \"terraform-state.example-internal.com\" acl = \"private\" # For security and compliance reasons, Provose recommends that you # configure AWS Key Management Service (KMS) encryption at rest for # the bucket. server_side_encryption_configuration { rule { apply_server_side_encryption_by_default { kms_master_key_id = aws_kms_key.terraform.arn sse_algorithm = \"aws:kms\" } } } versioning { enabled = true } tags = { Provose = \"terraform\" } # This lifecycle parameter prevents Terraform from destroying the # bucket that contains its own state. lifecycle { prevent_destroy = true } } # This prevents public access to our remote Terraform tfstate. resource \"aws_s3_bucket_public_access_block\" \"terraform\" { bucket = aws_s3_bucket.terraform.id block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true lifecycle { prevent_destroy = true } } . We recommend that you place the above configuration in a file named terraform.tf. To keep things organized, you may want to put your Provose configuration into a separate file. This configuration creates the S3 bucket that you will use for remote state, but Terraform must store state in the local file terraform.tfstate before the S3 bucket is created. After saving terraform.tf, run the following commands: . terraform init terraform plan -out plan.out terrafrom apply \"plan.out\" . ",
    "url": "/v4.0/tutorial/#set-up-an-s3-bucket-to-store-terraform-state",
    "relUrl": "/v4.0/tutorial/#set-up-an-s3-bucket-to-store-terraform-state"
  },"140": {
    "doc": "Tutorial",
    "title": "Begin using Terraform remote state",
    "content": "In the previous step, we created the S3 bucket to store remote state, but we have not started using the bucket yet–Terraform is still configured to use local state. To move our state to a remote S3 bucket, we need to change the terraform block at the beginning of our terraform.tf file to reference remote state: . terraform { backend \"s3\" { # This is the name of the S3 bucket we use to store Terraform state. # We create this bucket below. bucket = \"terraform-state.example-internal.com\" key = \"tfstate\" region = \"us-east-1\" acl = \"private\" } required_providers { # Provose v3.0 currently uses the Terraform AWS provider 3.9.0. # Stick with this version for your own code to avoid compatibility # issues. aws = \"3.9.0\" } } provider \"aws\" { # ... this file continues from the previous example . Rerun terraform init and you should see the following prompt asking if you want to copy your local state to the S3 bucket. Answer with yes. Initializing the backend... Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"local\" backend to the newly configured \"s3\" backend. No existing state was found in the newly configured \"s3\" backend. Do you want to copy this state to the new \"s3\" backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: yes . ",
    "url": "/v4.0/tutorial/#begin-using-terraform-remote-state",
    "relUrl": "/v4.0/tutorial/#begin-using-terraform-remote-state"
  },"141": {
    "doc": "Tutorial",
    "title": "Initialize Provose",
    "content": "Now that you have Terraform’s remote state configuration set up, create another file named after the project you want to create with Provose. In these examples, we will go with the name myproject. In myproject.tf, we will enter the bare minimum Provose configuration: . module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } } . Run terraform init again to download Provose v3.0.0 and the Terraform modules and providers that Provose depends on. You should rerun terraform init every time you update Provose, or if you change any Terraform modules or providers that you use elsewhere in your code. ",
    "url": "/v4.0/tutorial/#initialize-provose",
    "relUrl": "/v4.0/tutorial/#initialize-provose"
  },"142": {
    "doc": "Tutorial",
    "title": "Start using Provose modules",
    "content": "You can now use Provose modules–like containers, s3_buckets, mysql_clusters, and more–to configure the AWS infrastructure that you need. Below is an example of two . # This is the Terraform `random_password` resource that we use to generate secure random # passwords for the AWS Aurora MySQL clusters that we provision below. # You can read more about the provider here # https://www.terraform.io/docs/providers/random/r/password.html resource \"random_password\" \"bigcluster_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } # This is another `random_password` resource that we use for the other cluster. resource \"random_password\" \"smallcluster_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { # These are various settings that are core to Provose. source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { # Provose is going to pull AWS credentials from you environment. # If you want to specify your keys in code, you can set the # `access_key` and `secret_key` map keys here. region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } # These are two S3 buckets, which will be made available to one of the # below containers. s3_buckets = { \"bucket-one.example-internal.com\" = { versioning = false } \"another-bucket.example-internal.com\" = { versioning = true } } # These are Docker images that we upload to Elastic Container Registry (ECR). images = { # We build this container from a local path and upload it to the registry. \"example/webserver\" = { local_path = \"../src/webserver\" } # We create the ECR image repository for this container, but you need to # build and upload the image yourself. \"example/anotherimage\" = {} } # These are Aurora MySQL clusters. mysql_clusters = { # This creates an AWS Aurora MySQL cluster available # at the host bigcluster.myproject.example-internal.com. # This host is only available within the VPC. bigcluster = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.bigcluster_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigmy.production.example-internal.com. # This host is only available within the VPC. smallcluster = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"otherdb\" password = random_password.smallcluster_password.result instances = { instance_type = \"db.t3.small\" instance_count = 3 } } } # These are names and values for secrets stored in AWS Secrets Manager. secrets = { bigcluster_password = random_password.bigcluster_password.result smallcluster_password = random_password.smallcluster_password.result } # These are Docker containers that we run on AWS Elastic Container Service (ECS). # When `private_registry` is `true`, we access one of the images from the # ECR repositories defined above in the `images` section. # When `private_registry` is `false`, we look for publicly-available containers # on Docker Hub. containers = { # This is an example of a container that runs an image that we privately # built and uploaded to ECR. # We run the container on AWS Fargate--which means that there are no EC2 # hosts exposed directly to the user. # The container is given access to one of the above MySQL # clusters via environment variables and the Secrets Manager. web = { image = { name = \"example/webserver\" tag = \"latest\" private_registry = true } public = { https = { internal_http_port = 8000 internal_http_health_check_path = \"/\" public_dns_names = [\"web.test.example.com\"] } } instances = { instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } environment = { MYSQL_HOST = \"bigcluster.myproject.example-internal.com\" MYSQL_USER = \"root\" MYSQL_DATABASE = \"exampledb\" MYSQL_PORT = 3306 } secrets = { MYSQL_PASSWORD = \"bigcluster_password\" } } # This is an example of a container that runs a publicly-available # image from Docker Hub on a few `t2.small` EC2 instances. helloworld = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"helloworld.example.com\"] } } instances = { instance_type = \"t2.small\" container_count = 4 instance_count = 2 cpu = 256 memory = 512 } } } } . You can read more about Provose’s capabilities in the Reference. ",
    "url": "/v4.0/tutorial/#start-using-provose-modules",
    "relUrl": "/v4.0/tutorial/#start-using-provose-modules"
  },"143": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": " ",
    "url": "/v4.0/tutorial/",
    "relUrl": "/v4.0/tutorial/"
  },"144": {
    "doc": "Reference v3.0 (Stable)",
    "title": "Provose v3.0 Reference",
    "content": "This is the reference page for Provose modules. ",
    "url": "/v3.0/reference/#provose-v30-reference",
    "relUrl": "/v3.0/reference/#provose-v30-reference"
  },"145": {
    "doc": "Reference v3.0 (Stable)",
    "title": "Reference v3.0 (Stable)",
    "content": " ",
    "url": "/v3.0/reference/",
    "relUrl": "/v3.0/reference/"
  },"146": {
    "doc": "Reference v4.0 (Alpha Preview)",
    "title": "Provose v3.0 Reference",
    "content": "This is the reference page for Provose modules. ",
    "url": "/v4.0/reference/#provose-v30-reference",
    "relUrl": "/v4.0/reference/#provose-v30-reference"
  },"147": {
    "doc": "Reference v4.0 (Alpha Preview)",
    "title": "Reference v4.0 (Alpha Preview)",
    "content": " ",
    "url": "/v4.0/reference/",
    "relUrl": "/v4.0/reference/"
  },"148": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": "This page describes various common errors that Terraform may throw when running a Provose-based configuration and how to fix them. If you are receiving an error message that you don’t know how to fix, feel free to file an issue on Provose’s GitHub page. ",
    "url": "/v4.0/debugging/#debugging",
    "relUrl": "/v4.0/debugging/#debugging"
  },"149": {
    "doc": "Debugging",
    "title": "Table of contents",
    "content": ". | Error putting S3 policy: OperationAborted: A conflicting conditional operation is currently in progress against this resource. Please try again. | Error creating IAM instance profile [...]: EntityAlreadyExists | Error: Provider configuration not present | Error deleting ECS cluster: ClusterContainsTasksException: The Cluster cannot be deleted while Tasks are active. | Error: you have reached your limit of 20 certificates in the last year. | . ",
    "url": "/v4.0/debugging/#table-of-contents",
    "relUrl": "/v4.0/debugging/#table-of-contents"
  },"150": {
    "doc": "Debugging",
    "title": "Error putting S3 policy: OperationAborted: A conflicting conditional operation is currently in progress against this resource. Please try again.",
    "content": "This error often appears when creating, deleting, or changing the security settings of an Amazon S3 bucket. It can happen for various reasons, especially if you have deleted and recreated a bucket. Try running terraform apply again. If the issue persists, file an issue on Provose’s GitHub page. Provose creates Amazon S3 buckets with the s3_buckets module. Provose also creates S3 buckets to store logs produced by Elastic Load Balancers provisioned by the containers module. ",
    "url": "/v4.0/debugging/#error-putting-s3-policy-operationaborted-a-conflicting-conditional-operation-is-currently-in-progress-against-this-resource-please-try-again",
    "relUrl": "/v4.0/debugging/#error-putting-s3-policy-operationaborted-a-conflicting-conditional-operation-is-currently-in-progress-against-this-resource-please-try-again"
  },"151": {
    "doc": "Debugging",
    "title": "Error creating IAM instance profile [...]: EntityAlreadyExists",
    "content": "This error often happens when a Terraform operation that was intended to destroy an IAM instance profile was interrupted. You can find the IAM instance profile in the AWS console, but attempting to delete it from the console will not resolve the error. Instead, you need to use the AWS CLI to delete the instance profile. Run the following command . aws iam delete-instance-profile --instance-profile-name &lt;name of your instance profile&gt; . and then run terraform apply. ",
    "url": "/v4.0/debugging/#error-creating-iam-instance-profile--entityalreadyexists",
    "relUrl": "/v4.0/debugging/#error-creating-iam-instance-profile--entityalreadyexists"
  },"152": {
    "doc": "Debugging",
    "title": "Error: Provider configuration not present",
    "content": "This may happen if you tried to delete the entire Provose module and then ran terraform apply or terraform destroy. This confuses Terraform because it does not know what to do with the now orphaned resources created by the module now that the module’s existence has been wiped out. It is easier to try and delete the resources created by your Provose module before removing the module entirely. You can read how to delete a Provose module in Uninstalling Provose. It is easier to try and delete the resources created by your Provose module before removing the module entirely. ",
    "url": "/v4.0/debugging/#error-provider-configuration-not-present",
    "relUrl": "/v4.0/debugging/#error-provider-configuration-not-present"
  },"153": {
    "doc": "Debugging",
    "title": "Error deleting ECS cluster: ClusterContainsTasksException: The Cluster cannot be deleted while Tasks are active.",
    "content": "This happens when you are deleting an Elastic Container Service cluster that still has tasks in it. Because Provose abstracts over ECS clusters, services, and tasks, they tend to be deleted all at the same time. However, deleting the cluster may not succeed until the tasks have finished draining. You can solve this by logging into the AWS console to stop the tasks belonging to the cluster you want to delete. ",
    "url": "/v4.0/debugging/#error-deleting-ecs-cluster-clustercontainstasksexception-the-cluster-cannot-be-deleted-while-tasks-are-active",
    "relUrl": "/v4.0/debugging/#error-deleting-ecs-cluster-clustercontainstasksexception-the-cluster-cannot-be-deleted-while-tasks-are-active"
  },"154": {
    "doc": "Debugging",
    "title": "Error: you have reached your limit of 20 certificates in the last year.",
    "content": "This happens when you repeatedly create and delete Provose configurations. By default, Amazon rate-limits the number of certificates that can be generated via AWS Certificate Manager (ACM). To fix this, contact AWS Support for your account and request that they increase the limit named ACM certificates created in last 365 days. ",
    "url": "/v4.0/debugging/#error-you-have-reached-your-limit-of-20-certificates-in-the-last-year",
    "relUrl": "/v4.0/debugging/#error-you-have-reached-your-limit-of-20-certificates-in-the-last-year"
  },"155": {
    "doc": "Debugging",
    "title": "Debugging",
    "content": " ",
    "url": "/v4.0/debugging/",
    "relUrl": "/v4.0/debugging/"
  },"156": {
    "doc": "Docs v3.0 (Stable)",
    "title": "Provose v3.0 Documentation",
    "content": " ",
    "url": "/v3.0/#provose-v30-documentation",
    "relUrl": "/v3.0/#provose-v30-documentation"
  },"157": {
    "doc": "Docs v3.0 (Stable)",
    "title": "Docs v3.0 (Stable)",
    "content": " ",
    "url": "/v3.0/",
    "relUrl": "/v3.0/"
  },"158": {
    "doc": "Home",
    "title": "Provose is the easiest way to manage your Amazon Web Services infrastructure.",
    "content": " ",
    "url": "/#provose-is-the-easiest-way-to-manage-your-amazon-web-services-infrastructure",
    "relUrl": "/#provose-is-the-easiest-way-to-manage-your-amazon-web-services-infrastructure"
  },"159": {
    "doc": "Home",
    "title": "Provose is built on top of HashiCorp Terraform, an industry-leading infrastructure-as-code tool.",
    "content": "Provose is a Terraform module that deploys hundreds of underlying cloud resources–containers, databases, TLS certificates, DNS rules, and more–with just a few lines of code. Provose is a layer on top of Terraform that makes intelligent configuration choices for you and reduces the amount of code needed. ",
    "url": "/#provose-is-built-on-top-of-hashicorp-terraform-an-industry-leading-infrastructure-as-code-tool",
    "relUrl": "/#provose-is-built-on-top-of-hashicorp-terraform-an-industry-leading-infrastructure-as-code-tool"
  },"160": {
    "doc": "Home",
    "title": "Provose is free and open-source software forever.",
    "content": "Provose is distributed under the MIT license. You can download Provose at github.com/provose/provose, which is also where you can also submit bug reports and contribute improvements. ",
    "url": "/#provose-is-free-and-open-source-software-forever",
    "relUrl": "/#provose-is-free-and-open-source-software-forever"
  },"161": {
    "doc": "Home",
    "title": "Learn Provose from Tutorial or the Reference.",
    "content": "Provose is easy to learn. You can get started with just a few lines of code. You can also find an example of setting up a Ghost.org blog on Amazon Web Services using Provose. ",
    "url": "/#learn-provose-from-tutorial-or-the-reference",
    "relUrl": "/#learn-provose-from-tutorial-or-the-reference"
  },"162": {
    "doc": "Home",
    "title": "Here is what Provose code looks like.",
    "content": "Below is an example of what Provose looks like, provisioning a container serving HTTP traffic on AWS Fargate: . module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" # Provose requires a domain name to be used for internal purposes. # This allows us to protect internal services using # AWS Certificate Manager (ACM) certificates. internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } containers = { hello = { image = { # This is the name of a publicly-available container on DockerHub. # Private Elastic Container Registry (ECR) containers can also be used. name = \"nginxdemos/hello\" # This is a container tag on DockerHub. tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" # You need to have example.com as a domain in your AWS # account with DNS managed by Route 53. # Provose will set up an Application Load Balancer serving # HTTP and HTTPS traffic to this group of containers. public_dns_names = [\"hello.example.com\"] } } instances = { # Set this to an EC2 instance type to use AWS ECS-EC2 # or FARGATE_SPOT to automatically save money by using Spot # instances. instance_type = \"FARGATE\" container_count = 1 cpu = 256 memory = 512 } } } } . You can also take a look at how to use Provose to provision: . | MySQL | PostgreSQL | Elasticsearch | Redis | Lustre | bare EC2 instances | . and a lot more on Amazon Web Services. ",
    "url": "/#here-is-what-provose-code-looks-like",
    "relUrl": "/#here-is-what-provose-code-looks-like"
  },"163": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"164": {
    "doc": "Tutorial",
    "title": "Provose v3.0 Tutorial",
    "content": "This is a tutorial aimed at teaching the beginner how to use Provose. It helps to be familiar with Amazon Web Services and HashiCorp Terraform, but this tutorial tries to give you the knowledge you need as you go. If you have ideas on improving this tutorial, please file an issue on Provose’s GitHub page. ",
    "url": "/v3.0/tutorial/#provose-v30-tutorial",
    "relUrl": "/v3.0/tutorial/#provose-v30-tutorial"
  },"165": {
    "doc": "Tutorial",
    "title": "Table of contents",
    "content": ". | Buy or move a domain name in your AWS account | Install Provose’s prerequisites . | HashiCorp Terraform | AWS CLI v2 | Docker (optional) | . | Set up your credentials | Set up an S3 bucket to store Terraform state | Begin using Terraform remote state | Initialize Provose | Start using Provose modules | . ",
    "url": "/v3.0/tutorial/#table-of-contents",
    "relUrl": "/v3.0/tutorial/#table-of-contents"
  },"166": {
    "doc": "Tutorial",
    "title": "Buy or move a domain name in your AWS account",
    "content": "Provose requires that you have a top-level domain name in your AWS account that will be delegated to serving internal services. This domain name is used as the root DNS name for service discovery internal to the VPC–not exposed to the public Internet. Provose needs this to be a real, registered public domain name attached to your AWS account so Provose can register AWS Certificate Manager (ACM) TLS certificates. These certificates are used to encrypt and authenticate traffic within the VPC. If your company’s main website is served on example.com, you should purchase another domain, such as example-internal.com for Provose. This domain name may host a few publicly-accessible instances, such as SSH “bastion” hosts or VPN endpoints, but mostly will only be used to route network requests within Virtual Private Clouds (VPCs) that Provose creates. Amazon has a guide for registering a new domain and another one for transferring a domain registered elsewhere into your AWS account. ",
    "url": "/v3.0/tutorial/#buy-or-move-a-domain-name-in-your-aws-account",
    "relUrl": "/v3.0/tutorial/#buy-or-move-a-domain-name-in-your-aws-account"
  },"167": {
    "doc": "Tutorial",
    "title": "Install Provose’s prerequisites",
    "content": "Whether you are running Provose on your local machine, an EC2 instance, or a Continuous Integration (CI) provider, you will need to make sure that the machine has the following dependencies installed: . HashiCorp Terraform . Provose is built on top of HashiCorp Terraform, and industry-leading infrastructure-as-code tool. Terraform’s documentation describes how to download and install the latest version of Terraform for your operating system. You will need Terraform 0.13 or newer to use Provose. AWS CLI v2 . There are some configurations on AWS that Terraform’s AWS provider does not currently support. Provose works around these limitations by setting some configurations with the AWS CLI. If you have v1 of the AWS CLI, make sure to uninstall it first. Then you can follow Amazon’s instructions for installing AWS CLI v2 on Mac, Windows, or Linux. Docker (optional) . The docker command is currently only required if you want to build and upload Docker images with the Provose images module. The Docker documentation describes how to install the docker command. ",
    "url": "/v3.0/tutorial/#install-provoses-prerequisites",
    "relUrl": "/v3.0/tutorial/#install-provoses-prerequisites"
  },"168": {
    "doc": "Tutorial",
    "title": "Set up your credentials",
    "content": "Provose strongly discourages placing credentials in code. If you want to run Provose (and its underlying Terraform setup) on your local machine, we recommend placing your credentials in the ~/.aws/credentials file in your home directory, with additional configuration in the ~/.aws/config directory. The AWS documentation has more information on setting up configuration and credential files. An example ~/.aws/credentials file might look like: . [my_profile_name] aws_access_key_id = ... aws_secret_access_key = ... and an example ‘~/.aws/config` file might look like: . [my-profile-name] region = us-east-1 . If you have multiple sets of credentials in these files, you can tell Provose which credentials you want to use with the AWS_PROFILE environment variable. You can also set credentials directly with the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. If you are running Provose on an AWS EC2 instance, Provose can use the credentials in the IAM instance profile for the instance. AWS has a documentation page about how to use instance profiles. ",
    "url": "/v3.0/tutorial/#set-up-your-credentials",
    "relUrl": "/v3.0/tutorial/#set-up-your-credentials"
  },"169": {
    "doc": "Tutorial",
    "title": "Set up an S3 bucket to store Terraform state",
    "content": "By default, Terraform stores state about your resources in a local file called terraform.tfstate. Terraform is also capable of storing this state in various remote backends like an Amazon S3 bucket. Terraform’s enterprise version also offers remote state storage, but that is beyond the scope of this tutorial. All features of Provose are available through the free and open-source version of Terraform. If you do not already have a project with pre-configured Terraform state, we recommend you follow these steps to: . | Use Terraform to create the S3 bucket that we will use to store Terraform state. | Terraform stores the local state from #1 in a local terraform.tfstate file. | We tell Terraform to move the contents of the local state file from #2 into the S3 bucket we created in #1. | . If the domain name you have chosen was example-internal.com and you want to deploy to the AWS region us-east-1, then the first version of our Terraform state configuration looks like this: . terraform { # Look out. We're going to replace this `backend \"local\"` block with # a `backend \"s3\"` block later in the tutorial. backend \"local\" { path = \"terraform.tfstate\" } required_providers { # Provose v3.0 currently uses the Terraform AWS provider 3.9.0. # Stick with this version for your own code to avoid compatibility # issues. aws = \"3.9.0\" } } provider \"aws\" { region = \"us-east-1\" } # This is an AWS Key Management Service (KMS) key that we will use to # encrypt the AWS S3 bucket. resource \"aws_kms_key\" \"terraform\" { description = \"Used to encrypt the Terraform state S3 bucket at rest.\" tags = { Provose = \"terraform\" } lifecycle { prevent_destroy = true } } # This is where we store the Terraform remote tfstate. resource \"aws_s3_bucket\" \"terraform\" { # This should be a globally-unique Amazon S3 bucket, although it # should not be accessible outside of the AWS credentials you use to run # Terraform. bucket = \"terraform-state.example-internal.com\" acl = \"private\" # For security and compliance reasons, Provose recommends that you # configure AWS Key Management Service (KMS) encryption at rest for # the bucket. server_side_encryption_configuration { rule { apply_server_side_encryption_by_default { kms_master_key_id = aws_kms_key.terraform.arn sse_algorithm = \"aws:kms\" } } } versioning { enabled = true } tags = { Provose = \"terraform\" } # This lifecycle parameter prevents Terraform from destroying the # bucket that contains its own state. lifecycle { prevent_destroy = true } } # This prevents public access to our remote Terraform tfstate. resource \"aws_s3_bucket_public_access_block\" \"terraform\" { bucket = aws_s3_bucket.terraform.id block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true lifecycle { prevent_destroy = true } } . We recommend that you place the above configuration in a file named terraform.tf. To keep things organized, you may want to put your Provose configuration into a separate file. This configuration creates the S3 bucket that you will use for remote state, but Terraform must store state in the local file terraform.tfstate before the S3 bucket is created. After saving terraform.tf, run the following commands: . terraform init terraform plan -out plan.out terrafrom apply \"plan.out\" . ",
    "url": "/v3.0/tutorial/#set-up-an-s3-bucket-to-store-terraform-state",
    "relUrl": "/v3.0/tutorial/#set-up-an-s3-bucket-to-store-terraform-state"
  },"170": {
    "doc": "Tutorial",
    "title": "Begin using Terraform remote state",
    "content": "In the previous step, we created the S3 bucket to store remote state, but we have not started using the bucket yet–Terraform is still configured to use local state. To move our state to a remote S3 bucket, we need to change the terraform block at the beginning of our terraform.tf file to reference remote state: . terraform { backend \"s3\" { # This is the name of the S3 bucket we use to store Terraform state. # We create this bucket below. bucket = \"terraform-state.example-internal.com\" key = \"tfstate\" region = \"us-east-1\" acl = \"private\" } required_providers { # Provose v3.0 currently uses the Terraform AWS provider 3.9.0. # Stick with this version for your own code to avoid compatibility # issues. aws = \"3.9.0\" } } provider \"aws\" { # ... this file continues from the previous example . Rerun terraform init and you should see the following prompt asking if you want to copy your local state to the S3 bucket. Answer with yes. Initializing the backend... Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"local\" backend to the newly configured \"s3\" backend. No existing state was found in the newly configured \"s3\" backend. Do you want to copy this state to the new \"s3\" backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: yes . ",
    "url": "/v3.0/tutorial/#begin-using-terraform-remote-state",
    "relUrl": "/v3.0/tutorial/#begin-using-terraform-remote-state"
  },"171": {
    "doc": "Tutorial",
    "title": "Initialize Provose",
    "content": "Now that you have Terraform’s remote state configuration set up, create another file named after the project you want to create with Provose. In these examples, we will go with the name myproject. In myproject.tf, we will enter the bare minimum Provose configuration: . module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } } . Run terraform init again to download Provose v3.0.0 and the Terraform modules and providers that Provose depends on. You should rerun terraform init every time you update Provose, or if you change any Terraform modules or providers that you use elsewhere in your code. ",
    "url": "/v3.0/tutorial/#initialize-provose",
    "relUrl": "/v3.0/tutorial/#initialize-provose"
  },"172": {
    "doc": "Tutorial",
    "title": "Start using Provose modules",
    "content": "You can now use Provose modules–like containers, s3_buckets, mysql_clusters, and more–to configure the AWS infrastructure that you need. Below is an example of two . # This is the Terraform `random_password` resource that we use to generate secure random # passwords for the AWS Aurora MySQL clusters that we provision below. # You can read more about the provider here # https://www.terraform.io/docs/providers/random/r/password.html resource \"random_password\" \"bigcluster_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } # This is another `random_password` resource that we use for the other cluster. resource \"random_password\" \"smallcluster_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { # These are various settings that are core to Provose. source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { # Provose is going to pull AWS credentials from you environment. # If you want to specify your keys in code, you can set the # `access_key` and `secret_key` map keys here. region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } # These are two S3 buckets, which will be made available to one of the # below containers. s3_buckets = { \"bucket-one.example-internal.com\" = { versioning = false } \"another-bucket.example-internal.com\" = { versioning = true } } # These are Docker images that we upload to Elastic Container Registry (ECR). images = { # We build this container from a local path and upload it to the registry. \"example/webserver\" = { local_path = \"../src/webserver\" } # We create the ECR image repository for this container, but you need to # build and upload the image yourself. \"example/anotherimage\" = {} } # These are Aurora MySQL clusters. mysql_clusters = { # This creates an AWS Aurora MySQL cluster available # at the host bigcluster.myproject.example-internal.com. # This host is only available within the VPC. bigcluster = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.bigcluster_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigmy.production.example-internal.com. # This host is only available within the VPC. smallcluster = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"otherdb\" password = random_password.smallcluster_password.result instances = { instance_type = \"db.t3.small\" instance_count = 3 } } } # These are names and values for secrets stored in AWS Secrets Manager. secrets = { bigcluster_password = random_password.bigcluster_password.result smallcluster_password = random_password.smallcluster_password.result } # These are Docker containers that we run on AWS Elastic Container Service (ECS). # When `private_registry` is `true`, we access one of the images from the # ECR repositories defined above in the `images` section. # When `private_registry` is `false`, we look for publicly-available containers # on Docker Hub. containers = { # This is an example of a container that runs an image that we privately # built and uploaded to ECR. # We run the container on AWS Fargate--which means that there are no EC2 # hosts exposed directly to the user. # The container is given access to one of the above MySQL # clusters via environment variables and the Secrets Manager. web = { image = { name = \"example/webserver\" tag = \"latest\" private_registry = true } public = { https = { internal_http_port = 8000 internal_http_health_check_path = \"/\" public_dns_names = [\"web.test.example.com\"] } } instances = { instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } environment = { MYSQL_HOST = \"bigcluster.myproject.example-internal.com\" MYSQL_USER = \"root\" MYSQL_DATABASE = \"exampledb\" MYSQL_PORT = 3306 } secrets = { MYSQL_PASSWORD = \"bigcluster_password\" } } # This is an example of a container that runs a publicly-available # image from Docker Hub on a few `t2.small` EC2 instances. helloworld = { image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"helloworld.example.com\"] } } instances = { instance_type = \"t2.small\" container_count = 4 instance_count = 2 cpu = 256 memory = 512 } } } } . You can read more about Provose’s capabilities in the Reference. ",
    "url": "/v3.0/tutorial/#start-using-provose-modules",
    "relUrl": "/v3.0/tutorial/#start-using-provose-modules"
  },"173": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": " ",
    "url": "/v3.0/tutorial/",
    "relUrl": "/v3.0/tutorial/"
  },"174": {
    "doc": "lustre_file_systems",
    "title": "lustre_file_systems",
    "content": " ",
    "url": "/v3.0/reference/lustre_file_systems/",
    "relUrl": "/v3.0/reference/lustre_file_systems/"
  },"175": {
    "doc": "lustre_file_systems",
    "title": "Description",
    "content": "This Provose configuration sets up AWS FSx Lustre clusters. Lustre is a high-performance networked filesytem commonly used in supercomputing. AWS FSx Lustre is Amazon Web Services’ managed Lustre offering. AWS FSx Lustre is appropriate for compute workloads that require large amounts of data and would otherwise be I/O-bound. For example, terabyte-scale machine learning typically requires fast storage, and FSx Lustre is a popular choice. Beware of long timeouts and “tainting” . AWS FSx Lustre clusters can take a long time to deploy–even multiple hours. By default, Terraform will wait 30 minutes before timing out. When this happens, you should use the AWS Web Console or the command line to check the status of your cluster. When you see a timeout from Terraform, you should not rerun Terraform before the cluster has finished deploying, otherwise this will destroy your cluster. When your cluster has finished deploying, Terraform might consider it “tainted” because it timed out during the cluster’s creation. However, in all likelihood, your cluster is most likely fine. If Terraform says something like . # module.{your-module-name}.aws_fsx_lustre_file_system.lustre_file_systems[\"{your-cluster-name}\"] is tainted, so must be replaced . then run this command: . terraform untaint 'module.{your-module-name}.aws_fsx_lustre_file_system.lustre_file_systems[\"{your-cluster-name}\"]' . with replacing {your-module-name} and {your-cluster-name} as appropriate. Make sure to wrap your resource name in single quotes (') to prevent your shell from interpreting the [, ]. and \" characters. ",
    "url": "/v3.0/reference/lustre_file_systems/#description",
    "relUrl": "/v3.0/reference/lustre_file_systems/#description"
  },"176": {
    "doc": "lustre_file_systems",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } # Here we create an AWS S3 bucket that we will use as the data repository # for our Lustre cluster. Data repositories are not necessary if you want # to spin up an empty Lustre cluster. s3_buckets = { \"mydata.myproject.example-internal.com\" = { versioning = false } } lustre_file_systems = { mydatacache = { # SCRATCH_2 clusters are appropriate for when you need fast reads and writes, # but do not need automated replication. If you want higher guarantees of # persistence, use the \"PERSISTENT\" deployment type. deployment_type = \"SCRATCH_2\" # Currently this is the smallest storage size that can be deployed. storage_capacity_gb = 12000 # Here we specify the S3 bucket and key prefix for loading into this # cluster. These can be set to different paths, but if they are set to the # same path, then writes to these files in Lustre will be written back # to S3. s3_import_path = \"s3://mydata.myproject.example-internal.com/prefix/\" s3_export_path = \"s3://mydata.myproject.example-internal.com/prefix/\" } } } . ",
    "url": "/v3.0/reference/lustre_file_systems/#examples",
    "relUrl": "/v3.0/reference/lustre_file_systems/#examples"
  },"177": {
    "doc": "lustre_file_systems",
    "title": "Inputs",
    "content": ". | deployment_type – Required. The filesystem deployment type. Currently this can be one of the following values. The AWS documentation has more information about the differences between the deployment types. | \"SCRATCH_1\" – The original storage type for AWS FSx Lustre. This is typically used for storing temporary data and intermediate computations. This storage type is not replicated, which makes it less reliable for long-term storage. | \"SCRATCH_2\" – A scratch storage type with a much higher burst speed. This is also not replicated. | \"PERSISTENT_1\" – A storage type that offers replication in the same Availability Zone (AZ), which makes it more appropriate for long-term storage. | . | storage_capacity_gb – Required. This is the total storage capacity of the Lustre cluster in gibibytes. The minimum value is 1200, which is about 1.2 tebibytes. The next valid value is 2400. From there, the valid capacity values go up in increments of 2400 for the \"PERSISTENT_1\" and \"SCRATCH_2\" types, and in increments of 3600 for the \"SCRATCH_1\" type. | per_unit_storage_throughput_mb_per_tb – Optional. This field is required only for the \"PERSISTENT_1\" deployment_type. It describes the throughput speed per tebibyte of provisioned storage. Valid values are 50, 100, and 200. More information about this key can be found under PerUnitStorageThroughput in the AWS documentation. | s3_import_path – Optional. If entered, this is an Amazon S3 path that can be used as a data repository for importing data into the Lustre cluster. | s3_export_path – Optional. If entered, this is an Amazon S3 path that the Lustre cluster will export to. | auto_import_policy – Optional. If you have specified s3_import_path to import data from an S3 bucket into your Lustre filesystem, you can set the following options for importing new data: . | \"NONE\" – This is the default value. AWS will not update your Lustre cluster with files created or changed after the Lustre cluster is created. | \"NEW\" – This setting will automatically import new files from S3 into Lustre, but not changed files or deletions. | \"NEW_CHANGED\" – This setting will automatically import new files and changed files. Files deleted from S3 will not be deleted in Lustre. | . | imported_file_chunk_size_mb – Optional. This value can be set if s3_import_path is set. It determines the stripe count and the maximum amount of data–in mebibytes–that can be located on a single physical disk. This value defaults to 1024, but can be set to any value between 1 and 512000 inclusive. | . ",
    "url": "/v3.0/reference/lustre_file_systems/#inputs",
    "relUrl": "/v3.0/reference/lustre_file_systems/#inputs"
  },"178": {
    "doc": "lustre_file_systems",
    "title": "Outputs",
    "content": ". | lustre_file_systems.aws_security_group.lustre_file_systems – This is a mapping between the cluster namd and the underlying aws_security_group resource. The security group opens up this filesystem to all traffic in the containing VPC. | lustre_file_systems.aws_fsx_lustre_file_system.lustre_file_systems – This is a mapping between the cluster name and the underlying aws_fsx_lustre_filesystem resource . | lustre_file_systems.aws_route53_record.lustre_file_systems – This is a mapping between the cluster name and the underlying aws_route53_record resource. | . ",
    "url": "/v3.0/reference/lustre_file_systems/#outputs",
    "relUrl": "/v3.0/reference/lustre_file_systems/#outputs"
  },"179": {
    "doc": "lustre_file_systems",
    "title": "lustre_file_systems",
    "content": " ",
    "url": "/v4.0/reference/lustre_file_systems/",
    "relUrl": "/v4.0/reference/lustre_file_systems/"
  },"180": {
    "doc": "lustre_file_systems",
    "title": "Description",
    "content": "This Provose configuration sets up AWS FSx Lustre clusters. Lustre is a high-performance networked filesytem commonly used in supercomputing. AWS FSx Lustre is Amazon Web Services’ managed Lustre offering. AWS FSx Lustre is appropriate for compute workloads that require large amounts of data and would otherwise be I/O-bound. For example, terabyte-scale machine learning typically requires fast storage, and FSx Lustre is a popular choice. Beware of long timeouts and “tainting” . AWS FSx Lustre clusters can take a long time to deploy–even multiple hours. By default, Terraform will wait 30 minutes before timing out. When this happens, you should use the AWS Web Console or the command line to check the status of your cluster. When you see a timeout from Terraform, you should not rerun Terraform before the cluster has finished deploying, otherwise this will destroy your cluster. When your cluster has finished deploying, Terraform might consider it “tainted” because it timed out during the cluster’s creation. However, in all likelihood, your cluster is most likely fine. If Terraform says something like . # module.{your-module-name}.aws_fsx_lustre_file_system.lustre_file_systems[\"{your-cluster-name}\"] is tainted, so must be replaced . then run this command: . terraform untaint 'module.{your-module-name}.aws_fsx_lustre_file_system.lustre_file_systems[\"{your-cluster-name}\"]' . with replacing {your-module-name} and {your-cluster-name} as appropriate. Make sure to wrap your resource name in single quotes (') to prevent your shell from interpreting the [, ]. and \" characters. ",
    "url": "/v4.0/reference/lustre_file_systems/#description",
    "relUrl": "/v4.0/reference/lustre_file_systems/#description"
  },"181": {
    "doc": "lustre_file_systems",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"myproject\" } # Here we create an AWS S3 bucket that we will use as the data repository # for our Lustre cluster. Data repositories are not necessary if you want # to spin up an empty Lustre cluster. s3_buckets = { \"mydata.myproject.example-internal.com\" = { versioning = false } } lustre_file_systems = { mydatacache = { # SCRATCH_2 clusters are appropriate for when you need fast reads and writes, # but do not need automated replication. If you want higher guarantees of # persistence, use the \"PERSISTENT\" deployment type. deployment_type = \"SCRATCH_2\" # Currently this is the smallest storage size that can be deployed. storage_capacity_gb = 12000 # Here we specify the S3 bucket and key prefix for loading into this # cluster. These can be set to different paths, but if they are set to the # same path, then writes to these files in Lustre will be written back # to S3. s3_import_path = \"s3://mydata.myproject.example-internal.com/prefix/\" s3_export_path = \"s3://mydata.myproject.example-internal.com/prefix/\" } } } . ",
    "url": "/v4.0/reference/lustre_file_systems/#examples",
    "relUrl": "/v4.0/reference/lustre_file_systems/#examples"
  },"182": {
    "doc": "lustre_file_systems",
    "title": "Inputs",
    "content": ". | deployment_type – Required. The filesystem deployment type. Currently this can be one of the following values. The AWS documentation has more information about the differences between the deployment types. | \"SCRATCH_1\" – The original storage type for AWS FSx Lustre. This is typically used for storing temporary data and intermediate computations. This storage type is not replicated, which makes it less reliable for long-term storage. | \"SCRATCH_2\" – A scratch storage type with a much higher burst speed. This is also not replicated. | \"PERSISTENT_1\" – A storage type that offers replication in the same Availability Zone (AZ), which makes it more appropriate for long-term storage. | . | storage_capacity_gb – Required. This is the total storage capacity of the Lustre cluster in gibibytes. The minimum value is 1200, which is about 1.2 tebibytes. The next valid value is 2400. From there, the valid capacity values go up in increments of 2400 for the \"PERSISTENT_1\" and \"SCRATCH_2\" types, and in increments of 3600 for the \"SCRATCH_1\" type. | per_unit_storage_throughput_mb_per_tb – Optional. This field is required only for the \"PERSISTENT_1\" deployment_type. It describes the throughput speed per tebibyte of provisioned storage. Valid values are 50, 100, and 200. More information about this key can be found under PerUnitStorageThroughput in the AWS documentation. | s3_import_path – Optional. If entered, this is an Amazon S3 path that can be used as a data repository for importing data into the Lustre cluster. | s3_export_path – Optional. If entered, this is an Amazon S3 path that the Lustre cluster will export to. | auto_import_policy – Optional. If you have specified s3_import_path to import data from an S3 bucket into your Lustre filesystem, you can set the following options for importing new data: . | \"NONE\" – This is the default value. AWS will not update your Lustre cluster with files created or changed after the Lustre cluster is created. | \"NEW\" – This setting will automatically import new files from S3 into Lustre, but not changed files or deletions. | \"NEW_CHANGED\" – This setting will automatically import new files and changed files. Files deleted from S3 will not be deleted in Lustre. | . | imported_file_chunk_size_mb – Optional. This value can be set if s3_import_path is set. It determines the stripe count and the maximum amount of data–in mebibytes–that can be located on a single physical disk. This value defaults to 1024, but can be set to any value between 1 and 512000 inclusive. | . ",
    "url": "/v4.0/reference/lustre_file_systems/#inputs",
    "relUrl": "/v4.0/reference/lustre_file_systems/#inputs"
  },"183": {
    "doc": "lustre_file_systems",
    "title": "Outputs",
    "content": ". | lustre_file_systems.aws_security_group.lustre_file_systems – This is a mapping between the cluster namd and the underlying aws_security_group resource. The security group opens up this filesystem to all traffic in the containing VPC. | lustre_file_systems.aws_fsx_lustre_file_system.lustre_file_systems – This is a mapping between the cluster name and the underlying aws_fsx_lustre_filesystem resource . | lustre_file_systems.aws_route53_record.lustre_file_systems – This is a mapping between the cluster name and the underlying aws_route53_record resource. | . ",
    "url": "/v4.0/reference/lustre_file_systems/#outputs",
    "relUrl": "/v4.0/reference/lustre_file_systems/#outputs"
  },"184": {
    "doc": "mysql_clusters",
    "title": "mysql_clusters",
    "content": " ",
    "url": "/v3.0/reference/mysql_clusters/",
    "relUrl": "/v3.0/reference/mysql_clusters/"
  },"185": {
    "doc": "mysql_clusters",
    "title": "Description",
    "content": "This Provose configuration sets up AWS Aurora MySQL database clusters. ",
    "url": "/v3.0/reference/mysql_clusters/#description",
    "relUrl": "/v3.0/reference/mysql_clusters/#description"
  },"186": {
    "doc": "mysql_clusters",
    "title": "Examples",
    "content": "resource \"random_password\" \"my1_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } resource \"random_password\" \"bigmy_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } mysql_clusters = { # This creates an AWS Aurora MySQL cluster available # at the host my1.production.example-internal.com. # This host is only available within the VPC. my1 = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.my1_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigmy.production.example-internal.com. # This host is only available within the VPC. bigmy = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.bigmy_password.result instances = { instance_type = \"db.t3.small\" instance_count = 3 } } } } . ",
    "url": "/v3.0/reference/mysql_clusters/#examples",
    "relUrl": "/v3.0/reference/mysql_clusters/#examples"
  },"187": {
    "doc": "mysql_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. Settings for the AWS RDS instances running the MySQL cluster. | instance_type – Required. The database instance type, like \"db.r5.large\". The accepted database instance types for AWS Aurora MySQL can be found here on the AWS website. | instance_count – Required. The number of database instances in the cluster. Provose requires that all database instances be of the same instance_type. | . | engine_version – Required. This is the version of the AWS Aurora MySQL cluster. See below to see the supported engine versions for AWS Aurora MySQL. | database_name – Required. The name of the initial database created by AWS RDS. You can create additional databases by logging into the instance with \"root\" as the username and the password you set below. | password – Required. The password for the database root user. | snapshot_identifier – Optional. If set, this is the ARN of the RDS snapshot to create this instance from. If not set, Provose provisions a blank database. | apply_immediately – Optional Defaults to true, which means that changes to the database are applied immediately. If set to false, any changes to the database configuration made through Provose or Terraform will be applied during the database’s next maintenance window. Be careful that making configuration changes can result in a database outage. | deletion_protection – Optional. Defaults to true, which is the opposite of the typical Terraform configuration. When set to true, the database cannot be deleted. Set to false if you are okay with deleting this database when running terraform destroy or other commands. | . ",
    "url": "/v3.0/reference/mysql_clusters/#inputs",
    "relUrl": "/v3.0/reference/mysql_clusters/#inputs"
  },"188": {
    "doc": "mysql_clusters",
    "title": "Outputs",
    "content": ". | mysql_clusters.aws_db_subnet_group.mysql – A mapping of aws_db_subnet_group resources that describe the subnets for every cluster specified. Provose defaults to setting all of the subnets available in the VPC. | mysql_clusters.aws_security_group.mysql – An aws_security_group resource that governs access to the MySQL clusters. By default, the database is open to connection from anywhere within the VPC. The database is not accessible to the public Internet. | mysql_clusters.aws_rds_cluster.mysql – A mapping of aws_rds_cluster resources for every cluster specified. | mysql_clusters.aws_rds_cluster_instance.mysql – A mapping of aws_rds_cluster_instance resources–of every instance in every Aurora MySQl cluster created by Provose. | mysql.aws_route53_record.mysql – A mapping of aws_route53_record resources that give a friendly DNS name for every Aurora MySQL cluster specified. | mysql.aws_route53_record.mysql__readonly – A mapping of aws_route53_record resources that give a friendly DNS name for the readonly endpoint for every Aurora MySQL cluster specified. | . ",
    "url": "/v3.0/reference/mysql_clusters/#outputs",
    "relUrl": "/v3.0/reference/mysql_clusters/#outputs"
  },"189": {
    "doc": "mysql_clusters",
    "title": "Supported engine versions",
    "content": "You can check which versions of AWS Aurora MySQL are available by running the following AWS CLI command and looking for the EngineVersion keys: . aws rds describe-db-engine-versions --engine aurora-mysql . Currently, the available versions are, in order from newest to oldest: . | \"5.7.mysql_aurora.2.08.0\" | \"5.7.mysql_aurora.2.07.2\" | \"5.7.mysql_aurora.2.07.1\" | \"5.7.mysql_aurora.2.07.0\" | \"5.7.mysql_aurora.2.06.0\" | \"5.7.mysql_aurora.2.05.0\" | \"5.7.mysql_aurora.2.04.8\" | \"5.7.mysql_aurora.2.04.7\" | \"5.7.mysql_aurora.2.04.6\" | \"5.7.mysql_aurora.2.04.5\" | \"5.7.mysql_aurora.2.04.4\" | \"5.7.mysql_aurora.2.04.3\" | \"5.7.mysql_aurora.2.04.2\" | \"5.7.mysql_aurora.2.04.1\" | \"5.7.mysql_aurora.2.04.0\" | \"5.7.mysql_aurora.2.03.4\" | \"5.7.mysql_aurora.2.03.3\" | \"5.7.mysql_aurora.2.03.2\" | \"5.7.12\" | . After version \"5.7.12\", AWS changed Aurora MySQL engine versioning to be in the format [compatible mysql version].mysql_aurora.[aurora version]. ",
    "url": "/v3.0/reference/mysql_clusters/#supported-engine-versions",
    "relUrl": "/v3.0/reference/mysql_clusters/#supported-engine-versions"
  },"190": {
    "doc": "mysql_clusters",
    "title": "mysql_clusters",
    "content": " ",
    "url": "/v4.0/reference/mysql_clusters/",
    "relUrl": "/v4.0/reference/mysql_clusters/"
  },"191": {
    "doc": "mysql_clusters",
    "title": "Description",
    "content": "This Provose configuration sets up AWS Aurora MySQL database clusters. ",
    "url": "/v4.0/reference/mysql_clusters/#description",
    "relUrl": "/v4.0/reference/mysql_clusters/#description"
  },"192": {
    "doc": "mysql_clusters",
    "title": "Examples",
    "content": "resource \"random_password\" \"my1_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } resource \"random_password\" \"bigmy_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } mysql_clusters = { # This creates an AWS Aurora MySQL cluster available # at the host my1.production.example-internal.com. # This host is only available within the VPC. my1 = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.my1_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigmy.production.example-internal.com. # This host is only available within the VPC. bigmy = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = random_password.bigmy_password.result instances = { instance_type = \"db.t3.small\" instance_count = 3 } } } } . ",
    "url": "/v4.0/reference/mysql_clusters/#examples",
    "relUrl": "/v4.0/reference/mysql_clusters/#examples"
  },"193": {
    "doc": "mysql_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. Settings for the AWS RDS instances running the MySQL cluster. | instance_type – Required. The database instance type, like \"db.r5.large\". The accepted database instance types for AWS Aurora MySQL can be found here on the AWS website. | instance_count – Required. The number of database instances in the cluster. Provose requires that all database instances be of the same instance_type. | . | engine_version – Required. This is the version of the AWS Aurora MySQL cluster. See below to see the supported engine versions for AWS Aurora MySQL. | database_name – Required. The name of the initial database created by AWS RDS. You can create additional databases by logging into the instance with \"root\" as the username and the password you set below. | password – Required. The password for the database root user. | snapshot_identifier – Optional. If set, this is the ARN of the RDS snapshot to create this instance from. If not set, Provose provisions a blank database. | apply_immediately – Optional Defaults to true, which means that changes to the database are applied immediately. If set to false, any changes to the database configuration made through Provose or Terraform will be applied during the database’s next maintenance window. Be careful that making configuration changes can result in a database outage. | deletion_protection – Optional. Defaults to true, which is the opposite of the typical Terraform configuration. When set to true, the database cannot be deleted. Set to false if you are okay with deleting this database when running terraform destroy or other commands. | . ",
    "url": "/v4.0/reference/mysql_clusters/#inputs",
    "relUrl": "/v4.0/reference/mysql_clusters/#inputs"
  },"194": {
    "doc": "mysql_clusters",
    "title": "Outputs",
    "content": ". | mysql_clusters.aws_db_subnet_group.mysql – A mapping of aws_db_subnet_group resources that describe the subnets for every cluster specified. Provose defaults to setting all of the subnets available in the VPC. | mysql_clusters.aws_security_group.mysql – An aws_security_group resource that governs access to the MySQL clusters. By default, the database is open to connection from anywhere within the VPC. The database is not accessible to the public Internet. | mysql_clusters.aws_rds_cluster.mysql – A mapping of aws_rds_cluster resources for every cluster specified. | mysql_clusters.aws_rds_cluster_instance.mysql – A mapping of aws_rds_cluster_instance resources–of every instance in every Aurora MySQl cluster created by Provose. | mysql.aws_route53_record.mysql – A mapping of aws_route53_record resources that give a friendly DNS name for every Aurora MySQL cluster specified. | mysql.aws_route53_record.mysql__readonly – A mapping of aws_route53_record resources that give a friendly DNS name for the readonly endpoint for every Aurora MySQL cluster specified. | . ",
    "url": "/v4.0/reference/mysql_clusters/#outputs",
    "relUrl": "/v4.0/reference/mysql_clusters/#outputs"
  },"195": {
    "doc": "mysql_clusters",
    "title": "Supported engine versions",
    "content": "You can check which versions of AWS Aurora MySQL are available by running the following AWS CLI command and looking for the EngineVersion keys: . aws rds describe-db-engine-versions --engine aurora-mysql . Currently, the available versions are, in order from newest to oldest: . | \"5.7.mysql_aurora.2.08.0\" | \"5.7.mysql_aurora.2.07.2\" | \"5.7.mysql_aurora.2.07.1\" | \"5.7.mysql_aurora.2.07.0\" | \"5.7.mysql_aurora.2.06.0\" | \"5.7.mysql_aurora.2.05.0\" | \"5.7.mysql_aurora.2.04.8\" | \"5.7.mysql_aurora.2.04.7\" | \"5.7.mysql_aurora.2.04.6\" | \"5.7.mysql_aurora.2.04.5\" | \"5.7.mysql_aurora.2.04.4\" | \"5.7.mysql_aurora.2.04.3\" | \"5.7.mysql_aurora.2.04.2\" | \"5.7.mysql_aurora.2.04.1\" | \"5.7.mysql_aurora.2.04.0\" | \"5.7.mysql_aurora.2.03.4\" | \"5.7.mysql_aurora.2.03.3\" | \"5.7.mysql_aurora.2.03.2\" | \"5.7.12\" | . After version \"5.7.12\", AWS changed Aurora MySQL engine versioning to be in the format [compatible mysql version].mysql_aurora.[aurora version]. ",
    "url": "/v4.0/reference/mysql_clusters/#supported-engine-versions",
    "relUrl": "/v4.0/reference/mysql_clusters/#supported-engine-versions"
  },"196": {
    "doc": "DNS and naming conventions",
    "title": "DNS and naming conventions",
    "content": "Provose requires a domain name in your AWS account to serve as the root domain name for internal services. This domain name is used as Common Name in the TLS certificates used to secure internal communications. This root domain is specified in the configuration provose_config.internal_root_domain. To enable multiple instances of Provose to share the same root domain, each Provose instance uses different subdomains, which is specified in provose_config.internal_subdomain. Provose sets up DNS names for services as subdomains of this internal subdomain. The below example is an abbreviated version of a configuration that would create four services available within the VPC: . | A Redis instance at redis1.production.example-internal.com | A Redis instance at redis2.production.example-internal.com | A PostgreSQL cluster at pg1.production.example-internal.com | A PostgreSQL cluster at pg2.production.example-internal.com | A Redis instance at redis1.testing.example-internal.com | A Redis instance at redis2.testing.example-internal.com | An Elasticsearch cluster at elastic1.testing.example-internal.com | An Elasticsearch cluster at elastic2.testing.example-internal.com | . module \"example1\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { ... } name = \"example1\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } # Here, we create two Redis clusters named `redis1` and `redis2`. redis_clusters = { # This Redis cluster's DNS name is `redis1.production.example-internal.com`. redis1 = { # fill out config here } # This Redis cluster's DNS name is `redis2.production.example-internal.com`. redis2 = { # fill out config here } } postgresql_clusters = { # This PostgreSQL cluster's DNS name is `pg1.production.example-internal.com`. pg1 = { # fill out config here } # This PostgreSQL cluster's DNS name is `pg2.production.example-internal.com`. pg2 = { # fill out config here } } } module \"example2\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { ... } name = \"example2\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"staging\" } # Here, we create two Redis clusters named `redis1` and `redis2`. redis_clusters = { # This cluster's DNS name is `redis1.staging.example-internal.com`. redis1 = { # fill details here... } # This cluster's DNS name is `redis2.staging.example-internal.com`. redis2 = { # fill details here... } } # Here, wqe create two Elasticsearch clusters named `elastic1` and `elastic2`. elasticsearch_clusters = { # This Elasticsearch cluster's DNS name is `elastic1.staging.example-internal.com`. elastic1 = { # fill details here... } # This Elasticsearch cluster's DNS name is `elastic2.staging.example-internal.com`. elastic2 = { # fill details here... } } } . ",
    "url": "/naming/",
    "relUrl": "/naming/"
  },"197": {
    "doc": "overrides",
    "title": "overrides",
    "content": " ",
    "url": "/v3.0/reference/overrides/",
    "relUrl": "/v3.0/reference/overrides/"
  },"198": {
    "doc": "overrides",
    "title": "Description",
    "content": "The overrides module exists for specific settings that are often used to maintain backwards-compatibility with older versions of Provose. They should not be used in newer versions of Provose. Sometimes a new version of Provose changes the name of a Terraform resource, which often requires that resource to be destroyed and recreated when upgrading to the newer version of Provose. This recreation may cause data loss, so to prevent it, an override key can be set to retain the old name. ",
    "url": "/v3.0/reference/overrides/#description",
    "relUrl": "/v3.0/reference/overrides/#description"
  },"199": {
    "doc": "overrides",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } overrides = { # `mysql-subnets` and `postgresql-subnets` are older names for the # AWS RDS database subnet groups in early versions of Provose. mysql_clusters__aws_db_subnet_group = \"mysql-subnets\" postgresql_clusters__aws_db_subnet_group = \"postgresql-subnets\" } } . ",
    "url": "/v3.0/reference/overrides/#examples",
    "relUrl": "/v3.0/reference/overrides/#examples"
  },"200": {
    "doc": "overrides",
    "title": "Inputs",
    "content": ". | mysql_clusters__aws_db_subnet_group – Sets the name for the MySQL database subnet group. This name must be unique within the AWS account. No other database subnet group must have this name. | postgresql_clusters__aws_db_subnet_group – Sets the name for the PostgreSQL database subnet group. This name must be unique within the AWS account. No other database subnet group must have this name. | redis_clusters__aws_elasticache_subnet_group – Sets the name for the ElastiCache Redis subnet group. This name must be unique within the AWS account. No other Redis cluster subnet group must have this name. | . ",
    "url": "/v3.0/reference/overrides/#inputs",
    "relUrl": "/v3.0/reference/overrides/#inputs"
  },"201": {
    "doc": "overrides",
    "title": "Outputs",
    "content": "There are no Terraform outputs for the overrides module. This is purely for setting configurations. ",
    "url": "/v3.0/reference/overrides/#outputs",
    "relUrl": "/v3.0/reference/overrides/#outputs"
  },"202": {
    "doc": "overrides",
    "title": "overrides",
    "content": " ",
    "url": "/v4.0/reference/overrides/",
    "relUrl": "/v4.0/reference/overrides/"
  },"203": {
    "doc": "overrides",
    "title": "Description",
    "content": "The overrides module exists for specific settings that are often used to maintain backwards-compatibility with older versions of Provose. They should not be used in newer versions of Provose. Sometimes a new version of Provose changes the name of a Terraform resource, which often requires that resource to be destroyed and recreated when upgrading to the newer version of Provose. This recreation may cause data loss, so to prevent it, an override key can be set to retain the old name. ",
    "url": "/v4.0/reference/overrides/#description",
    "relUrl": "/v4.0/reference/overrides/#description"
  },"204": {
    "doc": "overrides",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } overrides = { # `mysql-subnets` and `postgresql-subnets` are older names for the # AWS RDS database subnet groups in early versions of Provose. mysql_clusters__aws_db_subnet_group = \"mysql-subnets\" postgresql_clusters__aws_db_subnet_group = \"postgresql-subnets\" } } . ",
    "url": "/v4.0/reference/overrides/#examples",
    "relUrl": "/v4.0/reference/overrides/#examples"
  },"205": {
    "doc": "overrides",
    "title": "Inputs",
    "content": ". | mysql_clusters__aws_db_subnet_group – Sets the name for the MySQL database subnet group. This name must be unique within the AWS account. No other database subnet group must have this name. | postgresql_clusters__aws_db_subnet_group – Sets the name for the PostgreSQL database subnet group. This name must be unique within the AWS account. No other database subnet group must have this name. | redis_clusters__aws_elasticache_subnet_group – Sets the name for the ElastiCache Redis subnet group. This name must be unique within the AWS account. No other Redis cluster subnet group must have this name. | . ",
    "url": "/v4.0/reference/overrides/#inputs",
    "relUrl": "/v4.0/reference/overrides/#inputs"
  },"206": {
    "doc": "overrides",
    "title": "Outputs",
    "content": "There are no Terraform outputs for the overrides module. This is purely for setting configurations. ",
    "url": "/v4.0/reference/overrides/#outputs",
    "relUrl": "/v4.0/reference/overrides/#outputs"
  },"207": {
    "doc": "Philosophy",
    "title": "Provose’s design philosophy",
    "content": "These are some of the design principles we consider when setting some of Provose’s intelligent defaults. Configuring AWS infrastructure is difficult because of the sheer number of settings needed to make a production configuration. Tools like Terraform and CloudFormation expose the full complexity of the cloud for advanced users, but Provose’s goal is to make everything simpler. ",
    "url": "/philosophy/#provoses-design-philosophy",
    "relUrl": "/philosophy/#provoses-design-philosophy"
  },"208": {
    "doc": "Philosophy",
    "title": "Provose favors immediacy, not maintenance windows.",
    "content": "By default, Terraform schedules many changes for various resources–like RDS, ElastiCache, and DocumentDB–for the resource’s next maintenance window. Provose’s default is the opposite and changes are applied to resources immediately. However, this triggers a small amount of downtime, and this behavior can typically be disabled by setting the apply_immediately flag to false when applicable. When apply_immediately is set to false on a database, that means changes to it will be made during the next maintenance window. ",
    "url": "/philosophy/#provose-favors-immediacy-not-maintenance-windows",
    "relUrl": "/philosophy/#provose-favors-immediacy-not-maintenance-windows"
  },"209": {
    "doc": "Philosophy",
    "title": "Provose favors affordability over scalability",
    "content": "Some of Provose’s pre-packaged configurations cannot be easily configured to autoscale to high loads, but they have been designed to use cloud resources economically at small scales. This is because Provose is appropriate for running many small-scale experiments in the cloud without breaking the bank, and in many cases scalability engineering requies domain expertise that a library like Provose would be unable to provide. Furthermore, it is important that Provose’s defaults are not needlessly expensive for most users. Cloud computing can become expensive, and cheap defaults are important. ",
    "url": "/philosophy/#provose-favors-affordability-over-scalability",
    "relUrl": "/philosophy/#provose-favors-affordability-over-scalability"
  },"210": {
    "doc": "Philosophy",
    "title": "Provose standardizes on x86_64 Amazon Linux 2 on Amazon Web Services",
    "content": "There are other architectures, other AMIs, and other cloud provider, but in an effort to reduce the surface area for debugging, Provose will limit itself to running software on x86_64 Amazon Linux 2 instances on Amazon Web Services, although we do use custom Amazon Machine Images. ",
    "url": "/philosophy/#provose-standardizes-on-x86_64-amazon-linux-2-on-amazon-web-services",
    "relUrl": "/philosophy/#provose-standardizes-on-x86_64-amazon-linux-2-on-amazon-web-services"
  },"211": {
    "doc": "Philosophy",
    "title": "Philosophy",
    "content": " ",
    "url": "/philosophy/",
    "relUrl": "/philosophy/"
  },"212": {
    "doc": "postgresql_clusters",
    "title": "postgresql_clusters",
    "content": " ",
    "url": "/v4.0/reference/postgresql_clusters/",
    "relUrl": "/v4.0/reference/postgresql_clusters/"
  },"213": {
    "doc": "postgresql_clusters",
    "title": "Description",
    "content": "This Provose configuration sets up AWS Aurora PostgreSQL clusters. ",
    "url": "/v4.0/reference/postgresql_clusters/#description",
    "relUrl": "/v4.0/reference/postgresql_clusters/#description"
  },"214": {
    "doc": "postgresql_clusters",
    "title": "Examples",
    "content": "resource \"random_password\" \"pg1_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } resource \"random_password\" \"bigpg_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } postgresql_clusters = { # This creates an AWS Aurora PostgreSQL cluster available # at the host pg1.production.example-internal.com. # This host is only available within the VPC. pg1 = { engine_version = \"11.6\" database_name = \"exampledb\" password = random_password.pg1_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigpg.production.example-internal.com. # This host is only available within the VPC. bigpg = { engine_version = \"11.6\" database_name = \"exampledb\" password = random_password.bigpg_password.result instances = { instance_type = \"db.t3.medium\" instance_count = 3 } } } } . ",
    "url": "/v4.0/reference/postgresql_clusters/#examples",
    "relUrl": "/v4.0/reference/postgresql_clusters/#examples"
  },"215": {
    "doc": "postgresql_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. Settings for the AWS RDS instances running the PostgreSQL cluster. | instance_type – Required. The database instance type, like \"db.r5.large\". The accepted database instance types for AWS Aurora PostgreSQL can be found here on the AWS website. | instance_count – Required. The number of database instances in the cluster. Provose requires that all database instances be of the same instance_type. | . | engine_version – Required. This is the version of the AWS Aurora PostgreSQL cluster. The currently-supported engine versions can be found here on the AWS website. | database_name – Required. The name of the initial database created by AWS RDS. You can create additional databases by logging into the instance with \"root\" as the username and the password you set below. | password – Required. The password for the database root user. | snapshot_identifier – Optional. If set, this is the ARN of the RDS snapshot to create this instance from. If not set, Provose provisions a blank database. | apply_immediately – Optional Defaults to true, which means that configuration changes to the database are applied immediately. If set to false, any changes to the database configuration made through Provose or Terraform will be applied during the database’s next maintenance window. Be careful that making configuration changes can result in a database outage. | deletion_protection – Optional. Defaults to true, which is the opposite of the typical Terraform configuration. When set to true, the database cannot be deleted. Set to false if you are okay with deleting this database when running terraform destroy or other commands. | . ",
    "url": "/v4.0/reference/postgresql_clusters/#inputs",
    "relUrl": "/v4.0/reference/postgresql_clusters/#inputs"
  },"216": {
    "doc": "postgresql_clusters",
    "title": "Outputs",
    "content": ". | postgresql_clusters.aws_db_subnet_group.postgresql – A mapping of aws_db_subnet_group resources that describe the subnets for every cluster specified. Provose defaults to setting all of the subnets available in the VPC. | postgresql_clusters.aws_security_group.postgresql – An aws_security_group resource that governs access to the PostgreSQL clusters. By default, the database is open to connection from anywhere within the VPC. The database is not accessible to the public Internet. | postgresql_clusters.aws_rds_cluster.postgresql – A mapping of aws_rds_cluster resources for every cluster specified. | postgresql_clusters.aws_rds_cluster_instance.postgresql – A mapping of aws_rds_cluster_instance resources–of every instance in every Aurora PostgreSQL cluster created by Provose. | postgresql.aws_route53_record.postgresql – A mapping of aws_route53_record resources that give a friendly DNS name for every Aurora PostgreSQL cluster specified. | postgresql.aws_route53_record.postgresql__readonly – A mapping of aws_route53_record resources that give a friendly DNS name for the readonly endpoint for every Aurora postgresql cluster specified. | . ",
    "url": "/v4.0/reference/postgresql_clusters/#outputs",
    "relUrl": "/v4.0/reference/postgresql_clusters/#outputs"
  },"217": {
    "doc": "postgresql_clusters",
    "title": "Supported engine versions",
    "content": "You can check which versions of AWS Aurora PostgreSQl are available by running the following AWS CLI comamnd and looking for the EngineVersion keys: . aws rds describe-db-engine-versions --engine aurora-postgresql . Currently, the available versions are, in order from newest to oldest: . | \"11.7\" | \"11.6\" | \"11.4\" | \"10.12\" | \"10.11\" | \"10.7\" | \"10.7\" | \"10.6\" | \"10.5\" | \"10.4\" | \"9.6.17\" | \"9.6.16\" | \"9.6.12\" | \"9.6.11\" | \"9.6.9\" | \"9.6.8\" | \"9.6.6\" | \"9.6.3\" | . ",
    "url": "/v4.0/reference/postgresql_clusters/#supported-engine-versions",
    "relUrl": "/v4.0/reference/postgresql_clusters/#supported-engine-versions"
  },"218": {
    "doc": "postgresql_clusters",
    "title": "postgresql_clusters",
    "content": " ",
    "url": "/v3.0/reference/postgresql_clusters/",
    "relUrl": "/v3.0/reference/postgresql_clusters/"
  },"219": {
    "doc": "postgresql_clusters",
    "title": "Description",
    "content": "This Provose configuration sets up AWS Aurora PostgreSQL clusters. ",
    "url": "/v3.0/reference/postgresql_clusters/#description",
    "relUrl": "/v3.0/reference/postgresql_clusters/#description"
  },"220": {
    "doc": "postgresql_clusters",
    "title": "Examples",
    "content": "resource \"random_password\" \"pg1_password\" { # AWS RDS passwords must be between 8 and 41 characters length = 41 # This is a list of special characters that can be included in the # password. This lits omits characters that often need to be # escaped. override_special = \"()-_=+[]{}&lt;&gt;?\" } resource \"random_password\" \"bigpg_password\" { length = 41 override_special = \"()-_=+[]{}&lt;&gt;?\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } postgresql_clusters = { # This creates an AWS Aurora PostgreSQL cluster available # at the host pg1.production.example-internal.com. # This host is only available within the VPC. pg1 = { engine_version = \"11.6\" database_name = \"exampledb\" password = random_password.pg1_password.result instances = { instance_type = \"db.r5.large\" instance_count = 1 } } # This creates a cluster at bigpg.production.example-internal.com. # This host is only available within the VPC. bigpg = { engine_version = \"11.6\" database_name = \"exampledb\" password = random_password.bigpg_password.result instances = { instance_type = \"db.t3.medium\" instance_count = 3 } } } } . ",
    "url": "/v3.0/reference/postgresql_clusters/#examples",
    "relUrl": "/v3.0/reference/postgresql_clusters/#examples"
  },"221": {
    "doc": "postgresql_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. Settings for the AWS RDS instances running the PostgreSQL cluster. | instance_type – Required. The database instance type, like \"db.r5.large\". The accepted database instance types for AWS Aurora PostgreSQL can be found here on the AWS website. | instance_count – Required. The number of database instances in the cluster. Provose requires that all database instances be of the same instance_type. | . | engine_version – Required. This is the version of the AWS Aurora PostgreSQL cluster. The currently-supported engine versions can be found here on the AWS website. | database_name – Required. The name of the initial database created by AWS RDS. You can create additional databases by logging into the instance with \"root\" as the username and the password you set below. | password – Required. The password for the database root user. | snapshot_identifier – Optional. If set, this is the ARN of the RDS snapshot to create this instance from. If not set, Provose provisions a blank database. | apply_immediately – Optional Defaults to true, which means that configuration changes to the database are applied immediately. If set to false, any changes to the database configuration made through Provose or Terraform will be applied during the database’s next maintenance window. Be careful that making configuration changes can result in a database outage. | deletion_protection – Optional. Defaults to true, which is the opposite of the typical Terraform configuration. When set to true, the database cannot be deleted. Set to false if you are okay with deleting this database when running terraform destroy or other commands. | . ",
    "url": "/v3.0/reference/postgresql_clusters/#inputs",
    "relUrl": "/v3.0/reference/postgresql_clusters/#inputs"
  },"222": {
    "doc": "postgresql_clusters",
    "title": "Outputs",
    "content": ". | postgresql_clusters.aws_db_subnet_group.postgresql – A mapping of aws_db_subnet_group resources that describe the subnets for every cluster specified. Provose defaults to setting all of the subnets available in the VPC. | postgresql_clusters.aws_security_group.postgresql – An aws_security_group resource that governs access to the PostgreSQL clusters. By default, the database is open to connection from anywhere within the VPC. The database is not accessible to the public Internet. | postgresql_clusters.aws_rds_cluster.postgresql – A mapping of aws_rds_cluster resources for every cluster specified. | postgresql_clusters.aws_rds_cluster_instance.postgresql – A mapping of aws_rds_cluster_instance resources–of every instance in every Aurora PostgreSQL cluster created by Provose. | postgresql.aws_route53_record.postgresql – A mapping of aws_route53_record resources that give a friendly DNS name for every Aurora PostgreSQL cluster specified. | postgresql.aws_route53_record.postgresql__readonly – A mapping of aws_route53_record resources that give a friendly DNS name for the readonly endpoint for every Aurora postgresql cluster specified. | . ",
    "url": "/v3.0/reference/postgresql_clusters/#outputs",
    "relUrl": "/v3.0/reference/postgresql_clusters/#outputs"
  },"223": {
    "doc": "postgresql_clusters",
    "title": "Supported engine versions",
    "content": "You can check which versions of AWS Aurora PostgreSQl are available by running the following AWS CLI comamnd and looking for the EngineVersion keys: . aws rds describe-db-engine-versions --engine aurora-postgresql . Currently, the available versions are, in order from newest to oldest: . | \"11.7\" | \"11.6\" | \"11.4\" | \"10.12\" | \"10.11\" | \"10.7\" | \"10.7\" | \"10.6\" | \"10.5\" | \"10.4\" | \"9.6.17\" | \"9.6.16\" | \"9.6.12\" | \"9.6.11\" | \"9.6.9\" | \"9.6.8\" | \"9.6.6\" | \"9.6.3\" | . ",
    "url": "/v3.0/reference/postgresql_clusters/#supported-engine-versions",
    "relUrl": "/v3.0/reference/postgresql_clusters/#supported-engine-versions"
  },"224": {
    "doc": "provose_config",
    "title": "provose_config",
    "content": " ",
    "url": "/v3.0/reference/provose_config/",
    "relUrl": "/v3.0/reference/provose_config/"
  },"225": {
    "doc": "provose_config",
    "title": "Description",
    "content": "The provose_config module is used to describe settings that are required by all of the other Provose modules. Authentication . Provose connects to your Amazon Web Services account using both the Terraform AWS provider and the AWS CLI v2–which must be installed on the machine running Provose. Both the provider and the CLI will look for credentials in these places in the following order: . | The AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. | Credentials under the \"default\" profile in the ~/.aws/credentials file on your system. You can select a different profile with the AWS_PROFILE environment variable. | Credentials under the \"default\" profile in the ~/.aws/config file on your system. You can select a different profile with the AWS_PROFILE environment variable. | Container credentials, if you happen to be running Provose inside of a container you deployed to Amazon Elastic Container Service (ECS). | Instance profile credentials, if you happen to be running Provose inside of an Amazon EC2 instance. | . For #2 or #3, you would install your credentials in an appropriate file and then run AWS_PROFILE=name-of-your-profile terraform plan (or terraform apply or terraform destroy). Amazon [has a guide for setting up the credentials and config files. ",
    "url": "/v3.0/reference/provose_config/#description",
    "relUrl": "/v3.0/reference/provose_config/#description"
  },"226": {
    "doc": "provose_config",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" access_key = var.access_key secret_key = var.secret_key } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } } . ",
    "url": "/v3.0/reference/provose_config/#examples",
    "relUrl": "/v3.0/reference/provose_config/#examples"
  },"227": {
    "doc": "provose_config",
    "title": "Inputs",
    "content": ". | authentication – Required. This is an object that contains the authentication information needed for Provose’s underlying Terraform providers. | aws – Required. This object contains information for connecting to AWS resources. | region – Required. This is the AWS region for Provose to connect to. | access_key – Optional. This is the AWS IAM access key. If you omit this, Terraform will look in your AWS config and environment variable for credentials. If you are running Terraform on an AWS EC2 instance, you can omit the access_key key to use the EC2 instance’s IAM instance profile. | secret_key – Optional. This is the AWS IAM access key. If you omit this, Terraform will look in your AWS config and environment variable for credentials. If you are running Terraform on an AWS EC2 instance, you can omit the access_key key to use the EC2 instance’s IAM instance profile. | . | . | name – Required. This is the “name” for this instance of Provose. This is the namespace for the underlying Terraform resources that Provose deploys, which lets you use the Provose module multiple times without creating conflicts in name resources. | internal_root_domain – Required. This is the domain name that you are using with Provose. Provose requires you to have a domain name in your AWS account as the base name for TLS certificates that Provose provisions. These certificates are used to secure access to internal services. | internal_subdomain – Required. This is the subdomain used for this instance of Provose. Give a different internal subdomain to every Provose module to prevent name conflicts. | vpc_cidr – Optional. This is the CIDR that specifies how many addresses to allocate in the VPC and what format they are. This is an optional parameter that defaults to \"10.0.0.0/16\". If you are setting up multiple Provose modules that can connect to each other with VPC peering, you will want to set non-overlapping CIDRs for the two modules. | . ",
    "url": "/v3.0/reference/provose_config/#inputs",
    "relUrl": "/v3.0/reference/provose_config/#inputs"
  },"228": {
    "doc": "provose_config",
    "title": "Outputs",
    "content": "This Provose configuration creates no resources by itself. It helps configure all of the other resources. ",
    "url": "/v3.0/reference/provose_config/#outputs",
    "relUrl": "/v3.0/reference/provose_config/#outputs"
  },"229": {
    "doc": "provose_config",
    "title": "provose_config",
    "content": " ",
    "url": "/v4.0/reference/provose_config/",
    "relUrl": "/v4.0/reference/provose_config/"
  },"230": {
    "doc": "provose_config",
    "title": "Description",
    "content": "The provose_config module is used to describe settings that are required by all of the other Provose modules. Authentication . Provose connects to your Amazon Web Services account using both the Terraform AWS provider and the AWS CLI v2–which must be installed on the machine running Provose. Both the provider and the CLI will look for credentials in these places in the following order: . | The AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. | Credentials under the \"default\" profile in the ~/.aws/credentials file on your system. You can select a different profile with the AWS_PROFILE environment variable. | Credentials under the \"default\" profile in the ~/.aws/config file on your system. You can select a different profile with the AWS_PROFILE environment variable. | Container credentials, if you happen to be running Provose inside of a container you deployed to Amazon Elastic Container Service (ECS). | Instance profile credentials, if you happen to be running Provose inside of an Amazon EC2 instance. | . For #2 or #3, you would install your credentials in an appropriate file and then run AWS_PROFILE=name-of-your-profile terraform plan (or terraform apply or terraform destroy). Amazon [has a guide for setting up the credentials and config files. ",
    "url": "/v4.0/reference/provose_config/#description",
    "relUrl": "/v4.0/reference/provose_config/#description"
  },"231": {
    "doc": "provose_config",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" access_key = var.access_key secret_key = var.secret_key } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } } . ",
    "url": "/v4.0/reference/provose_config/#examples",
    "relUrl": "/v4.0/reference/provose_config/#examples"
  },"232": {
    "doc": "provose_config",
    "title": "Inputs",
    "content": ". | authentication – Required. This is an object that contains the authentication information needed for Provose’s underlying Terraform providers. | name – Required. This is the “name” for this instance of Provose. This is the namespace for the underlying Terraform resources that Provose deploys, which lets you use the Provose module multiple times without creating conflicts in name resources. | internal_root_domain – Required. This is the domain name that you are using with Provose. Provose requires you to have a domain name in your AWS account as the base name for TLS certificates that Provose provisions. These certificates are used to secure access to internal services. | internal_subdomain – Required. This is the subdomain used for this instance of Provose. Give a different internal subdomain to every Provose module to prevent name conflicts. | vpc_cidr – Optional. This is the CIDR that specifies how many addresses to allocate in the VPC and what format they are. This is an optional parameter that defaults to \"10.0.0.0/16\". If you are setting up multiple Provose modules that can connect to each other with VPC peering, you will want to set non-overlapping CIDRs for the two modules. | . ",
    "url": "/v4.0/reference/provose_config/#inputs",
    "relUrl": "/v4.0/reference/provose_config/#inputs"
  },"233": {
    "doc": "provose_config",
    "title": "Outputs",
    "content": "This Provose configuration creates no resources by itself. It helps configure all of the other resources. ",
    "url": "/v4.0/reference/provose_config/#outputs",
    "relUrl": "/v4.0/reference/provose_config/#outputs"
  },"234": {
    "doc": "redis_clusters",
    "title": "redis_clusters",
    "content": " ",
    "url": "/v3.0/reference/redis_clusters/",
    "relUrl": "/v3.0/reference/redis_clusters/"
  },"235": {
    "doc": "redis_clusters",
    "title": "Description",
    "content": "The Provose redis_clusters module sets up a Redis instance using Amazon ElastiCache for Redis. Currently, Provose only supports setting up clusters that contain a single instance, meaning that the instances.instance_count parameter is not yet supported. ",
    "url": "/v3.0/reference/redis_clusters/#description",
    "relUrl": "/v3.0/reference/redis_clusters/#description"
  },"236": {
    "doc": "redis_clusters",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } # Here, we create two Redis clusters named \"cluster1\" and \"cluster2\". redis_clusters = { # This cluster's DNS name is cluster1.production.example-internal.com cluster1 = { engine_version = \"5.0.6\" instances = { instance_type = \"cache.t3.micro\" } } # This cluster's DNS name is cluster2.production.example-internal.com cluster2 = { engine_version = \"5.0.6\" instances = { instance_type = \"cache.m5.large\" } # This means that changes to the cluster are applied in the next # mantenance window as opposed to immediately. apply_immediately = false } } } . ",
    "url": "/v3.0/reference/redis_clusters/#examples",
    "relUrl": "/v3.0/reference/redis_clusters/#examples"
  },"237": {
    "doc": "redis_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. This is a group of settings for the instances that run the ElastiCache cluster. | instance_type – Required. This lists the ElastiCache-specific instance type to deploy. An example value is \"cache.m5.large\". A complete list of available instance types is here in the AWS documentation. | . | engine_version – Required. This lists the Redis version to deploy–like \"5.0.6\". A complete list of available versions is available here in the AWS documentation. | apply_immediately – Optional. This defaults to true, which makes configuration changes apply to the cluster immediately. When set to false, configuration changes are instead applied during the cluster’s next maintenance window. | . ",
    "url": "/v3.0/reference/redis_clusters/#inputs",
    "relUrl": "/v3.0/reference/redis_clusters/#inputs"
  },"238": {
    "doc": "redis_clusters",
    "title": "Outputs",
    "content": ". | redis_clusters.aws_security_group.redis – This is a aws_security_group resource that governs access to the cluster. By default, the Redis cluster is accessible within the containing VPC created by Provose. | redis_clusters.aws_elasticache_subnet_group.redis – This is the aws_elasticache_subnet_group resource that defines which subnets are available to the clusters. By default, this is all of the subnets in the VPC. | redis_clusters.aws_elasticache_cluster.redis – This is a mapping from cluster names to aws_elasticache_cluster resources that configure each cluster. | redis_clusters.aws_route53_record.redis – This is a mapping from cluster names to aws_route53_record resources representing Route 53 DNS records that give friendly names to the clusters. | . ",
    "url": "/v3.0/reference/redis_clusters/#outputs",
    "relUrl": "/v3.0/reference/redis_clusters/#outputs"
  },"239": {
    "doc": "redis_clusters",
    "title": "redis_clusters",
    "content": " ",
    "url": "/v4.0/reference/redis_clusters/",
    "relUrl": "/v4.0/reference/redis_clusters/"
  },"240": {
    "doc": "redis_clusters",
    "title": "Description",
    "content": "The Provose redis_clusters module sets up a Redis instance using Amazon ElastiCache for Redis. Currently, Provose only supports setting up clusters that contain a single instance, meaning that the instances.instance_count parameter is not yet supported. ",
    "url": "/v4.0/reference/redis_clusters/#description",
    "relUrl": "/v4.0/reference/redis_clusters/#description"
  },"241": {
    "doc": "redis_clusters",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } # Here, we create two Redis clusters named \"cluster1\" and \"cluster2\". redis_clusters = { # This cluster's DNS name is cluster1.production.example-internal.com cluster1 = { engine_version = \"5.0.6\" instances = { instance_type = \"cache.t3.micro\" } } # This cluster's DNS name is cluster2.production.example-internal.com cluster2 = { engine_version = \"5.0.6\" instances = { instance_type = \"cache.m5.large\" } # This means that changes to the cluster are applied in the next # mantenance window as opposed to immediately. apply_immediately = false } } } . ",
    "url": "/v4.0/reference/redis_clusters/#examples",
    "relUrl": "/v4.0/reference/redis_clusters/#examples"
  },"242": {
    "doc": "redis_clusters",
    "title": "Inputs",
    "content": ". | instances – Required. This is a group of settings for the instances that run the ElastiCache cluster. | instance_type – Required. This lists the ElastiCache-specific instance type to deploy. An example value is \"cache.m5.large\". A complete list of available instance types is here in the AWS documentation. | . | engine_version – Required. This lists the Redis version to deploy–like \"5.0.6\". A complete list of available versions is available here in the AWS documentation. | apply_immediately – Optional. This defaults to true, which makes configuration changes apply to the cluster immediately. When set to false, configuration changes are instead applied during the cluster’s next maintenance window. | . ",
    "url": "/v4.0/reference/redis_clusters/#inputs",
    "relUrl": "/v4.0/reference/redis_clusters/#inputs"
  },"243": {
    "doc": "redis_clusters",
    "title": "Outputs",
    "content": ". | redis_clusters.aws_security_group.redis – This is a aws_security_group resource that governs access to the cluster. By default, the Redis cluster is accessible within the containing VPC created by Provose. | redis_clusters.aws_elasticache_subnet_group.redis – This is the aws_elasticache_subnet_group resource that defines which subnets are available to the clusters. By default, this is all of the subnets in the VPC. | redis_clusters.aws_elasticache_cluster.redis – This is a mapping from cluster names to aws_elasticache_cluster resources that configure each cluster. | redis_clusters.aws_route53_record.redis – This is a mapping from cluster names to aws_route53_record resources representing Route 53 DNS records that give friendly names to the clusters. | . ",
    "url": "/v4.0/reference/redis_clusters/#outputs",
    "relUrl": "/v4.0/reference/redis_clusters/#outputs"
  },"244": {
    "doc": "Release notes",
    "title": "Provose v3.0 Release notes",
    "content": "Provose 3.0 comes with a number of improvements aimed at simplifying deployments. There are not a whole lot of new features compared to version 2.0, but this is a a major version bump because some changes may be incompatible. Users migrating up from version 2.0 may notice that this version creates many more AWS tags. This should help users track their cloud expenses with more granularity in AWS Cost Explorer. ",
    "url": "/v4.0/release-notes/#provose-v30-release-notes",
    "relUrl": "/v4.0/release-notes/#provose-v30-release-notes"
  },"245": {
    "doc": "Release notes",
    "title": "Release notes",
    "content": " ",
    "url": "/v4.0/release-notes/",
    "relUrl": "/v4.0/release-notes/"
  },"246": {
    "doc": "Release notes",
    "title": "Provose v3.0 Release notes",
    "content": "Provose 3.0 comes with a number of improvements aimed at simplifying deployments. There are not a whole lot of new features compared to version 2.0, but this is a a major version bump because some changes may be incompatible. Users migrating up from version 2.0 may notice that this version creates many more AWS tags. This should help users track their cloud expenses with more granularity in AWS Cost Explorer. ",
    "url": "/v3.0/release-notes/#provose-v30-release-notes",
    "relUrl": "/v3.0/release-notes/#provose-v30-release-notes"
  },"247": {
    "doc": "Release notes",
    "title": "Release notes",
    "content": " ",
    "url": "/v3.0/release-notes/",
    "relUrl": "/v3.0/release-notes/"
  },"248": {
    "doc": "s3_buckets",
    "title": "s3_buckets",
    "content": " ",
    "url": "/v4.0/reference/s3_buckets/",
    "relUrl": "/v4.0/reference/s3_buckets/"
  },"249": {
    "doc": "s3_buckets",
    "title": "Description",
    "content": "The Provose s3_buckets module is a mapping of S3 buckets–which must have globally unique names–and some basic settings. ",
    "url": "/v4.0/reference/s3_buckets/#description",
    "relUrl": "/v4.0/reference/s3_buckets/#description"
  },"250": {
    "doc": "s3_buckets",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } s3_buckets = { \"example-bucket-name.example.com\" = {} \"some-other-unique-bucket-name\" = { versioning = true } \"third-bucket\" = { acl = \"private\" } } } . ",
    "url": "/v4.0/reference/s3_buckets/#examples",
    "relUrl": "/v4.0/reference/s3_buckets/#examples"
  },"251": {
    "doc": "s3_buckets",
    "title": "Inputs",
    "content": ". | versioning – Optional. Defaults to false. If set to true, then object versioning is enabled in the S3 bucket. | acl – Optional. This is a field to specify a “canned ACL.” The default ACL is \"private”, where the bucket owner gets full control and nobody else has access rights. Valid ACL values are \"private\", \"public-read\", \"public-read-write\", \"aws-read-exec\", \"authenticated-read\", \"bucket-owner-read\", \"bucket-owner-full-control\", or \"log-delivery-write\". The meaning of these canned ACLs can be read on this page in the AWS documentation. | . ",
    "url": "/v4.0/reference/s3_buckets/#inputs",
    "relUrl": "/v4.0/reference/s3_buckets/#inputs"
  },"252": {
    "doc": "s3_buckets",
    "title": "Outputs",
    "content": ". | s3_buckets.aws_s3_bucket.s3 – This is a mapping from bucket names to aws_s3_bucket resources. | . ",
    "url": "/v4.0/reference/s3_buckets/#outputs",
    "relUrl": "/v4.0/reference/s3_buckets/#outputs"
  },"253": {
    "doc": "s3_buckets",
    "title": "s3_buckets",
    "content": " ",
    "url": "/v3.0/reference/s3_buckets/",
    "relUrl": "/v3.0/reference/s3_buckets/"
  },"254": {
    "doc": "s3_buckets",
    "title": "Description",
    "content": "The Provose s3_buckets module is a mapping of S3 buckets–which must have globally unique names–and some basic settings. ",
    "url": "/v3.0/reference/s3_buckets/#description",
    "relUrl": "/v3.0/reference/s3_buckets/#description"
  },"255": {
    "doc": "s3_buckets",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } s3_buckets = { \"example-bucket-name.example.com\" = {} \"some-other-unique-bucket-name\" = { versioning = true } \"third-bucket\" = { acl = \"private\" } } } . ",
    "url": "/v3.0/reference/s3_buckets/#examples",
    "relUrl": "/v3.0/reference/s3_buckets/#examples"
  },"256": {
    "doc": "s3_buckets",
    "title": "Inputs",
    "content": ". | versioning – Optional. Defaults to false. If set to true, then object versioning is enabled in the S3 bucket. | acl – Optional. This is a field to specify a “canned ACL.” The default ACL is \"private”, where the bucket owner gets full control and nobody else has access rights. Valid ACL values are \"private\", \"public-read\", \"public-read-write\", \"aws-read-exec\", \"authenticated-read\", \"bucket-owner-read\", \"bucket-owner-full-control\", or \"log-delivery-write\". The meaning of these canned ACLs can be read on this page in the AWS documentation. | . ",
    "url": "/v3.0/reference/s3_buckets/#inputs",
    "relUrl": "/v3.0/reference/s3_buckets/#inputs"
  },"257": {
    "doc": "s3_buckets",
    "title": "Outputs",
    "content": ". | s3_buckets.aws_s3_bucket.s3 – This is a mapping from bucket names to aws_s3_bucket resources. | . ",
    "url": "/v3.0/reference/s3_buckets/#outputs",
    "relUrl": "/v3.0/reference/s3_buckets/#outputs"
  },"258": {
    "doc": "secrets",
    "title": "secrets",
    "content": " ",
    "url": "/v3.0/reference/secrets/",
    "relUrl": "/v3.0/reference/secrets/"
  },"259": {
    "doc": "secrets",
    "title": "Description",
    "content": "The Provose secrets module is a mapping of names to secret values. These are stored in Amazon Secrets Manager and can be accessed by containers deployed with the Provose containers module. ",
    "url": "/v3.0/reference/secrets/#description",
    "relUrl": "/v3.0/reference/secrets/#description"
  },"260": {
    "doc": "secrets",
    "title": "Examples",
    "content": "This is an example of defining secrets with the Provose secrets module, and then consuming them in the containers module. variable \"some_secret\" { type = string default = \"This is how to use a Terraform variable.\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } # Be careful not to put the actual values of your secrets into Git # or your version control. secrets = { root_password = \"...insert password here...\" secret_key = \"...insert secret key here...\" # Use Terraform variables to specify secret values so they don't end # up in your source code. other_secret = var.some_secret } # Here we include a container configuration as an example of how # other Provose modules use secrets. containers = { helloexample = { # these secrets named `root_password` and `secret_key` are retrieved # from AWS Secrets Manager and inserted into this Elastic Container Service # configuration as environment variables named `ROOT_PASSWORD` and # `APPLICATION_SECRET_KEY`. secrets = { ROOT_PASSWORD = \"root_password\" APPLICATION_SECRET_KEY = \"secret_key\" } # The configuration below this comment is just standard setup # for a container. image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"demo.example.com\"] } } instances = { instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } } } } . ",
    "url": "/v3.0/reference/secrets/#examples",
    "relUrl": "/v3.0/reference/secrets/#examples"
  },"261": {
    "doc": "secrets",
    "title": "Inputs",
    "content": "A typical Provose secrets module configuration looks like . secrets = { secret_name = \"secret value as a string\" other_secret_name = var.some_secret_variable } . ",
    "url": "/v3.0/reference/secrets/#inputs",
    "relUrl": "/v3.0/reference/secrets/#inputs"
  },"262": {
    "doc": "secrets",
    "title": "Outputs",
    "content": ". | secrets.aws_secretsmanager_secret.secrets – This is a mapping of aws_secretsmanager_secret resources for every secret specified. | secrets.aws_secretsmanager_secret_version.secrets – This is a mapping of aws_secretsmanager_secret_version resources for every secret specified. | . ",
    "url": "/v3.0/reference/secrets/#outputs",
    "relUrl": "/v3.0/reference/secrets/#outputs"
  },"263": {
    "doc": "secrets",
    "title": "secrets",
    "content": " ",
    "url": "/v4.0/reference/secrets/",
    "relUrl": "/v4.0/reference/secrets/"
  },"264": {
    "doc": "secrets",
    "title": "Description",
    "content": "The Provose secrets module is a mapping of names to secret values. These are stored in Amazon Secrets Manager and can be accessed by containers deployed with the Provose containers module. ",
    "url": "/v4.0/reference/secrets/#description",
    "relUrl": "/v4.0/reference/secrets/#description"
  },"265": {
    "doc": "secrets",
    "title": "Examples",
    "content": "This is an example of defining secrets with the Provose secrets module, and then consuming them in the containers module. variable \"some_secret\" { type = string default = \"This is how to use a Terraform variable.\" } module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } # Be careful not to put the actual values of your secrets into Git # or your version control. secrets = { root_password = \"...insert password here...\" secret_key = \"...insert secret key here...\" # Use Terraform variables to specify secret values so they don't end # up in your source code. other_secret = var.some_secret } # Here we include a container configuration as an example of how # other Provose modules use secrets. containers = { helloexample = { # these secrets named `root_password` and `secret_key` are retrieved # from AWS Secrets Manager and inserted into this Elastic Container Service # configuration as environment variables named `ROOT_PASSWORD` and # `APPLICATION_SECRET_KEY`. secrets = { ROOT_PASSWORD = \"root_password\" APPLICATION_SECRET_KEY = \"secret_key\" } # The configuration below this comment is just standard setup # for a container. image = { name = \"nginxdemos/hello\" tag = \"latest\" private_registry = false } public = { https = { internal_http_port = 80 internal_http_health_check_path = \"/\" public_dns_names = [\"demo.example.com\"] } } instances = { instance_type = \"FARGATE\" container_count = 10 cpu = 256 memory = 512 } } } } . ",
    "url": "/v4.0/reference/secrets/#examples",
    "relUrl": "/v4.0/reference/secrets/#examples"
  },"266": {
    "doc": "secrets",
    "title": "Inputs",
    "content": "A typical Provose secrets module configuration looks like . secrets = { secret_name = \"secret value as a string\" other_secret_name = var.some_secret_variable } . ",
    "url": "/v4.0/reference/secrets/#inputs",
    "relUrl": "/v4.0/reference/secrets/#inputs"
  },"267": {
    "doc": "secrets",
    "title": "Outputs",
    "content": ". | secrets.aws_secretsmanager_secret.secrets – This is a mapping of aws_secretsmanager_secret resources for every secret specified. | secrets.aws_secretsmanager_secret_version.secrets – This is a mapping of aws_secretsmanager_secret_version resources for every secret specified. | . ",
    "url": "/v4.0/reference/secrets/#outputs",
    "relUrl": "/v4.0/reference/secrets/#outputs"
  },"268": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "/security/",
    "relUrl": "/security/"
  },"269": {
    "doc": "Security",
    "title": "How to report a security issue",
    "content": "To report a security issue, email security@neocrym.com. Do not post on GitHub. ",
    "url": "/security/#how-to-report-a-security-issue",
    "relUrl": "/security/#how-to-report-a-security-issue"
  },"270": {
    "doc": "Security",
    "title": "Provose’s threat model",
    "content": "Provose keeps sensitive information in Terraform state. Terraform synchronizes its operations by storing “state” about what has been deployed. By default, the state is written to the local filesystem, but Terraform also makes it easy to store state remotely–such as in an Amazon S3 bucket. This allows multiple contributors to run Terraform on different machines while only keeping one synchronized copy of state. Other Terraform modules avoid writing sensitive information to Terraform state, instead writing sensitive information to the local filesystem. However, this makes it more difficult to synchronize Terraform on multiple machines. Provose assumes that your Terraform state is secure. We recommend storing state in an encrypted, versioned Amazon S3 bucket that can only be accessed by the AWS accounts that need to run Terraform. Provose believes in trusted VPCs. Many organizations structure their cloud infrastructure with the principle of defense-in-depth. However, Provose creates VPCs and provisions resources in them with network access open to other resources within the VPC. Provose makes the assumption that resources deployed within the same VPC can trust each other, and that defense-in-depth should be practiced by deploying unrelated resources in separate, unpeered VPCs. Provose recommends using AWS credentials that cannot destroy resources. Provose and Terraform have some superficial protection of important resources, like enabling deletion protection by default on AWS Relational Database Service (RDS) databases. ",
    "url": "/security/#provoses-threat-model",
    "relUrl": "/security/#provoses-threat-model"
  },"271": {
    "doc": "source",
    "title": "source",
    "content": " ",
    "url": "/v4.0/reference/source/",
    "relUrl": "/v4.0/reference/source/"
  },"272": {
    "doc": "source",
    "title": "Description",
    "content": "The source parameter is used in Terraform modules to describe where to find the module. When using Provose, you should set source to be \"github.com/provose/provose?ref=v3.0.0\". Always make sure that the ref= parameter is pinned to a specific version to avoid Provose upgrading without your intention. If you want to make modifications to Provose and use them, you should clone the Provose repository on GitHub with with shell command git clone --recurse-submodules https://github.com/provose/provose.git. After that, you can set the source parameter to be the local filesystem path where you cloned the repository. THe source parameter comes from Terraform’s underlying module syntax–not Provose itself. The Terraform documentation has more information about how to specify source in a module configuration. ",
    "url": "/v4.0/reference/source/#description",
    "relUrl": "/v4.0/reference/source/#description"
  },"273": {
    "doc": "source",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } } . ",
    "url": "/v4.0/reference/source/#examples",
    "relUrl": "/v4.0/reference/source/#examples"
  },"274": {
    "doc": "source",
    "title": "Inputs",
    "content": "The value for the source parameter is just a string that points at the online repository or filesystem path. This parameter is not a mapping. ",
    "url": "/v4.0/reference/source/#inputs",
    "relUrl": "/v4.0/reference/source/#inputs"
  },"275": {
    "doc": "source",
    "title": "Outputs",
    "content": "There are no outputs for the source parameter. ",
    "url": "/v4.0/reference/source/#outputs",
    "relUrl": "/v4.0/reference/source/#outputs"
  },"276": {
    "doc": "source",
    "title": "source",
    "content": " ",
    "url": "/v3.0/reference/source/",
    "relUrl": "/v3.0/reference/source/"
  },"277": {
    "doc": "source",
    "title": "Description",
    "content": "The source parameter is used in Terraform modules to describe where to find the module. When using Provose, you should set source to be \"github.com/provose/provose?ref=v3.0.0\". Always make sure that the ref= parameter is pinned to a specific version to avoid Provose upgrading without your intention. If you want to make modifications to Provose and use them, you should clone the Provose repository on GitHub with with shell command git clone --recurse-submodules https://github.com/provose/provose.git. After that, you can set the source parameter to be the local filesystem path where you cloned the repository. THe source parameter comes from Terraform’s underlying module syntax–not Provose itself. The Terraform documentation has more information about how to specify source in a module configuration. ",
    "url": "/v3.0/reference/source/#description",
    "relUrl": "/v3.0/reference/source/#description"
  },"278": {
    "doc": "source",
    "title": "Examples",
    "content": "module \"myproject\" { source = \"github.com/provose/provose?ref=v3.0.0\" provose_config = { authentication = { aws = { region = \"us-east-1\" } } name = \"myproject\" internal_root_domain = \"example-internal.com\" internal_subdomain = \"production\" } } . ",
    "url": "/v3.0/reference/source/#examples",
    "relUrl": "/v3.0/reference/source/#examples"
  },"279": {
    "doc": "source",
    "title": "Inputs",
    "content": "The value for the source parameter is just a string that points at the online repository or filesystem path. This parameter is not a mapping. ",
    "url": "/v3.0/reference/source/#inputs",
    "relUrl": "/v3.0/reference/source/#inputs"
  },"280": {
    "doc": "source",
    "title": "Outputs",
    "content": "There are no outputs for the source parameter. ",
    "url": "/v3.0/reference/source/#outputs",
    "relUrl": "/v3.0/reference/source/#outputs"
  },"281": {
    "doc": "Uninstalling Provose",
    "title": "Uninstalling Provose",
    "content": "This page describes how to delete a Provose (a Terraform module) and the underlying Terraform resources it creates. ",
    "url": "/v3.0/uninstalling/#uninstalling-provose",
    "relUrl": "/v3.0/uninstalling/#uninstalling-provose"
  },"282": {
    "doc": "Uninstalling Provose",
    "title": "Table of contents",
    "content": ". | Deleting a Provose module with terraform destroy | Deleting databases | Deleting S3 buckets | . ",
    "url": "/v3.0/uninstalling/#table-of-contents",
    "relUrl": "/v3.0/uninstalling/#table-of-contents"
  },"283": {
    "doc": "Uninstalling Provose",
    "title": "Deleting a Provose module with terraform destroy",
    "content": "Provose is implemented as a HashiCorp Terraform module. Most Terraform resources can be deleted by removing them from the source code, but Terraform treats modules as special. Removing them from a file or deleting the entire file will cause an error. Instead, use Terraform’s selective delete feature to remove modules, including Provose. For a module named myproject that looks like: . module \"myproject\" { // contents go here } . run terraform destroy -target module.myproject to destroy all resources in the project. ",
    "url": "/v3.0/uninstalling/#deleting-a-provose-module-with-terraform-destroy",
    "relUrl": "/v3.0/uninstalling/#deleting-a-provose-module-with-terraform-destroy"
  },"284": {
    "doc": "Uninstalling Provose",
    "title": "Deleting databases",
    "content": "If you have deployed a MySQL or PostgreSQL database, you may have to disable deletion protection in order for Terraform to succeed in deleting the database. This can be done in the AWS console or it can be done by adding deletion_protection = false in the Provose configuration. The below example shows how to disable deletion protection on a MySQL database. module \"myproject\" { // ...omitted configuration here... mysql_clusters = { db1 = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = \"some long password\" instances = { instance_type = \"db.r5.large\" instance_count = 1 } // Add this line to allow the database to be deleted. // If this line is omitted, deletion protection is enabled // and the instance cannot be deleted from Provose or // the AWS console. deletion_protection = false } } } . ",
    "url": "/v3.0/uninstalling/#deleting-databases",
    "relUrl": "/v3.0/uninstalling/#deleting-databases"
  },"285": {
    "doc": "Uninstalling Provose",
    "title": "Deleting S3 buckets",
    "content": "S3 buckets can only be deleted when they are empty. Provose also automatically creates S3 buckets for storing logs from the Application Load Balancers (ALBs) used to route HTTP traffic from the Internet and from within the VPC. If you are deleting a Provose module, you should delete or empty S3 buckets created by that module through the AWS console or the AWS CLI. ",
    "url": "/v3.0/uninstalling/#deleting-s3-buckets",
    "relUrl": "/v3.0/uninstalling/#deleting-s3-buckets"
  },"286": {
    "doc": "Uninstalling Provose",
    "title": "Uninstalling Provose",
    "content": " ",
    "url": "/v3.0/uninstalling/",
    "relUrl": "/v3.0/uninstalling/"
  },"287": {
    "doc": "Uninstalling Provose",
    "title": "Uninstalling Provose",
    "content": "This page describes how to delete a Provose (a Terraform module) and the underlying Terraform resources it creates. ",
    "url": "/v4.0/uninstalling/#uninstalling-provose",
    "relUrl": "/v4.0/uninstalling/#uninstalling-provose"
  },"288": {
    "doc": "Uninstalling Provose",
    "title": "Table of contents",
    "content": ". | Deleting a Provose module with terraform destroy | Deleting databases | Deleting S3 buckets | . ",
    "url": "/v4.0/uninstalling/#table-of-contents",
    "relUrl": "/v4.0/uninstalling/#table-of-contents"
  },"289": {
    "doc": "Uninstalling Provose",
    "title": "Deleting a Provose module with terraform destroy",
    "content": "Provose is implemented as a HashiCorp Terraform module. Most Terraform resources can be deleted by removing them from the source code, but Terraform treats modules as special. Removing them from a file or deleting the entire file will cause an error. Instead, use Terraform’s selective delete feature to remove modules, including Provose. For a module named myproject that looks like: . module \"myproject\" { // contents go here } . run terraform destroy -target module.myproject to destroy all resources in the project. ",
    "url": "/v4.0/uninstalling/#deleting-a-provose-module-with-terraform-destroy",
    "relUrl": "/v4.0/uninstalling/#deleting-a-provose-module-with-terraform-destroy"
  },"290": {
    "doc": "Uninstalling Provose",
    "title": "Deleting databases",
    "content": "If you have deployed a MySQL or PostgreSQL database, you may have to disable deletion protection in order for Terraform to succeed in deleting the database. This can be done in the AWS console or it can be done by adding deletion_protection = false in the Provose configuration. The below example shows how to disable deletion protection on a MySQL database. module \"myproject\" { // ...omitted configuration here... mysql_clusters = { db1 = { engine_version = \"5.7.mysql_aurora.2.08.0\" database_name = \"exampledb\" password = \"some long password\" instances = { instance_type = \"db.r5.large\" instance_count = 1 } // Add this line to allow the database to be deleted. // If this line is omitted, deletion protection is enabled // and the instance cannot be deleted from Provose or // the AWS console. deletion_protection = false } } } . ",
    "url": "/v4.0/uninstalling/#deleting-databases",
    "relUrl": "/v4.0/uninstalling/#deleting-databases"
  },"291": {
    "doc": "Uninstalling Provose",
    "title": "Deleting S3 buckets",
    "content": "S3 buckets can only be deleted when they are empty. Provose also automatically creates S3 buckets for storing logs from the Application Load Balancers (ALBs) used to route HTTP traffic from the Internet and from within the VPC. If you are deleting a Provose module, you should delete or empty S3 buckets created by that module through the AWS console or the AWS CLI. ",
    "url": "/v4.0/uninstalling/#deleting-s3-buckets",
    "relUrl": "/v4.0/uninstalling/#deleting-s3-buckets"
  },"292": {
    "doc": "Uninstalling Provose",
    "title": "Uninstalling Provose",
    "content": " ",
    "url": "/v4.0/uninstalling/",
    "relUrl": "/v4.0/uninstalling/"
  }
}
